{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chd415/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "sns.set(color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                   \n",
    "import nltk                                         \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer  \n",
    "LA = np.linalg\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          \n",
    "from gensim.models.word2vec import Word2Vec                                  \n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data text\n",
    "def load_data(filename):\n",
    "    load_file = pd.read_csv(filename,delimiter=',', header=0,\n",
    "                        dtype={'name':str, 'lvl1':str, 'lvl2':str, 'lvl3':str, 'descrption':str, 'type':str})\n",
    "    load_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "    load_file.duplicated(subset=None, keep='first')\n",
    "    load_file.set_index('id', inplace = True)\n",
    "    load_file.head()\n",
    "    return load_file\n",
    "#print(len(train_file))\n",
    "def load_label(filename):\n",
    "    load_label = pd.read_csv(filename,delimiter=',', header=0)\n",
    "    load_label.columns = ['id', 'score']\n",
    "    load_label.duplicated(subset=None, keep='first')\n",
    "    load_label.set_index('id', inplace = True)\n",
    "    return load_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_mathod(column):\n",
    "    values = []\n",
    "    indexs = []\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for count in range(len(train_file)):\n",
    "        value = train_file.get_value(count+1,column)\n",
    "        if value in values and value != np.nan:\n",
    "            continue\n",
    "        values.append(value)\n",
    "        indexs.append(len(values))\n",
    "    for j in range(len(indexs)):\n",
    "        mapping[values[j]] = indexs[j]\n",
    "    mapping[np.nan] = 0.0\n",
    "    return mapping\n",
    "#train_file['lvl3'] = train_file['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "#mapping_lvl3 = map_mathod('lvl3')\n",
    "#print(mapping_lvl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=5000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(max_features=5000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=1,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "    \n",
    "    return avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "temp = load_data('train_data.csv')\n",
    "temp['descrption'] = temp['descrption'].str.lower()\n",
    "description_X = temp.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "def test_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=5000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english',max_features=5000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=5,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "\n",
    "    tf_w_data = []\n",
    "    tf_data = tf_data.toarray()\n",
    "    i = 0\n",
    "    for row in splitted:\n",
    "        vec = [0 for i in range(50)]\n",
    "    \n",
    "        temp_tfidf = []\n",
    "        for val in tf_data[i]:\n",
    "            if val != 0:\n",
    "                temp_tfidf.append(val)\n",
    "    \n",
    "        count = 0\n",
    "        tf_idf_sum = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                count += 1\n",
    "                tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "                vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        if (tf_idf_sum == 0):\n",
    "            tf_w_data.append(vec)\n",
    "        else:\n",
    "            tf_w_data.append(vec/tf_idf_sum)\n",
    "#            vec = float(1/tf_idf_sum) * vec\n",
    "#        tf_w_data.append(vec)\n",
    "        i = i + 1\n",
    "    \n",
    "    return tf_w_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "temp['descrption'] = test_embedding(description_X)\n",
    "temp.descrption.head()\n",
    "#tfidf_n = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    #clean up data for lvl 1&2&3\n",
    "    filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "    filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].str.lower()\n",
    "    filename['name'] = filename['name'].str.lower()\n",
    "    filename['type'] = filename['type'].str.lower()\n",
    "    \n",
    "    mapping_lvl1 = map_mathod('lvl1')\n",
    "    mapping_lvl2 = map_mathod('lvl2')\n",
    "    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "    filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "    filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    #normalize price\n",
    "    maxp = filename.price.max()\n",
    "    valuethred = 600.\n",
    "    filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "\n",
    "    #clean up type \n",
    "    mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "    filename['type'] = filename['type'].map(mapping_type)\n",
    "    \n",
    "    #clean up text\n",
    "    description_X = filename.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['descrption'] = text_embedding(description_X)\n",
    "#    filename['descrption'] = filename.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    \n",
    "    name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['name'] = text_embedding(name_X)\n",
    "#    filename['name'] = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "\n",
    "    return filename,mapping_lvl1,mapping_lvl2,mapping_lvl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_file = load_data('train_data.csv')\n",
    "cleaned_train,mapping_lvl1,mapping_lvl2,mapping_lvl3 = clean_data(train_file)\n",
    "train_score = load_label('train_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell\n",
    "cleaned_train['descrption'] = temp['descrption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lvl1</th>\n",
       "      <th>lvl2</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>descrption</th>\n",
       "      <th>price</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.6084743364351906, -0.07872107653464708, 0....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.5623485353878803, 0.9936194160125322, -0.3...</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1.1191386995571, -0.22967379753078734, 0.377...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[-1.4841277248342521, 0.7558323889970779, 0.00...</td>\n",
       "      <td>0.024483</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.8892187004464425, -0.29880958139388397, 0....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[-0.44623231748118997, 1.0417288683820516, -0....</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-1.2035368266789352, -0.12202504792195909, 0....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[-0.9171022658195832, 0.35998929182634404, -1....</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.9923117461285609, -0.15841046061969416, 0....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[-1.534878188951148, -0.011181613637341393, -0...</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.1839767349167512, -0.08125560333540377, 0....</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[-0.9644463394338695, 0.13029184125032492, 0.6...</td>\n",
       "      <td>0.648317</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.6701826236265547, -0.07484207372181118, 0....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[-1.4704366902199884, 0.3081015112887447, -0.0...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-0.19215399978889358, -0.36905203304357, -0.0...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.15657642094921026, 0.4505208784211723, -0.0...</td>\n",
       "      <td>0.036233</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-0.17610700177028776, 0.017570930300280452, 0...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[-0.3327831854261062, 0.6277877012802724, -0.1...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[-0.06737104683582272, -0.14946113686476434, 0...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[0.2694536820054054, 0.5045704129669402, 0.333...</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  lvl1  lvl2  lvl3  \\\n",
       "id                                                                        \n",
       "1   [-0.6084743364351906, -0.07872107653464708, 0....   1.0   1.0   1.0   \n",
       "2   [-1.1191386995571, -0.22967379753078734, 0.377...   2.0   2.0   2.0   \n",
       "3   [-0.8892187004464425, -0.29880958139388397, 0....   3.0   3.0   3.0   \n",
       "4   [-1.2035368266789352, -0.12202504792195909, 0....   3.0   3.0   3.0   \n",
       "5   [-0.9923117461285609, -0.15841046061969416, 0....   3.0   3.0   3.0   \n",
       "6   [-0.1839767349167512, -0.08125560333540377, 0....   4.0   4.0   4.0   \n",
       "7   [-0.6701826236265547, -0.07484207372181118, 0....   2.0   5.0   5.0   \n",
       "8   [-0.19215399978889358, -0.36905203304357, -0.0...   5.0   6.0   6.0   \n",
       "9   [-0.17610700177028776, 0.017570930300280452, 0...   6.0   7.0   7.0   \n",
       "10  [-0.06737104683582272, -0.14946113686476434, 0...   6.0   7.0   8.0   \n",
       "\n",
       "                                           descrption     price  type  \n",
       "id                                                                     \n",
       "1   [-0.5623485353878803, 0.9936194160125322, -0.3...  0.213333   1.0  \n",
       "2   [-1.4841277248342521, 0.7558323889970779, 0.00...  0.024483   1.0  \n",
       "3   [-0.44623231748118997, 1.0417288683820516, -0....  0.023500   1.0  \n",
       "4   [-0.9171022658195832, 0.35998929182634404, -1....  0.029900   1.0  \n",
       "5   [-1.534878188951148, -0.011181613637341393, -0...  0.011333   1.0  \n",
       "6   [-0.9644463394338695, 0.13029184125032492, 0.6...  0.648317   1.0  \n",
       "7   [-1.4704366902199884, 0.3081015112887447, -0.0...  1.000000   2.0  \n",
       "8   [0.15657642094921026, 0.4505208784211723, -0.0...  0.036233   1.0  \n",
       "9   [-0.3327831854261062, 0.6277877012802724, -0.1...  0.041667   2.0  \n",
       "10  [0.2694536820054054, 0.5045704129669402, 0.333...  0.015800   1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train.head(10)\n",
    "\n",
    "#print(mapping_lvl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def rearrange(cleaned_data):\n",
    "    la = cleaned_data.lvl1.as_matrix(columns=None).tolist()\n",
    "    lb = cleaned_data.lvl2.as_matrix(columns=None).tolist()\n",
    "    lc = cleaned_data.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = la\n",
    "    X = np.column_stack((X,lb))\n",
    "    X = np.column_stack((X,lc))\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    \n",
    "    ld = cleaned_data.price.as_matrix(columns=None).tolist()\n",
    "    le = cleaned_data.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = np.column_stack((X,ld))\n",
    "    X = np.column_stack((X,le))\n",
    "\n",
    "    lf = cleaned_data.name.as_matrix(columns=None).tolist()\n",
    "    lg = cleaned_data.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "    s[8:] = 0.\n",
    "    new_lg = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "    sf[5:] = 0.\n",
    "    new_lf = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "    \n",
    "    X = np.column_stack((X,new_lf))\n",
    "    X = np.column_stack((X,new_lg))\n",
    "    X = X.tolist()\n",
    "    return X\n",
    "\n",
    "    \n",
    "    #print(len(X))\n",
    "X = rearrange(cleaned_train)\n",
    "#X = cleaned_train\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.00226352,\n",
       "        -0.28301015,  1.0625554 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.04953761,\n",
       "        -0.09987227,  0.3835247 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.20589787,\n",
       "        -0.348119  ,  0.19582243],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.10281686,\n",
       "        -0.19069737,  0.39116517],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.37300962,\n",
       "        -0.22298138,  0.2797751 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.9847661 ,\n",
       "        -0.16539523,  0.25280136]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data for my own record\n",
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "maxlen = 150\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, dtype='float32')\n",
    "#X_train = np.any(np.isnan(X_train))\n",
    "#X_train = np.all(np.isfinite(X_train))\n",
    "#print(X_train[1400].size)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Classifier\n",
    "def linear_regression_classifier(train_x, train_y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    param_grid = {'alpha': [math.pow(10,-i) for i in range(11)]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = MultinomialNB(alpha = best_parameters['alpha'])  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    param_grid = {'n_neighbors': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = best_parameters['n_neighbors'], algorithm='kd_tree')\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(100,300,10))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    #for para, val in best_parameters.items():\n",
    "        #print para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "def feature_select(x,y):\n",
    "    clf = ExtraTreesClassifier()\n",
    "    clf = clf.fit(x, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for my own record\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['KNN','LR','RF','DT']    \n",
    "    classifiers = {\n",
    "                   'KNN':knn_classifier,\n",
    "                    'LR':logistic_regression_classifier,\n",
    "                    'RF':random_forest_classifier,\n",
    "                    'DT':decision_tree_classifier,                 \n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_validation = select_model.transform(X_validation)\n",
    "\n",
    "    result = []\n",
    "        \n",
    "    for classifier in test_classifiers:    \n",
    "        print('******************* %s ********************' % classifier)    \n",
    "        start_time = time.time()    \n",
    "        model = classifiers[classifier](X_train, y_train)   \n",
    "        print('training took %fs!' % (time.time() - start_time))    \n",
    "        predict = model.predict(X_validation)\n",
    "\n",
    "        precision = metrics.precision_score(y_validation, predict)    \n",
    "        recall = metrics.recall_score(y_validation, predict)    \n",
    "        print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))    \n",
    "        accuracy = metrics.accuracy_score(y_validation, predict)    \n",
    "        print('accuracy: %.2f%%' % (100 * accuracy))\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.05176178,\n",
       "        -0.20214237,  0.5242788 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.10847525,\n",
       "        -0.33652806,  0.1886628 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.41879958,\n",
       "        -0.3271419 , -0.0795871 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.1034631 ,\n",
       "        -0.26796663,  0.15398878],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.01858747,\n",
       "        -0.3001894 ,  0.49771988],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.0362149 ,\n",
       "        -0.36573732,  0.68570286]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "18141 train sequences\n",
      "3629 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (18141, 353)\n",
      "x_test shape: (3629, 353)\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#test cnn model\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Embedding\n",
    "max_features = 353\n",
    "maxlen = 353\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "#X_train, X_validation, y_train, y_validation\n",
    "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "#X_train = np.asarray(np.abs(X))\n",
    "X_train = np.asarray(np.abs(X_train))\n",
    "X_validation = np.asarray(np.abs(X_validation))\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_validation), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, padding='post')\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_validation.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size,input_length=maxlen))\n",
    "#model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 14512 samples, validate on 3629 samples\n",
      "Epoch 1/2\n",
      "14512/14512 [==============================] - 91s 6ms/step - loss: 0.6223 - acc: 0.6894 - val_loss: 0.6251 - val_acc: 0.6831\n",
      "Epoch 2/2\n",
      "14512/14512 [==============================] - 89s 6ms/step - loss: 0.6198 - acc: 0.6894 - val_loss: 0.6247 - val_acc: 0.6831\n",
      "3629/3629 [==============================] - 5s 1ms/step\n",
      "Test score: 0.6246640345353205\n",
      "Test accuracy: 0.683108297285215\n"
     ]
    }
   ],
   "source": [
    "#cnn train \n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "score, acc = model.evaluate(X_validation, y_validation, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = load_data('test_data.csv')\n",
    "#cleaned_test = clean_data(test_file)\n",
    "X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_file\n",
    "#clean up data for lvl 1&2&3\n",
    "filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "filename['descrption'] = filename['descrption'].str.lower()\n",
    "filename['name'] = filename['name'].str.lower()\n",
    "    \n",
    "#    mapping_lvl1 = map_mathod('lvl1')\n",
    "#    mapping_lvl2 = map_mathod('lvl2')\n",
    "#    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    #normalize price\n",
    "maxp = filename.price.max()\n",
    "valuethred = 600.\n",
    "filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    #hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "\n",
    "    #clean up type \n",
    "mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "filename['type'] = filename['type'].map(mapping_type)\n",
    "    \n",
    "    #clean up text\n",
    "description_X = filename.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "filename['descrption'] = text_embedding(description_X)\n",
    "    \n",
    "name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "filename['name'] = text_embedding(name_X)\n",
    "\n",
    "#    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = filename\n",
    "lat = cleaned_test.lvl1.as_matrix(columns=None).tolist()\n",
    "lbt = cleaned_test.lvl2.as_matrix(columns=None).tolist()\n",
    "lct = cleaned_test.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "Xt = lat\n",
    "Xt = np.column_stack((Xt,lbt))\n",
    "Xt = np.column_stack((Xt,lct))\n",
    "\n",
    "enct = preprocessing.OneHotEncoder()\n",
    "enct.fit(Xt)\n",
    "Xt = enct.transform(Xt).toarray()\n",
    "\n",
    "ldt = cleaned_test.price.as_matrix(columns=None).tolist()\n",
    "let = cleaned_test.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "Xt = np.column_stack((Xt,ldt))\n",
    "Xt = np.column_stack((Xt,let))\n",
    "\n",
    "lft = cleaned_test.name.as_matrix(columns=None).tolist()\n",
    "lgt = cleaned_test.descrption.as_matrix(columns=None).tolist()\n",
    "lg_arrayt = np.vstack( lgt )\n",
    "lf_arrayt = np.vstack( lft )\n",
    "\n",
    "Ut, st, Vht = LA.svd(lg_arrayt, full_matrices=False)\n",
    "assert np.allclose(lg_arrayt, np.dot(Ut, np.dot(np.diag(st), Vht)))\n",
    "\n",
    "st[10:] = 0.\n",
    "new_lgt = np.dot(Ut, np.dot(np.diag(st), Vht))\n",
    "\n",
    "Uf, sf, Vhf = LA.svd(lf_arrayt, full_matrices=False)\n",
    "assert np.allclose(lf_arrayt, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "sf[5:] = 0.\n",
    "new_lft = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "\n",
    "\n",
    "#X = la\n",
    "#X = np.column_stack((X,lb))\n",
    "#X = np.column_stack((X,lc))\n",
    "#X = np.column_stack((X,ld))\n",
    "#X = np.column_stack((X,le))\n",
    "Xt = np.column_stack((Xt,new_lft))\n",
    "Xt = np.column_stack((Xt,new_lgt))\n",
    "Xt = Xt.tolist()\n",
    "#cleaned_test = Xt\n",
    "#return X,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lvl1</th>\n",
       "      <th>lvl2</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>descrption</th>\n",
       "      <th>price</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18142</th>\n",
       "      <td>[-0.3997471257379012, 0.2520856463150786, 0.01...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>[-0.45138139003778205, -0.3850018681191346, 0....</td>\n",
       "      <td>0.081667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18143</th>\n",
       "      <td>[-1.2714371785987169, 0.4088976033963263, 0.12...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>[-0.6847902453523509, 0.8004213119952046, 0.01...</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18144</th>\n",
       "      <td>[-0.8488824397680306, 0.28272791274468456, 0.0...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>[-0.49072218105635224, 0.9107943826820701, -0....</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18145</th>\n",
       "      <td>[-0.45606777879099053, 0.13733294396661222, 0....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[-0.2935936341295019, 0.929608801662107, 0.120...</td>\n",
       "      <td>0.196667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18146</th>\n",
       "      <td>[-0.6443462886640595, 0.23275727625069623, 0.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[-0.9712255646785101, 1.199740418465808, 0.064...</td>\n",
       "      <td>0.191333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18147</th>\n",
       "      <td>[-1.9098076740136514, 0.6792950309239901, 0.11...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>[-0.7178006283938885, 0.6413981612099128, 0.98...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18148</th>\n",
       "      <td>[-1.4283913671970367, 0.43280950114907074, 0.2...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>[-1.6521246461641221, 0.8136984603036017, -0.3...</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>[-0.2758771926164627, 0.12726246068874994, 0.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.29691647479194216, 0.5113996078260243, 0.3...</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18150</th>\n",
       "      <td>[-1.0949250489043503, 0.42797613616746205, 0.1...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>[-1.7333045482635498, 2.2086067676544188, 0.50...</td>\n",
       "      <td>0.057550</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>[-1.3527134557565053, 0.5046901653210322, -0.1...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>[-0.3767805846541056, 1.222330191338967, 0.059...</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  lvl1  lvl2   lvl3  \\\n",
       "id                                                                            \n",
       "18142  [-0.3997471257379012, 0.2520856463150786, 0.01...   5.0  21.0  166.0   \n",
       "18143  [-1.2714371785987169, 0.4088976033963263, 0.12...   9.0  32.0   47.0   \n",
       "18144  [-0.8488824397680306, 0.28272791274468456, 0.0...   9.0  32.0   68.0   \n",
       "18145  [-0.45606777879099053, 0.13733294396661222, 0....   1.0  25.0   40.0   \n",
       "18146  [-0.6443462886640595, 0.23275727625069623, 0.0...   1.0  47.0  106.0   \n",
       "18147  [-1.9098076740136514, 0.6792950309239901, 0.11...   2.0  52.0  181.0   \n",
       "18148  [-1.4283913671970367, 0.43280950114907074, 0.2...   2.0   2.0   84.0   \n",
       "18149  [-0.2758771926164627, 0.12726246068874994, 0.0...   1.0   1.0    1.0   \n",
       "18150  [-1.0949250489043503, 0.42797613616746205, 0.1...   2.0  19.0  177.0   \n",
       "18151  [-1.3527134557565053, 0.5046901653210322, -0.1...   8.0  18.0   19.0   \n",
       "\n",
       "                                              descrption     price  type  \n",
       "id                                                                        \n",
       "18142  [-0.45138139003778205, -0.3850018681191346, 0....  0.081667   2.0  \n",
       "18143  [-0.6847902453523509, 0.8004213119952046, 0.01...  0.005383   1.0  \n",
       "18144  [-0.49072218105635224, 0.9107943826820701, -0....  0.041783   1.0  \n",
       "18145  [-0.2935936341295019, 0.929608801662107, 0.120...  0.196667   2.0  \n",
       "18146  [-0.9712255646785101, 1.199740418465808, 0.064...  0.191333   1.0  \n",
       "18147  [-0.7178006283938885, 0.6413981612099128, 0.98...  0.013333   1.0  \n",
       "18148  [-1.6521246461641221, 0.8136984603036017, -0.3...  0.028950   1.0  \n",
       "18149  [-0.29691647479194216, 0.5113996078260243, 0.3...  0.017333   1.0  \n",
       "18150  [-1.7333045482635498, 2.2086067676544188, 0.50...  0.057550   1.0  \n",
       "18151  [-0.3767805846541056, 1.222330191338967, 0.059...  0.015500   1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned_test = clean_test(test_file)\n",
    "#cleaned_test = filename\n",
    "cleaned_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing import sequence\n",
    "#maxlen = 150\n",
    "X_test = rearrange(cleaned_test)\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=maxlen, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "training took 0.506622s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 67.168967s!\n",
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['LR','RF']    \n",
    "    classifiers = {\n",
    "                   'LR':logistic_regression_classifier,\n",
    "                   'RF':random_forest_classifier         \n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "#    X_test = rearrange(Xt)\n",
    "#    X_test = Xt\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_test = select_model.transform(X_test)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = classifiers['LR'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_lr = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['RF'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_rf = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Embedding\n",
    "max_features = 353\n",
    "maxlen = 353\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18141/18141 [==============================] - 111s 6ms/step - loss: 0.6108 - acc: 0.6878\n",
      "Epoch 2/5\n",
      "18141/18141 [==============================] - 110s 6ms/step - loss: 0.5883 - acc: 0.6992\n",
      "Epoch 3/5\n",
      "18141/18141 [==============================] - 109s 6ms/step - loss: 0.5738 - acc: 0.7109\n",
      "Epoch 4/5\n",
      "18141/18141 [==============================] - 110s 6ms/step - loss: 0.5505 - acc: 0.7226\n",
      "Epoch 5/5\n",
      "18141/18141 [==============================] - 116s 6ms/step - loss: 0.5480 - acc: 0.7233\n"
     ]
    }
   ],
   "source": [
    "#cnn test\n",
    "X_train, y_train = X,Y\n",
    "X_train = np.asarray(np.abs(X_train))\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = np.asarray(np.abs(X_test))\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "model.fit(X_train, y_train,batch_size=batch_size,epochs=5)\n",
    "\n",
    "Y_predict_cnn = model.predict(X_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6723297 ],\n",
       "       [0.67681545],\n",
       "       [0.6763889 ],\n",
       "       ...,\n",
       "       [0.674564  ],\n",
       "       [0.6656798 ],\n",
       "       [0.663556  ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79162752, 0.65679351, 0.72286857, ..., 0.64126559, 0.45111808,\n",
       "       0.54720251])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78947368, 0.42105263, 0.36842105, ..., 0.47368421, 0.31578947,\n",
       "       0.73684211])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_predict_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ab6052a35d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mY_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predict_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mY_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predict_lr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY_predict_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY_predict_cnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_predict_rf' is not defined"
     ]
    }
   ],
   "source": [
    "Y_predict = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict.append((Y_predict_lr[i] + Y_predict_rf[i] + Y_predict_cnn[i]) / 3.0)\n",
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7905506018626285,\n",
       " 0.5389230700508951,\n",
       " 0.5456448126117691,\n",
       " 0.7615332625078359,\n",
       " 0.6964557085122125,\n",
       " 0.6922442685720573,\n",
       " 0.6631435775888013,\n",
       " 0.5690408378288059,\n",
       " 0.6170369950766343,\n",
       " 0.5750049314314054,\n",
       " 0.6988875967844698,\n",
       " 0.6237114257782729,\n",
       " 0.5801986306472262,\n",
       " 0.5545690341996896,\n",
       " 0.6981240027529748,\n",
       " 0.44421407416462,\n",
       " 0.7156358190171896,\n",
       " 0.6320022803329409,\n",
       " 0.734229617527003,\n",
       " 0.5240016682343368,\n",
       " 0.6973517919626471,\n",
       " 0.7992450315423762,\n",
       " 0.7093779434144778,\n",
       " 0.7501994368036741,\n",
       " 0.8172100221298204,\n",
       " 0.7409656553853292,\n",
       " 0.7235022635820609,\n",
       " 0.8426902131305243,\n",
       " 0.6967842114253233,\n",
       " 0.7044323321908251,\n",
       " 0.6203400956139553,\n",
       " 0.4972549753629961,\n",
       " 0.6843346493890947,\n",
       " 0.6361391579757184,\n",
       " 0.5346090070152356,\n",
       " 0.466954012742473,\n",
       " 0.5468075231601449,\n",
       " 0.6862756553190797,\n",
       " 0.5846757249397795,\n",
       " 0.7616097782650648,\n",
       " 0.5750904469367697,\n",
       " 0.8059581021694862,\n",
       " 0.6555049792904271,\n",
       " 0.48057180456075677,\n",
       " 0.5232262898136184,\n",
       " 0.6425979758983977,\n",
       " 0.6270377054473281,\n",
       " 0.41319288414099675,\n",
       " 0.741162104252896,\n",
       " 0.7293149278751423,\n",
       " 0.575954922711686,\n",
       " 0.6529092101581868,\n",
       " 0.6402852626476685,\n",
       " 0.5473722725861079,\n",
       " 0.3675074068096553,\n",
       " 0.6956395734019951,\n",
       " 0.5511670671883337,\n",
       " 0.6657169018360001,\n",
       " 0.4016338695957569,\n",
       " 0.7584686215502339,\n",
       " 0.6203500275248974,\n",
       " 0.48599252219345856,\n",
       " 0.5980495042685383,\n",
       " 0.5435482529635801,\n",
       " 0.5519455504017219,\n",
       " 0.7869566572248798,\n",
       " 0.5401478722085742,\n",
       " 0.6549981716311629,\n",
       " 0.6627906706368436,\n",
       " 0.511939286248388,\n",
       " 0.4451128096354607,\n",
       " 0.39758382850658236,\n",
       " 0.5773165905205563,\n",
       " 0.6326773631696685,\n",
       " 0.772053213304456,\n",
       " 0.5918921472945176,\n",
       " 0.5640642847091718,\n",
       " 0.5339286984614621,\n",
       " 0.5244956777157266,\n",
       " 0.5484680997037269,\n",
       " 0.6375912443724883,\n",
       " 0.40661241078739285,\n",
       " 0.3278730279558038,\n",
       " 0.48979051827732145,\n",
       " 0.7096058858350223,\n",
       " 0.5091777391233607,\n",
       " 0.657728414501005,\n",
       " 0.39236441211190687,\n",
       " 0.6686700736144748,\n",
       " 0.711581716224988,\n",
       " 0.5641283749528218,\n",
       " 0.5560768902165255,\n",
       " 0.643689324534655,\n",
       " 0.6777117888333094,\n",
       " 0.7884426888424068,\n",
       " 0.7694527196288654,\n",
       " 0.781978784356736,\n",
       " 0.7610903716120856,\n",
       " 0.7124827473227431,\n",
       " 0.6894590257285487,\n",
       " 0.7736554706853441,\n",
       " 0.7423148092963565,\n",
       " 0.820301725067575,\n",
       " 0.7359754037884929,\n",
       " 0.7448131432990629,\n",
       " 0.7164032018444223,\n",
       " 0.5907730227751213,\n",
       " 0.7659985446868667,\n",
       " 0.6123574871554087,\n",
       " 0.7981406965300819,\n",
       " 0.703922086927854,\n",
       " 0.6147554820745964,\n",
       " 0.6978493209665038,\n",
       " 0.6634237488546859,\n",
       " 0.7548010986134157,\n",
       " 0.6408451825195913,\n",
       " 0.6379979689567082,\n",
       " 0.7260967385198978,\n",
       " 0.5710966228032863,\n",
       " 0.7129228795668789,\n",
       " 0.6697122589359599,\n",
       " 0.702786844394116,\n",
       " 0.6836711181665648,\n",
       " 0.5086448380686274,\n",
       " 0.7075889536093474,\n",
       " 0.7896734697631729,\n",
       " 0.684234580519663,\n",
       " 0.5974974430837241,\n",
       " 0.6426199888057481,\n",
       " 0.7174237512444491,\n",
       " 0.7172657446668678,\n",
       " 0.6561476191493373,\n",
       " 0.6730094883161527,\n",
       " 0.5596371991893525,\n",
       " 0.7133069921186446,\n",
       " 0.7863405484555249,\n",
       " 0.8316037848722124,\n",
       " 0.6774951709494023,\n",
       " 0.4720177587870853,\n",
       " 0.6230991088290447,\n",
       " 0.6237165464186689,\n",
       " 0.5696894552197588,\n",
       " 0.676003551555983,\n",
       " 0.6952416307355165,\n",
       " 0.790448839838485,\n",
       " 0.3695278961057817,\n",
       " 0.7422674557984406,\n",
       " 0.8081801390326147,\n",
       " 0.6937918426828911,\n",
       " 0.8040023144102623,\n",
       " 0.6656205935075004,\n",
       " 0.5235722618313791,\n",
       " 0.5750154326949435,\n",
       " 0.725815398081476,\n",
       " 0.6645953977402943,\n",
       " 0.5459169745721666,\n",
       " 0.7948678221213996,\n",
       " 0.4718216209118753,\n",
       " 0.5973362354949535,\n",
       " 0.6825162032929178,\n",
       " 0.5897281062616777,\n",
       " 0.6988075796605532,\n",
       " 0.5841129885133702,\n",
       " 0.5958319507298917,\n",
       " 0.4429418726149675,\n",
       " 0.531828496049326,\n",
       " 0.6447325711256455,\n",
       " 0.441242779316083,\n",
       " 0.40780308364486906,\n",
       " 0.6893718136199951,\n",
       " 0.6695218515528665,\n",
       " 0.5746955400408614,\n",
       " 0.5930378695484866,\n",
       " 0.5213142425851508,\n",
       " 0.8760192485147213,\n",
       " 0.5407873920664734,\n",
       " 0.5704009241002814,\n",
       " 0.6845750289020618,\n",
       " 0.7291107354756372,\n",
       " 0.48905657384452483,\n",
       " 0.7760831719397845,\n",
       " 0.7286384300707874,\n",
       " 0.6743682685068164,\n",
       " 0.8609301986387592,\n",
       " 0.6820879266402258,\n",
       " 0.45404097269043225,\n",
       " 0.5525543781302475,\n",
       " 0.4902614599513304,\n",
       " 0.5674280812355207,\n",
       " 0.6820324961069117,\n",
       " 0.48619217117346947,\n",
       " 0.7530427528298154,\n",
       " 0.7690111595250305,\n",
       " 0.7670443030686729,\n",
       " 0.40570469887729416,\n",
       " 0.6634436813137052,\n",
       " 0.782681195621418,\n",
       " 0.7011703146215111,\n",
       " 0.6462620906034253,\n",
       " 0.6080668188025938,\n",
       " 0.5027636218772866,\n",
       " 0.7798683699271585,\n",
       " 0.6015785571433234,\n",
       " 0.6408433130793347,\n",
       " 0.7524328556962072,\n",
       " 0.560161056016424,\n",
       " 0.6870294168719853,\n",
       " 0.7054281232249631,\n",
       " 0.627237270555833,\n",
       " 0.6467267473435122,\n",
       " 0.6252720807222084,\n",
       " 0.7566816492901659,\n",
       " 0.4923046122843667,\n",
       " 0.6027622380215338,\n",
       " 0.32868972990502066,\n",
       " 0.4984729225406781,\n",
       " 0.6862884870506738,\n",
       " 0.6344646847856688,\n",
       " 0.8790809242330715,\n",
       " 0.6675702773939058,\n",
       " 0.5591403951962741,\n",
       " 0.5125951934877364,\n",
       " 0.5265392496362541,\n",
       " 0.7371710990862863,\n",
       " 0.6862793027529046,\n",
       " 0.7314885057011705,\n",
       " 0.5759262966042715,\n",
       " 0.6502581650597636,\n",
       " 0.7510960141407239,\n",
       " 0.5415640351424315,\n",
       " 0.5427079609630061,\n",
       " 0.477771207142795,\n",
       " 0.7132234459924225,\n",
       " 0.3491529450098194,\n",
       " 0.7521523415525704,\n",
       " 0.657156039153332,\n",
       " 0.7014868500505582,\n",
       " 0.5225170732787006,\n",
       " 0.5030235122835698,\n",
       " 0.699322586444219,\n",
       " 0.5356350680800571,\n",
       " 0.6700421677332669,\n",
       " 0.7731464070420894,\n",
       " 0.4979351701147877,\n",
       " 0.4372283695071971,\n",
       " 0.49726674064742715,\n",
       " 0.6629073614182375,\n",
       " 0.5875590393646977,\n",
       " 0.5096660036921459,\n",
       " 0.7512172122135418,\n",
       " 0.5089842269861913,\n",
       " 0.5462304127871183,\n",
       " 0.8251572801696285,\n",
       " 0.7020006938903998,\n",
       " 0.6980106030700781,\n",
       " 0.6307429481303019,\n",
       " 0.7594709179125297,\n",
       " 0.6376413335187309,\n",
       " 0.7266820283406804,\n",
       " 0.7593980055852902,\n",
       " 0.7900782246433959,\n",
       " 0.432717633080572,\n",
       " 0.7428559567314843,\n",
       " 0.6265565821476078,\n",
       " 0.5933591462873715,\n",
       " 0.5038238448524965,\n",
       " 0.7562693263056285,\n",
       " 0.7736197253870392,\n",
       " 0.6906361480629295,\n",
       " 0.6625266579859609,\n",
       " 0.6513333789450554,\n",
       " 0.6415126941869298,\n",
       " 0.8246590120535748,\n",
       " 0.726246118837435,\n",
       " 0.6493346518403668,\n",
       " 0.8574700412397382,\n",
       " 0.6669076387959036,\n",
       " 0.5000919868648617,\n",
       " 0.754898017397138,\n",
       " 0.5522477598861372,\n",
       " 0.8357816405432787,\n",
       " 0.7097061530910331,\n",
       " 0.7839326192914593,\n",
       " 0.7763349382579011,\n",
       " 0.5297480942778098,\n",
       " 0.7207441625654236,\n",
       " 0.4834080321566344,\n",
       " 0.7073274180038207,\n",
       " 0.6670592368874528,\n",
       " 0.7277444398715827,\n",
       " 0.4927409397608554,\n",
       " 0.6503113413839277,\n",
       " 0.6530904049360389,\n",
       " 0.7048583590234829,\n",
       " 0.48736322015882066,\n",
       " 0.5776889559154802,\n",
       " 0.59812320834931,\n",
       " 0.5899113600666359,\n",
       " 0.6011486456374702,\n",
       " 0.5976036109504003,\n",
       " 0.44986107384878604,\n",
       " 0.7541703259236205,\n",
       " 0.7976850876106079,\n",
       " 0.8193640485538647,\n",
       " 0.7614050087307195,\n",
       " 0.696680320225601,\n",
       " 0.737189794340705,\n",
       " 0.5128796551359225,\n",
       " 0.6800537189471717,\n",
       " 0.6330393033629809,\n",
       " 0.6731362659740215,\n",
       " 0.7944960381137766,\n",
       " 0.7928011354593401,\n",
       " 0.6174815348881364,\n",
       " 0.4935935406106461,\n",
       " 0.7262919062558263,\n",
       " 0.5722352494550496,\n",
       " 0.4915135356982272,\n",
       " 0.7536499720328231,\n",
       " 0.6552588634784808,\n",
       " 0.7251642447058243,\n",
       " 0.7069072378966925,\n",
       " 0.691270925992877,\n",
       " 0.5506708828133575,\n",
       " 0.6342931539336588,\n",
       " 0.5561748578728269,\n",
       " 0.6573166526291477,\n",
       " 0.5197823697823506,\n",
       " 0.646622315095801,\n",
       " 0.5909091568642674,\n",
       " 0.346268863169119,\n",
       " 0.5388651014838473,\n",
       " 0.6057168454716422,\n",
       " 0.5800544057483331,\n",
       " 0.6588908836613931,\n",
       " 0.522684491188356,\n",
       " 0.4331745862844939,\n",
       " 0.5614299329994891,\n",
       " 0.5051309008091187,\n",
       " 0.4025884989582485,\n",
       " 0.6105896202739575,\n",
       " 0.5078338236642834,\n",
       " 0.6738009866292446,\n",
       " 0.573174902572345,\n",
       " 0.7470116913503455,\n",
       " 0.3763891851233904,\n",
       " 0.758324315408351,\n",
       " 0.5500701698122308,\n",
       " 0.6680857115806003,\n",
       " 0.5541226837744786,\n",
       " 0.550178889947697,\n",
       " 0.5717984565414449,\n",
       " 0.59741499532958,\n",
       " 0.6607141839576018,\n",
       " 0.48224330104622104,\n",
       " 0.7229186742445777,\n",
       " 0.7530249955609293,\n",
       " 0.7063075779180059,\n",
       " 0.7231062775588823,\n",
       " 0.7605081636578488,\n",
       " 0.8011758089815848,\n",
       " 0.7880532514111076,\n",
       " 0.6243158434149556,\n",
       " 0.6031468551270609,\n",
       " 0.5031096221521814,\n",
       " 0.7410797113712932,\n",
       " 0.6890722640689941,\n",
       " 0.5273946372164223,\n",
       " 0.5400166143613533,\n",
       " 0.5525582324200304,\n",
       " 0.6826652627018116,\n",
       " 0.6146046062246354,\n",
       " 0.5697681832942167,\n",
       " 0.6907137275386988,\n",
       " 0.6970786482458127,\n",
       " 0.7245041354947561,\n",
       " 0.6057432579429793,\n",
       " 0.5500506485532276,\n",
       " 0.7446390905444139,\n",
       " 0.5880190941659527,\n",
       " 0.3794616426394042,\n",
       " 0.7180625087840335,\n",
       " 0.7575956655611416,\n",
       " 0.4925993929781417,\n",
       " 0.6770200755189034,\n",
       " 0.5158264985918545,\n",
       " 0.5241218700253126,\n",
       " 0.58963904600225,\n",
       " 0.5191826674551252,\n",
       " 0.7947057378368112,\n",
       " 0.40380339905856383,\n",
       " 0.7143233627605999,\n",
       " 0.6242451619099312,\n",
       " 0.5532216425834947,\n",
       " 0.4568234739670278,\n",
       " 0.706534595552552,\n",
       " 0.733965902197302,\n",
       " 0.6839617142528303,\n",
       " 0.5321293831890891,\n",
       " 0.7136796908887684,\n",
       " 0.3814895992496985,\n",
       " 0.7377073177471192,\n",
       " 0.6576012133796223,\n",
       " 0.5952265987000098,\n",
       " 0.6772180613373462,\n",
       " 0.595238314977741,\n",
       " 0.5113570125848867,\n",
       " 0.4388864946933365,\n",
       " 0.654692683891892,\n",
       " 0.6357291483180334,\n",
       " 0.4401942955458621,\n",
       " 0.7577973573827539,\n",
       " 0.6380052969049733,\n",
       " 0.6311427192309137,\n",
       " 0.42354038276303735,\n",
       " 0.3652080293975427,\n",
       " 0.36577799809765515,\n",
       " 0.784682951698411,\n",
       " 0.6771222935017149,\n",
       " 0.5288453388004244,\n",
       " 0.6294955223951574,\n",
       " 0.7119181585710557,\n",
       " 0.8701707412992665,\n",
       " 0.6601421724428302,\n",
       " 0.6134316573571758,\n",
       " 0.647128050035154,\n",
       " 0.7507195507169302,\n",
       " 0.6481903059102299,\n",
       " 0.47786207140958475,\n",
       " 0.7862113653768458,\n",
       " 0.7023249999754284,\n",
       " 0.6703927052858462,\n",
       " 0.6627478251950092,\n",
       " 0.7682236969067764,\n",
       " 0.5745752044997314,\n",
       " 0.7006948937323969,\n",
       " 0.5328583224479473,\n",
       " 0.37114764593705746,\n",
       " 0.6017760070342179,\n",
       " 0.7713271038272832,\n",
       " 0.8068793011116349,\n",
       " 0.4326107007679115,\n",
       " 0.6487851603267539,\n",
       " 0.5463226606374437,\n",
       " 0.35776813732387835,\n",
       " 0.7951844051428478,\n",
       " 0.5093162794926156,\n",
       " 0.5161716985934434,\n",
       " 0.9052141979533569,\n",
       " 0.7289792039889231,\n",
       " 0.6625570505292089,\n",
       " 0.5014393753895905,\n",
       " 0.5443285018980033,\n",
       " 0.6647667035206781,\n",
       " 0.7747715678815946,\n",
       " 0.7563248463040695,\n",
       " 0.6254324215396396,\n",
       " 0.7169427459871225,\n",
       " 0.7023469137266398,\n",
       " 0.7312383114413707,\n",
       " 0.47443361606122003,\n",
       " 0.6930227056863235,\n",
       " 0.6407356756628976,\n",
       " 0.5817966142433528,\n",
       " 0.6195742653581606,\n",
       " 0.5326391912018172,\n",
       " 0.5643790627196046,\n",
       " 0.7135593628907191,\n",
       " 0.6082035402258801,\n",
       " 0.6717530841876822,\n",
       " 0.6222907920996452,\n",
       " 0.5252126500669079,\n",
       " 0.658644279154919,\n",
       " 0.6769323335294051,\n",
       " 0.7825824068215466,\n",
       " 0.7214117949997332,\n",
       " 0.679351115099819,\n",
       " 0.6811486881145632,\n",
       " 0.7432925462550424,\n",
       " 0.7548687722867579,\n",
       " 0.6962313317424567,\n",
       " 0.662652266916073,\n",
       " 0.8083996351496939,\n",
       " 0.6784930650918947,\n",
       " 0.7061448733719815,\n",
       " 0.7692107564581712,\n",
       " 0.8282938810836239,\n",
       " 0.7792088268014699,\n",
       " 0.8799868132059885,\n",
       " 0.6220666152880949,\n",
       " 0.7228487285219395,\n",
       " 0.7320644748920965,\n",
       " 0.4046913904152453,\n",
       " 0.7382237041538497,\n",
       " 0.780154896868956,\n",
       " 0.5190503473728931,\n",
       " 0.6891377469715041,\n",
       " 0.6203421584055558,\n",
       " 0.5933409599472115,\n",
       " 0.6583886676014234,\n",
       " 0.47438469596934196,\n",
       " 0.5878636904272837,\n",
       " 0.6721557304464183,\n",
       " 0.7373496045636032,\n",
       " 0.6781787519230688,\n",
       " 0.5015903567465398,\n",
       " 0.7339077573468662,\n",
       " 0.4977445151051607,\n",
       " 0.522759165529755,\n",
       " 0.44033804290909034,\n",
       " 0.6394493581699953,\n",
       " 0.4417507176255708,\n",
       " 0.4351947835160079,\n",
       " 0.6866187932920848,\n",
       " 0.5822985858807401,\n",
       " 0.5559372438177664,\n",
       " 0.3369857398705933,\n",
       " 0.6188672434503909,\n",
       " 0.5587092500373192,\n",
       " 0.5923693280784346,\n",
       " 0.5780021695732054,\n",
       " 0.5586801349664988,\n",
       " 0.6641686415584865,\n",
       " 0.595111493962331,\n",
       " 0.6162266767857496,\n",
       " 0.5830676812902637,\n",
       " 0.5716785556781635,\n",
       " 0.7786857558569749,\n",
       " 0.6497510764307615,\n",
       " 0.6954128423915558,\n",
       " 0.4551828948256871,\n",
       " 0.7408631896890965,\n",
       " 0.6271427404562531,\n",
       " 0.7855966012717543,\n",
       " 0.6681718794818828,\n",
       " 0.6801592645237737,\n",
       " 0.7083925746079137,\n",
       " 0.7452662377849362,\n",
       " 0.7359452431960494,\n",
       " 0.6626977522175725,\n",
       " 0.6718151509540458,\n",
       " 0.7461536965367881,\n",
       " 0.703275467386098,\n",
       " 0.7597358204206203,\n",
       " 0.5036609717693985,\n",
       " 0.45223237436492303,\n",
       " 0.5424441917853772,\n",
       " 0.7328163555954044,\n",
       " 0.5936356316111422,\n",
       " 0.6880884189136669,\n",
       " 0.6196452159220556,\n",
       " 0.5712335186901063,\n",
       " 0.6349791643944225,\n",
       " 0.7463379490970478,\n",
       " 0.6674029428908064,\n",
       " 0.6824687000864416,\n",
       " 0.6279075296667855,\n",
       " 0.4078904888553777,\n",
       " 0.6851413928765154,\n",
       " 0.4846717517954782,\n",
       " 0.5927205211299106,\n",
       " 0.6485378840160365,\n",
       " 0.31102891747119554,\n",
       " 0.6239178844027544,\n",
       " 0.5918464682924969,\n",
       " 0.6422607631302735,\n",
       " 0.5478741401758573,\n",
       " 0.64556079149132,\n",
       " 0.6664907366036499,\n",
       " 0.5687540453738624,\n",
       " 0.5653843120482789,\n",
       " 0.6159803860379192,\n",
       " 0.5640601863840395,\n",
       " 0.5522457498835096,\n",
       " 0.816583006820945,\n",
       " 0.6945211088266301,\n",
       " 0.5938514308279069,\n",
       " 0.7578720816975829,\n",
       " 0.7595506224053992,\n",
       " 0.6039277846160883,\n",
       " 0.6542739426910684,\n",
       " 0.7026055875948702,\n",
       " 0.3905345553859505,\n",
       " 0.6078624688490482,\n",
       " 0.4843625850015806,\n",
       " 0.6324148683840767,\n",
       " 0.4237625500652252,\n",
       " 0.47198284415153186,\n",
       " 0.5918903465422045,\n",
       " 0.6448104419796168,\n",
       " 0.6331491966993619,\n",
       " 0.40752249426801107,\n",
       " 0.5643837728327914,\n",
       " 0.6121220831661665,\n",
       " 0.5001971309040627,\n",
       " 0.5338150318438248,\n",
       " 0.4150527271201127,\n",
       " 0.5435936716104031,\n",
       " 0.5039413409202107,\n",
       " 0.34587863082988546,\n",
       " 0.6986838691701115,\n",
       " 0.35145481944142287,\n",
       " 0.6045544475644329,\n",
       " 0.567971294442223,\n",
       " 0.8204459643495441,\n",
       " 0.5981173221461105,\n",
       " 0.6609592401293922,\n",
       " 0.5481374844047531,\n",
       " 0.3649412448431111,\n",
       " 0.6537495917279033,\n",
       " 0.6646589243555386,\n",
       " 0.787327703792716,\n",
       " 0.6271357231723409,\n",
       " 0.41871642651693897,\n",
       " 0.7729610995528371,\n",
       " 0.7538808672267552,\n",
       " 0.7770586548591303,\n",
       " 0.5653604436586399,\n",
       " 0.6094796822601909,\n",
       " 0.6876568567342696,\n",
       " 0.7325563613859711,\n",
       " 0.43166502016096986,\n",
       " 0.8253020045996698,\n",
       " 0.7579805545387438,\n",
       " 0.7948875323939969,\n",
       " 0.7578704549057963,\n",
       " 0.7787927298569578,\n",
       " 0.5330225236000846,\n",
       " 0.8474035537354032,\n",
       " 0.6783299873280582,\n",
       " 0.7148377544315901,\n",
       " 0.6948994583006534,\n",
       " 0.772676389542609,\n",
       " 0.7200031106234117,\n",
       " 0.7362860832330324,\n",
       " 0.700581619556387,\n",
       " 0.40025764171394573,\n",
       " 0.5315794068740525,\n",
       " 0.7617936555007923,\n",
       " 0.5943496935978425,\n",
       " 0.7089829804727164,\n",
       " 0.712654028435028,\n",
       " 0.6198228614370855,\n",
       " 0.46426881818083054,\n",
       " 0.6689385526946925,\n",
       " 0.6414728369551034,\n",
       " 0.7063255029107763,\n",
       " 0.5749668200159093,\n",
       " 0.39951510914885474,\n",
       " 0.7480502719015396,\n",
       " 0.675785316906705,\n",
       " 0.54697493381671,\n",
       " 0.7506479684932323,\n",
       " 0.6596713373742438,\n",
       " 0.6873961567681275,\n",
       " 0.6335564257340873,\n",
       " 0.6968103937991255,\n",
       " 0.6786597051202818,\n",
       " 0.5390030968961688,\n",
       " 0.5766445097007509,\n",
       " 0.6969766228010577,\n",
       " 0.45133825949492246,\n",
       " 0.6987341314421637,\n",
       " 0.4982154762485268,\n",
       " 0.4986036410402711,\n",
       " 0.4728524380006519,\n",
       " 0.41107205940895175,\n",
       " 0.7909096873307011,\n",
       " 0.7003465410336097,\n",
       " 0.709076893852523,\n",
       " 0.8217658082959248,\n",
       " 0.7519768618887366,\n",
       " 0.8177406759782037,\n",
       " 0.6030502925777783,\n",
       " 0.7732937902590532,\n",
       " 0.5268609305557446,\n",
       " 0.7366650035591018,\n",
       " 0.6230061712397555,\n",
       " 0.5905622085316595,\n",
       " 0.4493863807433197,\n",
       " 0.7272891138545252,\n",
       " 0.7840643687662635,\n",
       " 0.787319620100542,\n",
       " 0.7022377657925001,\n",
       " 0.5928635450634209,\n",
       " 0.6348841986371933,\n",
       " 0.7162438290750875,\n",
       " 0.5232081054685627,\n",
       " 0.6258857533504945,\n",
       " 0.7814176756486327,\n",
       " 0.7157081515470818,\n",
       " 0.7469897261130429,\n",
       " 0.7471421237188367,\n",
       " 0.5166006837390013,\n",
       " 0.6817803186372933,\n",
       " 0.6691769591416368,\n",
       " 0.5075201936444942,\n",
       " 0.6891969896410288,\n",
       " 0.703534035574521,\n",
       " 0.6914330780245601,\n",
       " 0.7900408061605446,\n",
       " 0.7330058484534518,\n",
       " 0.37105658741409886,\n",
       " 0.4968609570710677,\n",
       " 0.7194053196151591,\n",
       " 0.6761635899702237,\n",
       " 0.5859845340745928,\n",
       " 0.5985006906585418,\n",
       " 0.73763588368198,\n",
       " 0.6998204804016503,\n",
       " 0.4170892135715506,\n",
       " 0.712015376268448,\n",
       " 0.49904640620180957,\n",
       " 0.5016967445218006,\n",
       " 0.3015165181512222,\n",
       " 0.6023130154104612,\n",
       " 0.6650602073672627,\n",
       " 0.7792869303498431,\n",
       " 0.6013483361287273,\n",
       " 0.5636374248270041,\n",
       " 0.7139263755024451,\n",
       " 0.7497191340072792,\n",
       " 0.39802399918205233,\n",
       " 0.5558942464942087,\n",
       " 0.7831380715428319,\n",
       " 0.6837686000546567,\n",
       " 0.3321669389820965,\n",
       " 0.7025086969755729,\n",
       " 0.7925841226435264,\n",
       " 0.748974538026064,\n",
       " 0.6301569958224352,\n",
       " 0.6580083927567135,\n",
       " 0.5673764275968347,\n",
       " 0.8075063217500917,\n",
       " 0.5084130916761711,\n",
       " 0.6467392219535735,\n",
       " 0.5407112487047662,\n",
       " 0.5885562033956142,\n",
       " 0.5060401810222643,\n",
       " 0.5742424227868849,\n",
       " 0.668193822736715,\n",
       " 0.7662639370433277,\n",
       " 0.8209170245797406,\n",
       " 0.6495505870022906,\n",
       " 0.6165174785592238,\n",
       " 0.7830399558412271,\n",
       " 0.7006532318369953,\n",
       " 0.693607828429494,\n",
       " 0.5690280210920791,\n",
       " 0.6066780231710892,\n",
       " 0.728219773934021,\n",
       " 0.6920665935903624,\n",
       " 0.7248805848426603,\n",
       " 0.6370504276455822,\n",
       " 0.6768930449143922,\n",
       " 0.5958279833279201,\n",
       " 0.5882994766665016,\n",
       " 0.5321181508254611,\n",
       " 0.7338764877157099,\n",
       " 0.7558266077586338,\n",
       " 0.6054193215490935,\n",
       " 0.645847029787938,\n",
       " 0.6284893075122147,\n",
       " 0.4488518572876136,\n",
       " 0.4589996327822805,\n",
       " 0.5281205181235361,\n",
       " 0.6434809821492651,\n",
       " 0.6089805093135097,\n",
       " 0.7367724934239707,\n",
       " 0.4420286609177537,\n",
       " 0.7197435418782241,\n",
       " 0.672145320093604,\n",
       " 0.5948720502091217,\n",
       " 0.48310803469730146,\n",
       " 0.630410171714311,\n",
       " 0.6948364672416972,\n",
       " 0.7174313958597601,\n",
       " 0.500276497201874,\n",
       " 0.6977208439813929,\n",
       " 0.6863269275562552,\n",
       " 0.799632000053037,\n",
       " 0.6761157540632285,\n",
       " 0.5937030283030775,\n",
       " 0.6937735777750824,\n",
       " 0.5421949233480357,\n",
       " 0.7728629894530696,\n",
       " 0.7817826473170112,\n",
       " 0.6659535293398542,\n",
       " 0.6871043342541672,\n",
       " 0.7709565894665,\n",
       " 0.8140623464597768,\n",
       " 0.6659384229374689,\n",
       " 0.8292258442809831,\n",
       " 0.33701075388393714,\n",
       " 0.29295016533281515,\n",
       " 0.5544940244298735,\n",
       " 0.6974917032295896,\n",
       " 0.7048725593411052,\n",
       " 0.5967985592069882,\n",
       " 0.9143996917484973,\n",
       " 0.4173606568838395,\n",
       " 0.5339131007289005,\n",
       " 0.7006587259688959,\n",
       " 0.6726337349363747,\n",
       " 0.5132282615695138,\n",
       " 0.7473782192770668,\n",
       " 0.8065088205347968,\n",
       " 0.544045039471996,\n",
       " 0.5957890188497735,\n",
       " 0.6702869264206262,\n",
       " 0.6125694404838065,\n",
       " 0.6547657473671815,\n",
       " 0.379400278609915,\n",
       " 0.7969903366827905,\n",
       " 0.7210717032186855,\n",
       " 0.4905320748011414,\n",
       " 0.8095054530032104,\n",
       " 0.7759006164554154,\n",
       " 0.6203546398515677,\n",
       " 0.4864518576287977,\n",
       " 0.5786464359024917,\n",
       " 0.6739273327153892,\n",
       " 0.6588551595976574,\n",
       " 0.6677740986209283,\n",
       " 0.7210463630503281,\n",
       " 0.6376627297609738,\n",
       " 0.2981311790351069,\n",
       " 0.5285765213946393,\n",
       " 0.7441177420762995,\n",
       " 0.7743504466155544,\n",
       " 0.6234788245785625,\n",
       " 0.7491668936179945,\n",
       " 0.6491307800731902,\n",
       " 0.6514894314640177,\n",
       " 0.5891161850517261,\n",
       " 0.8539635009697225,\n",
       " 0.5157015871798589,\n",
       " 0.4313226326917469,\n",
       " 0.5071710681170692,\n",
       " 0.627739931386079,\n",
       " 0.7627156441117635,\n",
       " 0.5751725721493361,\n",
       " 0.6390321115847787,\n",
       " 0.5790793904512255,\n",
       " 0.45454553957079435,\n",
       " 0.6823171125884937,\n",
       " 0.6732361403334945,\n",
       " 0.49373158283000906,\n",
       " 0.4727786940473363,\n",
       " 0.7728665208105732,\n",
       " 0.4314190773317626,\n",
       " 0.5995866568638467,\n",
       " 0.5437127647207451,\n",
       " 0.44208315577786095,\n",
       " 0.7586281404153818,\n",
       " 0.8292790781042992,\n",
       " 0.293546352957367,\n",
       " 0.670181273833282,\n",
       " 0.5692052329336318,\n",
       " 0.5687613019601319,\n",
       " 0.6700320396242423,\n",
       " 0.6813910391491251,\n",
       " 0.6432561399144676,\n",
       " 0.4662057672363866,\n",
       " 0.7769098990679357,\n",
       " 0.7550668110940137,\n",
       " 0.7282856253175148,\n",
       " 0.5921808209266433,\n",
       " 0.5619598857632351,\n",
       " 0.6192679258886222,\n",
       " 0.5640433173311814,\n",
       " 0.684931321918433,\n",
       " 0.6473868107513847,\n",
       " 0.62717735558297,\n",
       " 0.7135175060030843,\n",
       " 0.39971570742925855,\n",
       " 0.46649475499594184,\n",
       " 0.5563400541511384,\n",
       " 0.5251646479445368,\n",
       " 0.5791362639633408,\n",
       " 0.5562151219424355,\n",
       " 0.6665605055549138,\n",
       " 0.6730187349108141,\n",
       " 0.7211159298808938,\n",
       " 0.5693520913785335,\n",
       " 0.5508309958130079,\n",
       " 0.6105138234417714,\n",
       " 0.5826108188496406,\n",
       " 0.5402251606191866,\n",
       " 0.30979910353945644,\n",
       " 0.6878415351167644,\n",
       " 0.6965390806644106,\n",
       " 0.48815792562753113,\n",
       " 0.7474431158129213,\n",
       " 0.717719537770033,\n",
       " 0.6100473221000032,\n",
       " 0.7519803635255441,\n",
       " 0.7305975339251604,\n",
       " 0.6619727206105539,\n",
       " 0.7468569560154883,\n",
       " 0.5922673596505182,\n",
       " 0.6344017806802253,\n",
       " 0.5987509750933457,\n",
       " 0.6436656554216302,\n",
       " 0.4669621157829392,\n",
       " 0.6852701179790703,\n",
       " 0.5207193259169187,\n",
       " 0.6771822785470573,\n",
       " 0.7215435559464902,\n",
       " 0.7601952393138617,\n",
       " 0.3449569846958954,\n",
       " 0.5077908319539972,\n",
       " 0.5181861383581838,\n",
       " 0.5873622953150759,\n",
       " 0.6505472123742999,\n",
       " 0.42394708055393415,\n",
       " 0.5541868673257783,\n",
       " 0.8140561965487565,\n",
       " 0.7713520813586568,\n",
       " 0.7212336216560348,\n",
       " 0.7363928608192667,\n",
       " 0.8144568647137422,\n",
       " 0.7474981611461756,\n",
       " 0.38081059818894625,\n",
       " 0.6146721739183223,\n",
       " 0.5277883978337069,\n",
       " 0.6387471946658692,\n",
       " 0.7847580823381617,\n",
       " 0.6237042804078858,\n",
       " 0.6251322550803953,\n",
       " 0.6341081674224316,\n",
       " 0.7461790256976869,\n",
       " 0.6266787301777094,\n",
       " 0.6658176882212489,\n",
       " 0.5518388463098929,\n",
       " 0.49806596763609445,\n",
       " 0.7688249957809016,\n",
       " 0.6393698030526902,\n",
       " 0.3924288125870923,\n",
       " 0.4169407839036382,\n",
       " 0.6161357731396071,\n",
       " 0.734517825001641,\n",
       " 0.567623944967035,\n",
       " 0.5332327097898019,\n",
       " 0.5848242839292545,\n",
       " 0.3598094382332976,\n",
       " 0.7678042923559771,\n",
       " 0.6678403900892393,\n",
       " 0.6809109550817252,\n",
       " 0.6588471568055991,\n",
       " 0.5960786124878557,\n",
       " 0.5865809805936539,\n",
       " 0.7041480447029463,\n",
       " 0.7222060955629346,\n",
       " 0.3626547491227511,\n",
       " 0.646339519818769,\n",
       " 0.7080125761691027,\n",
       " 0.6389147991758776,\n",
       " 0.7953995760158976,\n",
       " 0.6317538212866667,\n",
       " 0.7639764461844323,\n",
       " 0.4833687258697519,\n",
       " 0.5646664715357844,\n",
       " 0.6325285902251682,\n",
       " 0.6183893047016442,\n",
       " 0.6309562852046671,\n",
       " 0.5156732522510552,\n",
       " 0.7813309403589577,\n",
       " 0.6672268755544215,\n",
       " 0.37155046609865167,\n",
       " 0.8050366334990112,\n",
       " 0.5474978750308445,\n",
       " 0.6962381796599368,\n",
       " 0.6300348399163167,\n",
       " 0.7208468999124185,\n",
       " 0.6289048204821498,\n",
       " 0.8267150258085828,\n",
       " 0.5787115927436742,\n",
       " 0.645439115676786,\n",
       " 0.40495307267516645,\n",
       " 0.718872647794409,\n",
       " 0.6042588892373506,\n",
       " 0.7338645300929594,\n",
       " 0.7082461395940355,\n",
       " 0.770266812663283,\n",
       " 0.7466614268647431,\n",
       " 0.6831125425842443,\n",
       " 0.45631473909841086,\n",
       " 0.7924990960596304,\n",
       " 0.5987671789319813,\n",
       " 0.7238360853907235,\n",
       " 0.6038883898767811,\n",
       " 0.729335784622003,\n",
       " 0.47092927887522823,\n",
       " 0.7389820011424827,\n",
       " 0.6292510896902128,\n",
       " 0.5628650299305533,\n",
       " 0.7296948905368863,\n",
       " 0.7167141626458232,\n",
       " 0.7264074926728257,\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_temp = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict_temp.append((Y_predict_lr[i] + Y_predict_rf[i]) / 2.0)\n",
    "Y_predict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict\n",
    "submit_score.to_csv('predict_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict_temp\n",
    "submit_score.to_csv('predict_result_nocnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(train_file,values = 'price', index=['lvl1','lvl2','lvl3'],columns=['type'],aggfunc=[min, max, np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
