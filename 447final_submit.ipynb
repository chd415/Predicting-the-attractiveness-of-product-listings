{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chd415/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "sns.set(color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                   \n",
    "import nltk                                         \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer  \n",
    "LA = np.linalg\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          \n",
    "from gensim.models.word2vec import Word2Vec                                  \n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data text\n",
    "def load_data(filename):\n",
    "    load_file = pd.read_csv(filename,delimiter=',', header=0,\n",
    "                        dtype={'name':str, 'lvl1':str, 'lvl2':str, 'lvl3':str, 'descrption':str, 'type':str})\n",
    "    load_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "    load_file.duplicated(subset=None, keep='first')\n",
    "    load_file.set_index('id', inplace = True)\n",
    "    load_file.head()\n",
    "    return load_file\n",
    "#print(len(train_file))\n",
    "def load_label(filename):\n",
    "    load_label = pd.read_csv(filename,delimiter=',', header=0)\n",
    "    load_label.columns = ['id', 'score']\n",
    "    load_label.duplicated(subset=None, keep='first')\n",
    "    load_label.set_index('id', inplace = True)\n",
    "    return load_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mathod(column):\n",
    "    values = []\n",
    "    indexs = []\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for count in range(len(train_file)):\n",
    "        value = train_file.get_value(count+1,column)\n",
    "        if value in values and value != np.nan:\n",
    "            continue\n",
    "        values.append(value)\n",
    "        indexs.append(len(values))\n",
    "    for j in range(len(indexs)):\n",
    "        mapping[values[j]] = indexs[j]\n",
    "    mapping[np.nan] = 0.0\n",
    "    return mapping\n",
    "#train_file['lvl3'] = train_file['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "#mapping_lvl3 = map_mathod('lvl3')\n",
    "#print(mapping_lvl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=5000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(max_features=5000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=1,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "    \n",
    "    return avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "temp = load_data('train_data.csv')\n",
    "temp['descrption'] = temp['descrption'].str.lower()\n",
    "description_X = temp.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "def test_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=5000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english',max_features=5000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=5,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "\n",
    "    tf_w_data = []\n",
    "    tf_data = tf_data.toarray()\n",
    "    i = 0\n",
    "    for row in splitted:\n",
    "        vec = [0 for i in range(50)]\n",
    "    \n",
    "        temp_tfidf = []\n",
    "        for val in tf_data[i]:\n",
    "            if val != 0:\n",
    "                temp_tfidf.append(val)\n",
    "    \n",
    "        count = 0\n",
    "        tf_idf_sum = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                count += 1\n",
    "                tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "                vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        if (tf_idf_sum == 0):\n",
    "            tf_w_data.append(vec)\n",
    "        else:\n",
    "            tf_w_data.append(vec/tf_idf_sum)\n",
    "#            vec = float(1/tf_idf_sum) * vec\n",
    "#        tf_w_data.append(vec)\n",
    "        i = i + 1\n",
    "    \n",
    "    return tf_w_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "temp['descrption'] = test_embedding(description_X)\n",
    "temp.descrption.head()\n",
    "#tfidf_n = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    #clean up data for lvl 1&2&3\n",
    "    filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "    filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].str.lower()\n",
    "    filename['name'] = filename['name'].str.lower()\n",
    "    \n",
    "    mapping_lvl1 = map_mathod('lvl1')\n",
    "    mapping_lvl2 = map_mathod('lvl2')\n",
    "    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "    filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "    filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    #normalize price\n",
    "    maxp = filename.price.max()\n",
    "    valuethred = 600.\n",
    "    filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    #hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "\n",
    "    #clean up type \n",
    "    mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "    filename['type'] = filename['type'].map(mapping_type)\n",
    "    \n",
    "    #clean up text\n",
    "    description_X = filename.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['descrption'] = text_embedding(description_X)\n",
    "    \n",
    "    name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['name'] = text_embedding(name_X)\n",
    "\n",
    "    return filename,mapping_lvl1,mapping_lvl2,mapping_lvl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = load_data('train_data.csv')\n",
    "cleaned_train,mapping_lvl1,mapping_lvl2,mapping_lvl3 = clean_data(train_file)\n",
    "train_score = load_label('train_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell\n",
    "cleaned_train['descrption'] = temp['descrption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lvl1</th>\n",
       "      <th>lvl2</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>descrption</th>\n",
       "      <th>price</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1112114392583155, -0.10216980137758785, 0.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.08458858515627475, -0.05196527924595608, 0....</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.4643172339004065, 0.18790752374167954, 0.3...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[-0.0821757186204195, 0.5118478735676035, 0.98...</td>\n",
       "      <td>0.024483</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.01779988112817095, 0.9916763860224322, -0....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[-0.5880512452567928, 0.6610294657875784, -0.2...</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.011420761180274627, 1.0104831246768726, -0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.09876202584912433, 0.5161218158293354, -0.4...</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.11495634096482878, 0.764323537272182, -0.2...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.09029336935944027, 0.24826423823833466, -0....</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.3112053220661787, -0.07709115697070956, 0.0...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0.014964479953050613, 0.05194679504515639, 0....</td>\n",
       "      <td>0.648317</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.044303586648311466, 0.2982949146946125, -0...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[-0.26340331617070156, 0.08268022217736062, 0....</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.6431926132904159, 0.08405322457353274, -0.0...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[-0.43231389040334356, -0.1330042877899749, -0...</td>\n",
       "      <td>0.036233</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.24886354403570293, 0.06856095297262073, 0.0...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[-0.19626282444818222, -0.2013505833381985, 0....</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.5326211040706507, -0.18841849907767028, -0....</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[0.9641661772297488, -0.5225564067562422, 0.09...</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  lvl1  lvl2  lvl3  \\\n",
       "id                                                                        \n",
       "1   [0.1112114392583155, -0.10216980137758785, 0.0...   1.0   1.0   1.0   \n",
       "2   [-0.4643172339004065, 0.18790752374167954, 0.3...   2.0   2.0   2.0   \n",
       "3   [-0.01779988112817095, 0.9916763860224322, -0....   3.0   3.0   3.0   \n",
       "4   [-0.011420761180274627, 1.0104831246768726, -0...   3.0   3.0   3.0   \n",
       "5   [-0.11495634096482878, 0.764323537272182, -0.2...   3.0   3.0   3.0   \n",
       "6   [0.3112053220661787, -0.07709115697070956, 0.0...   4.0   4.0   4.0   \n",
       "7   [-0.044303586648311466, 0.2982949146946125, -0...   2.0   5.0   5.0   \n",
       "8   [0.6431926132904159, 0.08405322457353274, -0.0...   5.0   6.0   6.0   \n",
       "9   [0.24886354403570293, 0.06856095297262073, 0.0...   6.0   7.0   7.0   \n",
       "10  [0.5326211040706507, -0.18841849907767028, -0....   6.0   7.0   8.0   \n",
       "\n",
       "                                           descrption     price  type  \n",
       "id                                                                     \n",
       "1   [0.08458858515627475, -0.05196527924595608, 0....  0.213333   1.0  \n",
       "2   [-0.0821757186204195, 0.5118478735676035, 0.98...  0.024483   1.0  \n",
       "3   [-0.5880512452567928, 0.6610294657875784, -0.2...  0.023500   1.0  \n",
       "4   [0.09876202584912433, 0.5161218158293354, -0.4...  0.029900   1.0  \n",
       "5   [0.09029336935944027, 0.24826423823833466, -0....  0.011333   1.0  \n",
       "6   [0.014964479953050613, 0.05194679504515639, 0....  0.648317   1.0  \n",
       "7   [-0.26340331617070156, 0.08268022217736062, 0....  1.000000   2.0  \n",
       "8   [-0.43231389040334356, -0.1330042877899749, -0...  0.036233   1.0  \n",
       "9   [-0.19626282444818222, -0.2013505833381985, 0....  0.041667   2.0  \n",
       "10  [0.9641661772297488, -0.5225564067562422, 0.09...  0.015800   1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train.head(10)\n",
    "#print(mapping_lvl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def rearrange(cleaned_data):\n",
    "    la = cleaned_data.lvl1.as_matrix(columns=None).tolist()\n",
    "    lb = cleaned_data.lvl2.as_matrix(columns=None).tolist()\n",
    "    lc = cleaned_data.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = la\n",
    "    X = np.column_stack((X,lb))\n",
    "    X = np.column_stack((X,lc))\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    \n",
    "    ld = cleaned_data.price.as_matrix(columns=None).tolist()\n",
    "    le = cleaned_data.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = np.column_stack((X,ld))\n",
    "    X = np.column_stack((X,le))\n",
    "\n",
    "    lf = cleaned_data.name.as_matrix(columns=None).tolist()\n",
    "    lg = cleaned_data.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "    s[8:] = 0.\n",
    "    new_lg = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "    sf[5:] = 0.\n",
    "    new_lf = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "    \n",
    "    X = np.column_stack((X,new_lf))\n",
    "    X = np.column_stack((X,new_lg))\n",
    "    X = X.tolist()\n",
    "    return X\n",
    "\n",
    "    \n",
    "    #print(len(X))\n",
    "X = rearrange(cleaned_train)\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "maxlen = 150\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, dtype='float32')\n",
    "#X_train = np.any(np.isnan(X_train))\n",
    "#X_train = np.all(np.isfinite(X_train))\n",
    "print(X_train[1400].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Classifier\n",
    "def linear_regression_classifier(train_x, train_y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    param_grid = {'alpha': [math.pow(10,-i) for i in range(11)]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = MultinomialNB(alpha = best_parameters['alpha'])  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    param_grid = {'n_neighbors': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = best_parameters['n_neighbors'], algorithm='kd_tree')\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(100,300,10))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    #for para, val in best_parameters.items():\n",
    "        #print para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "def feature_select(x,y):\n",
    "    clf = ExtraTreesClassifier()\n",
    "    clf = clf.fit(x, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for my own record\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['KNN','LR','RF','DT']    \n",
    "    classifiers = {\n",
    "                   'KNN':knn_classifier,\n",
    "                    'LR':logistic_regression_classifier,\n",
    "                    'RF':random_forest_classifier,\n",
    "                    'DT':decision_tree_classifier,                 \n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_validation = select_model.transform(X_validation)\n",
    "\n",
    "    result = []\n",
    "        \n",
    "    for classifier in test_classifiers:    \n",
    "        print('******************* %s ********************' % classifier)    \n",
    "        start_time = time.time()    \n",
    "        model = classifiers[classifier](X_train, y_train)   \n",
    "        print('training took %fs!' % (time.time() - start_time))    \n",
    "        predict = model.predict(X_validation)\n",
    "\n",
    "        precision = metrics.precision_score(y_validation, predict)    \n",
    "        recall = metrics.recall_score(y_validation, predict)    \n",
    "        print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))    \n",
    "        accuracy = metrics.accuracy_score(y_validation, predict)    \n",
    "        print('accuracy: %.2f%%' % (100 * accuracy))\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cnn model\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Embedding\n",
    "max_features = 5000\n",
    "maxlen = 150\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "#X_train, X_validation, y_train, y_validation\n",
    "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_validation), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "#x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_validation.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "#model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "score, acc = model.evaluate(X_validation, y_validation, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = load_data('test_data.csv')\n",
    "#cleaned_test = clean_data(test_file)\n",
    "X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_file\n",
    "#clean up data for lvl 1&2&3\n",
    "filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "filename['descrption'] = filename['descrption'].str.lower()\n",
    "filename['name'] = filename['name'].str.lower()\n",
    "    \n",
    "#    mapping_lvl1 = map_mathod('lvl1')\n",
    "#    mapping_lvl2 = map_mathod('lvl2')\n",
    "#    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    #normalize price\n",
    "maxp = filename.price.max()\n",
    "valuethred = 600.\n",
    "filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    #hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "\n",
    "    #clean up type \n",
    "mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "filename['type'] = filename['type'].map(mapping_type)\n",
    "    \n",
    "    #clean up text\n",
    "description_X = filename.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "filename['descrption'] = text_embedding(description_X)\n",
    "    \n",
    "name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "filename['name'] = text_embedding(name_X)\n",
    "\n",
    "#    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = filename\n",
    "lat = cleaned_test.lvl1.as_matrix(columns=None).tolist()\n",
    "lbt = cleaned_test.lvl2.as_matrix(columns=None).tolist()\n",
    "lct = cleaned_test.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "Xt = lat\n",
    "Xt = np.column_stack((Xt,lbt))\n",
    "Xt = np.column_stack((Xt,lct))\n",
    "\n",
    "enct = preprocessing.OneHotEncoder()\n",
    "enct.fit(Xt)\n",
    "Xt = enct.transform(Xt).toarray()\n",
    "\n",
    "ldt = cleaned_test.price.as_matrix(columns=None).tolist()\n",
    "let = cleaned_test.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "Xt = np.column_stack((Xt,ldt))\n",
    "Xt = np.column_stack((Xt,let))\n",
    "\n",
    "lft = cleaned_test.name.as_matrix(columns=None).tolist()\n",
    "lgt = cleaned_test.descrption.as_matrix(columns=None).tolist()\n",
    "lg_arrayt = np.vstack( lgt )\n",
    "lf_arrayt = np.vstack( lft )\n",
    "\n",
    "Ut, st, Vht = LA.svd(lg_arrayt, full_matrices=False)\n",
    "assert np.allclose(lg_arrayt, np.dot(Ut, np.dot(np.diag(st), Vht)))\n",
    "\n",
    "st[10:] = 0.\n",
    "new_lgt = np.dot(Ut, np.dot(np.diag(st), Vht))\n",
    "\n",
    "Uf, sf, Vhf = LA.svd(lf_arrayt, full_matrices=False)\n",
    "assert np.allclose(lf_arrayt, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "sf[5:] = 0.\n",
    "new_lft = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "\n",
    "\n",
    "#X = la\n",
    "#X = np.column_stack((X,lb))\n",
    "#X = np.column_stack((X,lc))\n",
    "#X = np.column_stack((X,ld))\n",
    "#X = np.column_stack((X,le))\n",
    "Xt = np.column_stack((Xt,new_lft))\n",
    "Xt = np.column_stack((Xt,new_lgt))\n",
    "Xt = Xt.tolist()\n",
    "#cleaned_test = Xt\n",
    "#return X,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lvl1</th>\n",
       "      <th>lvl2</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>descrption</th>\n",
       "      <th>price</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18142</th>\n",
       "      <td>[0.2910505139401981, 0.04427307163963893, 0.00...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>[0.4974043408189626, 0.03650149753358325, 0.22...</td>\n",
       "      <td>0.081667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18143</th>\n",
       "      <td>[-0.0008019954548217356, 0.2919247818063013, 0...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>[-0.5066361925659739, 0.24731269746790735, 0.8...</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18144</th>\n",
       "      <td>[0.032295664971960444, 0.647023093678789, 0.00...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>[-0.3520956976743251, 0.24306542413850205, 0.6...</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18145</th>\n",
       "      <td>[0.17624174249875876, 0.02362049308915933, 0.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[-0.26399251909460875, -0.02353048401128035, 0...</td>\n",
       "      <td>0.196667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18146</th>\n",
       "      <td>[0.27825695250390303, 0.4065230344939563, -0.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[0.419692053925246, 0.010667627832541863, 0.21...</td>\n",
       "      <td>0.191333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18147</th>\n",
       "      <td>[-0.08950607349666265, 0.5613701820660096, 0.2...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>[-0.144322803347475, -0.11823871918022633, 0.8...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18148</th>\n",
       "      <td>[-0.0010690220321218173, 0.9767497105834385, -...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>[0.1553927365069588, 0.5552089061765444, 1.336...</td>\n",
       "      <td>0.028950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>[0.22926618655522665, 0.044200843976189695, -0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-0.24068953905953094, 0.07620786027982832, 0....</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18150</th>\n",
       "      <td>[0.14598204439076093, 0.3737279163100399, 0.09...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>[0.04258049875497818, 0.9404868662357331, 2.60...</td>\n",
       "      <td>0.057550</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>[-0.029650477692484856, 0.31232431282599765, 0...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>[-0.5886171401611396, 0.20962297916412354, 0.6...</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  lvl1  lvl2   lvl3  \\\n",
       "id                                                                            \n",
       "18142  [0.2910505139401981, 0.04427307163963893, 0.00...   5.0  21.0  166.0   \n",
       "18143  [-0.0008019954548217356, 0.2919247818063013, 0...   9.0  32.0   47.0   \n",
       "18144  [0.032295664971960444, 0.647023093678789, 0.00...   9.0  32.0   68.0   \n",
       "18145  [0.17624174249875876, 0.02362049308915933, 0.0...   1.0  25.0   40.0   \n",
       "18146  [0.27825695250390303, 0.4065230344939563, -0.0...   1.0  47.0  106.0   \n",
       "18147  [-0.08950607349666265, 0.5613701820660096, 0.2...   2.0  52.0  181.0   \n",
       "18148  [-0.0010690220321218173, 0.9767497105834385, -...   2.0   2.0   84.0   \n",
       "18149  [0.22926618655522665, 0.044200843976189695, -0...   1.0   1.0    1.0   \n",
       "18150  [0.14598204439076093, 0.3737279163100399, 0.09...   2.0  19.0  177.0   \n",
       "18151  [-0.029650477692484856, 0.31232431282599765, 0...   8.0  18.0   19.0   \n",
       "\n",
       "                                              descrption     price  type  \n",
       "id                                                                        \n",
       "18142  [0.4974043408189626, 0.03650149753358325, 0.22...  0.081667   2.0  \n",
       "18143  [-0.5066361925659739, 0.24731269746790735, 0.8...  0.005383   1.0  \n",
       "18144  [-0.3520956976743251, 0.24306542413850205, 0.6...  0.041783   1.0  \n",
       "18145  [-0.26399251909460875, -0.02353048401128035, 0...  0.196667   2.0  \n",
       "18146  [0.419692053925246, 0.010667627832541863, 0.21...  0.191333   1.0  \n",
       "18147  [-0.144322803347475, -0.11823871918022633, 0.8...  0.013333   1.0  \n",
       "18148  [0.1553927365069588, 0.5552089061765444, 1.336...  0.028950   1.0  \n",
       "18149  [-0.24068953905953094, 0.07620786027982832, 0....  0.017333   1.0  \n",
       "18150  [0.04258049875497818, 0.9404868662357331, 2.60...  0.057550   1.0  \n",
       "18151  [-0.5886171401611396, 0.20962297916412354, 0.6...  0.015500   1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned_test = clean_test(test_file)\n",
    "#cleaned_test = filename\n",
    "cleaned_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing import sequence\n",
    "#maxlen = 150\n",
    "X_test = rearrange(cleaned_test)\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=maxlen, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "training took 0.186404s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   30.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 33.509690s!\n",
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['LR','RF']    \n",
    "    classifiers = {\n",
    "                   'LR':logistic_regression_classifier,\n",
    "                   'RF':random_forest_classifier         \n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "#    X_test = rearrange(Xt)\n",
    "#    X_test = Xt\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_test = select_model.transform(X_test)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = classifiers['LR'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_lr = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['RF'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_rf = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    Y_predict = []\n",
    "    for i in range(len(Y_predict_rf)):\n",
    "        Y_predict.append((Y_predict_lr[i] + Y_predict_rf[i]) / 2.0)\n",
    "        \n",
    "#    for classifier in test_classifiers:    \n",
    "#        print('******************* %s ********************' % classifier)    \n",
    "#        start_time = time.time()    \n",
    "#        model = classifiers[classifier](X_train, y_train)   \n",
    "#        print('training took %fs!' % (time.time() - start_time))    \n",
    "#        Y_predict = model.predict_proba(X_test)[:,1]\n",
    "#        print('predict finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8753618815616444,\n",
       " 0.558157957130386,\n",
       " 0.7346397451325362,\n",
       " 0.7982709246250408,\n",
       " 0.7529775888185125,\n",
       " 0.7086165942435747,\n",
       " 0.6804558254034009,\n",
       " 0.6014131820887256,\n",
       " 0.6652423409275128,\n",
       " 0.6613623412731372,\n",
       " 0.8246220680117915,\n",
       " 0.5764272009459317,\n",
       " 0.6230980745846861,\n",
       " 0.6070260755547547,\n",
       " 0.8047708291157444,\n",
       " 0.34770011105394566,\n",
       " 0.8514530020137331,\n",
       " 0.5993519697899268,\n",
       " 0.6922362632819707,\n",
       " 0.6108998325067976,\n",
       " 0.6892892224826154,\n",
       " 0.7999942644408942,\n",
       " 0.6915838272017485,\n",
       " 0.7143226026828122,\n",
       " 0.8376872702329736,\n",
       " 0.7807174541017853,\n",
       " 0.7266510912656121,\n",
       " 0.8139099078963132,\n",
       " 0.765441215789599,\n",
       " 0.728271397619115,\n",
       " 0.5955901480379577,\n",
       " 0.7595465622689227,\n",
       " 0.8227441110128091,\n",
       " 0.5779241489665796,\n",
       " 0.5118353720042099,\n",
       " 0.43855214586064306,\n",
       " 0.6971970931101545,\n",
       " 0.7138950492347378,\n",
       " 0.54285696358531,\n",
       " 0.8229581545573905,\n",
       " 0.5699621170457707,\n",
       " 0.6636332263306659,\n",
       " 0.8302194771837841,\n",
       " 0.6763691782162033,\n",
       " 0.6251543309966527,\n",
       " 0.8120451729462754,\n",
       " 0.6685703431075003,\n",
       " 0.3962351304266314,\n",
       " 0.7938722603776112,\n",
       " 0.6716006281630822,\n",
       " 0.7372898803010035,\n",
       " 0.6549683714093444,\n",
       " 0.8244123463232303,\n",
       " 0.5046215330790293,\n",
       " 0.37863330760121405,\n",
       " 0.591868694750988,\n",
       " 0.31637416361019194,\n",
       " 0.7422074677144439,\n",
       " 0.5348048801385803,\n",
       " 0.7770609982747712,\n",
       " 0.6304520219654107,\n",
       " 0.44748530774477846,\n",
       " 0.6261533568187234,\n",
       " 0.712329011128979,\n",
       " 0.6015340797982921,\n",
       " 0.8746392853811104,\n",
       " 0.5464439180812779,\n",
       " 0.7422044224819193,\n",
       " 0.7692379281428381,\n",
       " 0.7653464984149787,\n",
       " 0.381138734130423,\n",
       " 0.5952236729284799,\n",
       " 0.49613335944084186,\n",
       " 0.5756569109153167,\n",
       " 0.8522077913672668,\n",
       " 0.6345577245805865,\n",
       " 0.7339514795423018,\n",
       " 0.7761820684865228,\n",
       " 0.4944138237457797,\n",
       " 0.6382177218466627,\n",
       " 0.7382604535796107,\n",
       " 0.48991826523768167,\n",
       " 0.4136393576956644,\n",
       " 0.6973318013463519,\n",
       " 0.7036812181505572,\n",
       " 0.6356737068094738,\n",
       " 0.7481740060684399,\n",
       " 0.5035056506348969,\n",
       " 0.8099263880026075,\n",
       " 0.6796364560912993,\n",
       " 0.5389921880505024,\n",
       " 0.6293889681009609,\n",
       " 0.6802265543894653,\n",
       " 0.7388516782707273,\n",
       " 0.8179224662812287,\n",
       " 0.7450876517898735,\n",
       " 0.8355962327443093,\n",
       " 0.7988653210405837,\n",
       " 0.8051238651014009,\n",
       " 0.7457339984517775,\n",
       " 0.8103859408474785,\n",
       " 0.6892573509456617,\n",
       " 0.7037030312019595,\n",
       " 0.7924060403190363,\n",
       " 0.7227271586037101,\n",
       " 0.640928175743306,\n",
       " 0.7383297993268545,\n",
       " 0.8053593275476234,\n",
       " 0.6515388927536714,\n",
       " 0.8287318374000878,\n",
       " 0.783388071881721,\n",
       " 0.702142858330493,\n",
       " 0.7763493566377004,\n",
       " 0.5880056786165633,\n",
       " 0.8389980691208436,\n",
       " 0.6980012673776358,\n",
       " 0.35052201098140556,\n",
       " 0.7407756421507741,\n",
       " 0.6280316828647894,\n",
       " 0.6434046796524828,\n",
       " 0.5385552207588792,\n",
       " 0.6801985090198821,\n",
       " 0.7922537709520208,\n",
       " 0.47908918365407094,\n",
       " 0.6179085825870978,\n",
       " 0.7962004524020123,\n",
       " 0.762732528015849,\n",
       " 0.6553107732217803,\n",
       " 0.6701680739777462,\n",
       " 0.6931899000105443,\n",
       " 0.8301156007568695,\n",
       " 0.8725948887805925,\n",
       " 0.77231505825416,\n",
       " 0.7041839973666565,\n",
       " 0.8313022628596723,\n",
       " 0.8172699759154205,\n",
       " 0.8319067352271319,\n",
       " 0.7276906022574963,\n",
       " 0.36506569594648197,\n",
       " 0.7034937524739747,\n",
       " 0.6574658155502544,\n",
       " 0.6539080885622046,\n",
       " 0.8266944025305873,\n",
       " 0.684423911879867,\n",
       " 0.7110968732211216,\n",
       " 0.37022231048036514,\n",
       " 0.8216320396756664,\n",
       " 0.9190726560585326,\n",
       " 0.5918827347447957,\n",
       " 0.9100751675433487,\n",
       " 0.7074519091779179,\n",
       " 0.7005160259749387,\n",
       " 0.4589939956168055,\n",
       " 0.8054751111317118,\n",
       " 0.6931358038746569,\n",
       " 0.5676515523746243,\n",
       " 0.8586084553468103,\n",
       " 0.5593344773319708,\n",
       " 0.49470598246358277,\n",
       " 0.5807597546544253,\n",
       " 0.6382542470856978,\n",
       " 0.7465782207725293,\n",
       " 0.42430083763442206,\n",
       " 0.693462427259511,\n",
       " 0.38916954129178055,\n",
       " 0.48526838481076273,\n",
       " 0.6218371073722485,\n",
       " 0.4755328307278025,\n",
       " 0.4960410785071223,\n",
       " 0.6365632230622822,\n",
       " 0.7738812953568397,\n",
       " 0.5891198911952418,\n",
       " 0.4010325871945312,\n",
       " 0.4252849860965462,\n",
       " 0.848988540780395,\n",
       " 0.6175184036497956,\n",
       " 0.6844486540403915,\n",
       " 0.8004788583220122,\n",
       " 0.814082803095802,\n",
       " 0.5225667099129501,\n",
       " 0.828729840331865,\n",
       " 0.786717337426333,\n",
       " 0.5857482420830317,\n",
       " 0.7840575659321485,\n",
       " 0.7200205573208756,\n",
       " 0.47907248517709533,\n",
       " 0.5720279909258827,\n",
       " 0.6289640071958528,\n",
       " 0.617382554981784,\n",
       " 0.5973961281720354,\n",
       " 0.5233034719714938,\n",
       " 0.8392769161577446,\n",
       " 0.8274155138760475,\n",
       " 0.7624189681075387,\n",
       " 0.4685366799763632,\n",
       " 0.7506965459230114,\n",
       " 0.7423392924274628,\n",
       " 0.5900460007359318,\n",
       " 0.6842415880462985,\n",
       " 0.7120179236467346,\n",
       " 0.4078099323016221,\n",
       " 0.8216101415733458,\n",
       " 0.6682735947096299,\n",
       " 0.7157922303036052,\n",
       " 0.7755395271528109,\n",
       " 0.683046081851318,\n",
       " 0.6978234321043348,\n",
       " 0.6742187776589019,\n",
       " 0.5860476434050178,\n",
       " 0.7005038609186628,\n",
       " 0.6284281830194184,\n",
       " 0.7117455575333761,\n",
       " 0.3048034325979905,\n",
       " 0.611515847305864,\n",
       " 0.2989267428446164,\n",
       " 0.6242577891424257,\n",
       " 0.5907648509844837,\n",
       " 0.6132791849398785,\n",
       " 0.8003756131591153,\n",
       " 0.6401237986881837,\n",
       " 0.5952904578456821,\n",
       " 0.6825443531029114,\n",
       " 0.5383938100041197,\n",
       " 0.8288723360010497,\n",
       " 0.6198776325659867,\n",
       " 0.7334163682374746,\n",
       " 0.8173372163900959,\n",
       " 0.7906799835055892,\n",
       " 0.7102841448412007,\n",
       " 0.6940267778703852,\n",
       " 0.6124178287698554,\n",
       " 0.4345518176181181,\n",
       " 0.7094421704035081,\n",
       " 0.5100362010205389,\n",
       " 0.7223398291343646,\n",
       " 0.66071077629668,\n",
       " 0.8242708135901367,\n",
       " 0.3226154872744571,\n",
       " 0.5528809277267883,\n",
       " 0.6225638743327855,\n",
       " 0.5529853656610579,\n",
       " 0.766061658023134,\n",
       " 0.6211795089175661,\n",
       " 0.5950792251197905,\n",
       " 0.44360699793059244,\n",
       " 0.3095382227205431,\n",
       " 0.7302005219035155,\n",
       " 0.6592236353584034,\n",
       " 0.49672720141201154,\n",
       " 0.6624026580955421,\n",
       " 0.6592993097579449,\n",
       " 0.6975497498861175,\n",
       " 0.8815061029000257,\n",
       " 0.8148086387682862,\n",
       " 0.775831760931172,\n",
       " 0.7612916488214055,\n",
       " 0.6126351377346744,\n",
       " 0.8065244019085518,\n",
       " 0.7167362179188299,\n",
       " 0.8576221090454623,\n",
       " 0.8202077199930295,\n",
       " 0.6021708556549875,\n",
       " 0.8102274934837841,\n",
       " 0.8340765000663317,\n",
       " 0.6193036722151397,\n",
       " 0.4932492701471224,\n",
       " 0.7777001263558134,\n",
       " 0.8276761834227642,\n",
       " 0.7701138352601301,\n",
       " 0.657373308400494,\n",
       " 0.7898883068658947,\n",
       " 0.5955729308555042,\n",
       " 0.6988864811915868,\n",
       " 0.8929560008719502,\n",
       " 0.8062293999884227,\n",
       " 0.9089948602763055,\n",
       " 0.8770329486353048,\n",
       " 0.4907262558288206,\n",
       " 0.7993001173141386,\n",
       " 0.3560395649190084,\n",
       " 0.7796741256696607,\n",
       " 0.7137285722888216,\n",
       " 0.7378480284656741,\n",
       " 0.7041815179788214,\n",
       " 0.5081811405210483,\n",
       " 0.7917097454426538,\n",
       " 0.644284543308406,\n",
       " 0.6958129830245732,\n",
       " 0.7514652842363173,\n",
       " 0.6743276708802431,\n",
       " 0.6577546161683219,\n",
       " 0.8197191617712973,\n",
       " 0.6777703493838985,\n",
       " 0.7314174030254248,\n",
       " 0.48723358712828213,\n",
       " 0.5693021683898984,\n",
       " 0.6869475878031278,\n",
       " 0.7407807592474085,\n",
       " 0.6252831127022204,\n",
       " 0.5986970707558961,\n",
       " 0.4209810097817602,\n",
       " 0.7828069723109031,\n",
       " 0.8342488076961458,\n",
       " 0.9117139238464693,\n",
       " 0.7426447370451632,\n",
       " 0.7202011845434637,\n",
       " 0.6683632347894098,\n",
       " 0.6380882593838317,\n",
       " 0.7764951099000101,\n",
       " 0.7703041386318914,\n",
       " 0.8573590932681283,\n",
       " 0.7489712671445062,\n",
       " 0.7538450813743179,\n",
       " 0.5953846496560347,\n",
       " 0.6060780403914674,\n",
       " 0.8431375208339567,\n",
       " 0.6441453502160792,\n",
       " 0.6034974097002306,\n",
       " 0.8776433240407425,\n",
       " 0.7992178909948662,\n",
       " 0.7758995862557823,\n",
       " 0.665642868414489,\n",
       " 0.7738117654588081,\n",
       " 0.5651451987651008,\n",
       " 0.6852182212714204,\n",
       " 0.5939726482389365,\n",
       " 0.8213887847799894,\n",
       " 0.6265390358634184,\n",
       " 0.6454977256994865,\n",
       " 0.6685107871785336,\n",
       " 0.512974254173261,\n",
       " 0.7616936596282564,\n",
       " 0.5140950220601338,\n",
       " 0.6794060315359153,\n",
       " 0.6558825461363725,\n",
       " 0.4600880304895949,\n",
       " 0.4048471707234963,\n",
       " 0.3907946346426538,\n",
       " 0.40225658338032355,\n",
       " 0.5591329915584586,\n",
       " 0.39850587567362705,\n",
       " 0.4914774203455067,\n",
       " 0.6843419808850099,\n",
       " 0.4060394381962611,\n",
       " 0.7296864053875791,\n",
       " 0.4836317003117849,\n",
       " 0.7136139285685001,\n",
       " 0.6346329899849992,\n",
       " 0.7353279712413461,\n",
       " 0.7574388714307043,\n",
       " 0.5089782228673961,\n",
       " 0.5600744742980674,\n",
       " 0.6356912707783003,\n",
       " 0.838021363853459,\n",
       " 0.6275198038232555,\n",
       " 0.7720404103030842,\n",
       " 0.7417120145093798,\n",
       " 0.8018007089004844,\n",
       " 0.856859780623356,\n",
       " 0.6518082445795486,\n",
       " 0.8678605972884931,\n",
       " 0.8301006070073996,\n",
       " 0.6086280091514528,\n",
       " 0.8777520797808438,\n",
       " 0.467702116218072,\n",
       " 0.6060061972270256,\n",
       " 0.6844199014347798,\n",
       " 0.448789098486147,\n",
       " 0.6056220528768677,\n",
       " 0.7489360722556112,\n",
       " 0.711545328115609,\n",
       " 0.7666018633072447,\n",
       " 0.5107230709547473,\n",
       " 0.778929743544045,\n",
       " 0.6230267903201266,\n",
       " 0.663008795718186,\n",
       " 0.6816060142493864,\n",
       " 0.671762868824259,\n",
       " 0.694012199447854,\n",
       " 0.625610638063145,\n",
       " 0.25649868383870067,\n",
       " 0.8271231745600598,\n",
       " 0.8016985161079455,\n",
       " 0.5385888066652238,\n",
       " 0.6737183215754192,\n",
       " 0.5292688216684742,\n",
       " 0.7052556671705443,\n",
       " 0.6150105339698121,\n",
       " 0.5228541397817456,\n",
       " 0.7489080831152319,\n",
       " 0.5218184944226757,\n",
       " 0.6926235265122674,\n",
       " 0.4798624247427725,\n",
       " 0.5367216276082449,\n",
       " 0.5608238322079317,\n",
       " 0.5995489614342657,\n",
       " 0.7637966496215507,\n",
       " 0.7501748690181183,\n",
       " 0.7557875607367193,\n",
       " 0.7043934278514699,\n",
       " 0.43745913717789364,\n",
       " 0.7255529540023273,\n",
       " 0.7319466179087653,\n",
       " 0.6284585839392233,\n",
       " 0.6976453930527009,\n",
       " 0.609779228285715,\n",
       " 0.6648667210776588,\n",
       " 0.3250739753580294,\n",
       " 0.6693776876988906,\n",
       " 0.6835346937571771,\n",
       " 0.45773531999270944,\n",
       " 0.659652365045989,\n",
       " 0.5978139578285239,\n",
       " 0.6868160676247135,\n",
       " 0.34633753049394334,\n",
       " 0.44893884335933326,\n",
       " 0.4569607496106516,\n",
       " 0.7356653532163514,\n",
       " 0.6801295567619563,\n",
       " 0.5958457444630969,\n",
       " 0.706885726138127,\n",
       " 0.7637616320597607,\n",
       " 0.9022288724324611,\n",
       " 0.6142779594928319,\n",
       " 0.7822582847895228,\n",
       " 0.7449980660035687,\n",
       " 0.7818104694071788,\n",
       " 0.5028118879681618,\n",
       " 0.3649447544639306,\n",
       " 0.7527651475750794,\n",
       " 0.652776562106944,\n",
       " 0.7712350202555331,\n",
       " 0.8388071571052208,\n",
       " 0.8972057047698501,\n",
       " 0.5928064599934743,\n",
       " 0.8053653062724762,\n",
       " 0.6335603154714384,\n",
       " 0.464834451310984,\n",
       " 0.5188739516982022,\n",
       " 0.8859085907068138,\n",
       " 0.8320056143417898,\n",
       " 0.4236442851562954,\n",
       " 0.7764542047900669,\n",
       " 0.7042115546928444,\n",
       " 0.47046816157560134,\n",
       " 0.7718814480665837,\n",
       " 0.629761285519923,\n",
       " 0.6339211044058168,\n",
       " 0.7284528507107122,\n",
       " 0.6840509973426587,\n",
       " 0.7368065624915372,\n",
       " 0.6156696192281903,\n",
       " 0.6254892739389983,\n",
       " 0.67415665902093,\n",
       " 0.7111033407912294,\n",
       " 0.840762670772365,\n",
       " 0.8195232313430834,\n",
       " 0.686930988314651,\n",
       " 0.8173854653295154,\n",
       " 0.7649622350627634,\n",
       " 0.452274935758546,\n",
       " 0.7284119462690188,\n",
       " 0.8041265818168641,\n",
       " 0.5556862938316325,\n",
       " 0.7074266392667721,\n",
       " 0.5667847895917963,\n",
       " 0.5396482024710231,\n",
       " 0.781589049309833,\n",
       " 0.5688376918344367,\n",
       " 0.7850604803755963,\n",
       " 0.6085433314605634,\n",
       " 0.7536612572125949,\n",
       " 0.6436193514253657,\n",
       " 0.6381534727151766,\n",
       " 0.7071799760024933,\n",
       " 0.8270659162618903,\n",
       " 0.48699879700034576,\n",
       " 0.8575336709655259,\n",
       " 0.8795911905959627,\n",
       " 0.7516312574568442,\n",
       " 0.638231250096604,\n",
       " 0.8065031895228576,\n",
       " 0.8517471722126286,\n",
       " 0.8263235369831883,\n",
       " 0.8522080652834606,\n",
       " 0.7308072944658672,\n",
       " 0.8074328171407923,\n",
       " 0.7256253583885868,\n",
       " 0.8447863081156153,\n",
       " 0.8325399473292219,\n",
       " 0.8752110158102618,\n",
       " 0.8220126090664526,\n",
       " 0.3180200531647895,\n",
       " 0.8218320790505566,\n",
       " 0.5915080019362301,\n",
       " 0.6140157382329767,\n",
       " 0.7050832529485382,\n",
       " 0.652349513395423,\n",
       " 0.7227522698907782,\n",
       " 0.5803504665753364,\n",
       " 0.6440426222700547,\n",
       " 0.5379150824891288,\n",
       " 0.73912037434466,\n",
       " 0.5790809590904973,\n",
       " 0.8033329911232256,\n",
       " 0.5688012136542937,\n",
       " 0.738295291965605,\n",
       " 0.605152781829082,\n",
       " 0.45233985039573577,\n",
       " 0.3799097745869479,\n",
       " 0.7814966534423858,\n",
       " 0.5354904690066324,\n",
       " 0.42304684457073866,\n",
       " 0.7475131752981518,\n",
       " 0.5005909025167116,\n",
       " 0.7301230011492617,\n",
       " 0.32737183688799154,\n",
       " 0.6209890354779891,\n",
       " 0.5001350764650325,\n",
       " 0.5766445870148809,\n",
       " 0.580720853162168,\n",
       " 0.6103197947039272,\n",
       " 0.7005420120343753,\n",
       " 0.562765939218727,\n",
       " 0.7263430155080393,\n",
       " 0.5101651372422837,\n",
       " 0.5634214577253877,\n",
       " 0.7215920243637259,\n",
       " 0.6995589011988133,\n",
       " 0.6604904345352351,\n",
       " 0.5212739941183308,\n",
       " 0.6342017711280793,\n",
       " 0.6875793912522989,\n",
       " 0.8131000751218178,\n",
       " 0.8244307115329208,\n",
       " 0.8298872982920563,\n",
       " 0.7391127620842253,\n",
       " 0.6960458621940026,\n",
       " 0.7390218887303219,\n",
       " 0.8042823389942779,\n",
       " 0.5536298819818718,\n",
       " 0.8146089238341578,\n",
       " 0.6846134137219977,\n",
       " 0.9011136981042704,\n",
       " 0.5690139531822057,\n",
       " 0.4428127778182151,\n",
       " 0.6421344595292713,\n",
       " 0.7688951630380334,\n",
       " 0.7316695740428873,\n",
       " 0.7456901612329092,\n",
       " 0.7400532475475384,\n",
       " 0.7269144271988175,\n",
       " 0.6659961003018944,\n",
       " 0.8424631147785162,\n",
       " 0.746310388067196,\n",
       " 0.8303763388052208,\n",
       " 0.5079982086366126,\n",
       " 0.5248142198482754,\n",
       " 0.7574074737818147,\n",
       " 0.5381601548899682,\n",
       " 0.5888784770101616,\n",
       " 0.619888024091068,\n",
       " 0.5309579755440406,\n",
       " 0.6591530776840843,\n",
       " 0.6918711746999733,\n",
       " 0.725859215088984,\n",
       " 0.6694325006447548,\n",
       " 0.6720509880217747,\n",
       " 0.5856477895065872,\n",
       " 0.6630090223822132,\n",
       " 0.6677456541816829,\n",
       " 0.7945114683944791,\n",
       " 0.5843043203428228,\n",
       " 0.541615130648126,\n",
       " 0.7108379906800688,\n",
       " 0.5794327652285225,\n",
       " 0.6302611677725402,\n",
       " 0.7816714423074527,\n",
       " 0.9017802747333105,\n",
       " 0.7135464192570755,\n",
       " 0.7141247910566322,\n",
       " 0.5897616737753433,\n",
       " 0.38988783272948113,\n",
       " 0.7201606100709435,\n",
       " 0.5386143227218638,\n",
       " 0.618005272307508,\n",
       " 0.34455748816049797,\n",
       " 0.5823668846439218,\n",
       " 0.45548110859185537,\n",
       " 0.6398301253197854,\n",
       " 0.554156671683876,\n",
       " 0.5017313599392185,\n",
       " 0.6463218501561642,\n",
       " 0.6755143048182295,\n",
       " 0.5963185492118274,\n",
       " 0.6018660078647475,\n",
       " 0.5051251601203303,\n",
       " 0.7038903661724305,\n",
       " 0.4160678531427393,\n",
       " 0.4901834149220091,\n",
       " 0.7962674642162154,\n",
       " 0.36970458575734644,\n",
       " 0.5531080297859667,\n",
       " 0.6667351659007144,\n",
       " 0.8171825150097769,\n",
       " 0.572505470776667,\n",
       " 0.47824169889422563,\n",
       " 0.6762270995235591,\n",
       " 0.46715335965195603,\n",
       " 0.6558696708075182,\n",
       " 0.8784612540567933,\n",
       " 0.7994844955055789,\n",
       " 0.7107953494548837,\n",
       " 0.3907646374415622,\n",
       " 0.7374288182098521,\n",
       " 0.8005449422057489,\n",
       " 0.7305736231341199,\n",
       " 0.6185666927790728,\n",
       " 0.6616150026585926,\n",
       " 0.7250572327631554,\n",
       " 0.7106508872210789,\n",
       " 0.44359544295496,\n",
       " 0.9123306406230867,\n",
       " 0.6917112243659813,\n",
       " 0.8107638491271398,\n",
       " 0.8900134804586287,\n",
       " 0.7936339267374407,\n",
       " 0.7588333228994084,\n",
       " 0.8474321555500259,\n",
       " 0.6345289290471212,\n",
       " 0.805217937398766,\n",
       " 0.7748763128088341,\n",
       " 0.8813303132316691,\n",
       " 0.7051247979195948,\n",
       " 0.6972899118659219,\n",
       " 0.7689148324510939,\n",
       " 0.4548079100562171,\n",
       " 0.5298657823969464,\n",
       " 0.6800806217615524,\n",
       " 0.6670210680902946,\n",
       " 0.7633114531403418,\n",
       " 0.7464769570842726,\n",
       " 0.80845456694703,\n",
       " 0.6199262900467152,\n",
       " 0.6626047067055167,\n",
       " 0.5976451659487029,\n",
       " 0.7328486060835622,\n",
       " 0.7997260755392515,\n",
       " 0.38182128190810627,\n",
       " 0.7497590403006759,\n",
       " 0.5377037934052846,\n",
       " 0.6579889474550669,\n",
       " 0.7289430429865345,\n",
       " 0.8546087319920941,\n",
       " 0.7562856680236725,\n",
       " 0.8006306859681492,\n",
       " 0.8951047501966597,\n",
       " 0.717317396483314,\n",
       " 0.4833527901858651,\n",
       " 0.7475096497958635,\n",
       " 0.6740901760240654,\n",
       " 0.49049781785828916,\n",
       " 0.6407762652254229,\n",
       " 0.6386505463384607,\n",
       " 0.5100544478488171,\n",
       " 0.35230915773439586,\n",
       " 0.45933078320094434,\n",
       " 0.8144383362305497,\n",
       " 0.8587338987056896,\n",
       " 0.8017123310705608,\n",
       " 0.7088549003435325,\n",
       " 0.7975440853690261,\n",
       " 0.7831068302290777,\n",
       " 0.601506791181444,\n",
       " 0.8633541467804009,\n",
       " 0.52485279139405,\n",
       " 0.6632604187246052,\n",
       " 0.8328859148822918,\n",
       " 0.5314300445267969,\n",
       " 0.6222316995437276,\n",
       " 0.7568059092493127,\n",
       " 0.9164455048574465,\n",
       " 0.815569037521613,\n",
       " 0.8635572221626662,\n",
       " 0.6603615325322938,\n",
       " 0.6352102143978724,\n",
       " 0.6394866233753591,\n",
       " 0.6739930710892426,\n",
       " 0.8861628060082845,\n",
       " 0.8179628089283763,\n",
       " 0.7582830528379462,\n",
       " 0.7192696893410984,\n",
       " 0.6326360201765714,\n",
       " 0.6175460088616007,\n",
       " 0.7903539663348131,\n",
       " 0.7235733674565588,\n",
       " 0.520447409090946,\n",
       " 0.5784724097193124,\n",
       " 0.7958379689749302,\n",
       " 0.7763766750075107,\n",
       " 0.7984979276706479,\n",
       " 0.6248927177130134,\n",
       " 0.5612248517380583,\n",
       " 0.5630210954700676,\n",
       " 0.6449233616913763,\n",
       " 0.6628469276536796,\n",
       " 0.6056350557548349,\n",
       " 0.7123980735468434,\n",
       " 0.7402466020750185,\n",
       " 0.7784918816006627,\n",
       " 0.4274526814808075,\n",
       " 0.6565997861541346,\n",
       " 0.6661483340164844,\n",
       " 0.6487070647980163,\n",
       " 0.2868458559168354,\n",
       " 0.575215561432208,\n",
       " 0.5627560101101734,\n",
       " 0.8777032062861443,\n",
       " 0.6271098937454498,\n",
       " 0.615210468278092,\n",
       " 0.6549847290853321,\n",
       " 0.7984934766816684,\n",
       " 0.4421666422971563,\n",
       " 0.7395973832238276,\n",
       " 0.8510868956444566,\n",
       " 0.6228462636786583,\n",
       " 0.4982623692008923,\n",
       " 0.6273109849567136,\n",
       " 0.7961782081878601,\n",
       " 0.6039569651932962,\n",
       " 0.4377950613353875,\n",
       " 0.6430371125903318,\n",
       " 0.5448852907486912,\n",
       " 0.8046804465701884,\n",
       " 0.4712386290999927,\n",
       " 0.7191634890812607,\n",
       " 0.7105421275575909,\n",
       " 0.6853541687118854,\n",
       " 0.4962774365733408,\n",
       " 0.6664071923764741,\n",
       " 0.6843037011465117,\n",
       " 0.8753495381310086,\n",
       " 0.8772351054917266,\n",
       " 0.6913658482639067,\n",
       " 0.7440432091940135,\n",
       " 0.7141556363687269,\n",
       " 0.8060044430742959,\n",
       " 0.7524753202384697,\n",
       " 0.7533778334337742,\n",
       " 0.6357196324597556,\n",
       " 0.892161397454856,\n",
       " 0.6804502305978215,\n",
       " 0.8021239947038163,\n",
       " 0.5962275704066933,\n",
       " 0.7325259736434376,\n",
       " 0.5420052653244165,\n",
       " 0.8247210425243626,\n",
       " 0.6567991142282926,\n",
       " 0.7530222009160186,\n",
       " 0.8087215745380559,\n",
       " 0.5688140954955853,\n",
       " 0.39289557150962356,\n",
       " 0.5758365097597804,\n",
       " 0.4692689189923618,\n",
       " 0.48669182339627964,\n",
       " 0.46143761306695,\n",
       " 0.622533673078778,\n",
       " 0.6180845070779457,\n",
       " 0.8383709856283477,\n",
       " 0.605698699085542,\n",
       " 0.8256563113076667,\n",
       " 0.5547889892034444,\n",
       " 0.5778942115152258,\n",
       " 0.5748040342216786,\n",
       " 0.7211942233857549,\n",
       " 0.8992511911139023,\n",
       " 0.6106579767640765,\n",
       " 0.5196845014756915,\n",
       " 0.6864478108222258,\n",
       " 0.7640226976244253,\n",
       " 0.7689982039530345,\n",
       " 0.8330076966998377,\n",
       " 0.6216832048605577,\n",
       " 0.6717088026720652,\n",
       " 0.5251668953107224,\n",
       " 0.8502332984426924,\n",
       " 0.7875541663525487,\n",
       " 0.7371730156145242,\n",
       " 0.6626843373304521,\n",
       " 0.8000618726448328,\n",
       " 0.8198467434631382,\n",
       " 0.7220407792909356,\n",
       " 0.857794022633734,\n",
       " 0.5523289780711919,\n",
       " 0.4864951146141597,\n",
       " 0.5515105927589303,\n",
       " 0.6681024689149274,\n",
       " 0.7055486149191065,\n",
       " 0.6813479213857851,\n",
       " 0.8651795164385528,\n",
       " 0.465544154948454,\n",
       " 0.6230537364957514,\n",
       " 0.6357398449177485,\n",
       " 0.655434549668606,\n",
       " 0.6631948773942786,\n",
       " 0.7023543882831755,\n",
       " 0.8673923504391579,\n",
       " 0.5579997192948004,\n",
       " 0.7029096823657539,\n",
       " 0.8767192767686425,\n",
       " 0.6909417344210667,\n",
       " 0.755551799665694,\n",
       " 0.4849986452561304,\n",
       " 0.7408266990865087,\n",
       " 0.8122419428409043,\n",
       " 0.3723031145386931,\n",
       " 0.7993222514354401,\n",
       " 0.7591238994855214,\n",
       " 0.771659198909121,\n",
       " 0.5391031365826269,\n",
       " 0.7166119218447238,\n",
       " 0.6627627148727182,\n",
       " 0.7572301214229575,\n",
       " 0.7365166447814693,\n",
       " 0.8172467296898713,\n",
       " 0.6736836858826074,\n",
       " 0.3220155706819129,\n",
       " 0.6063440210628552,\n",
       " 0.8398466331461683,\n",
       " 0.8302987767644083,\n",
       " 0.6751583797034393,\n",
       " 0.5568867343321914,\n",
       " 0.5828411826008362,\n",
       " 0.6491066820472078,\n",
       " 0.7909668654779772,\n",
       " 0.8806802710920967,\n",
       " 0.49242070915670366,\n",
       " 0.4488791089485501,\n",
       " 0.6182305422076451,\n",
       " 0.6167198156172237,\n",
       " 0.6316632338481104,\n",
       " 0.6887501712726803,\n",
       " 0.4857097854486825,\n",
       " 0.6385986047183385,\n",
       " 0.6899459299012635,\n",
       " 0.6850961570369455,\n",
       " 0.6815352061689122,\n",
       " 0.727304616677682,\n",
       " 0.5249702476151935,\n",
       " 0.8711043505632954,\n",
       " 0.41203297815533696,\n",
       " 0.7428944663872413,\n",
       " 0.4699396459640176,\n",
       " 0.5068157801766306,\n",
       " 0.7982890106040782,\n",
       " 0.8301900226366319,\n",
       " 0.44961936084875853,\n",
       " 0.6729562365955724,\n",
       " 0.6812533673220702,\n",
       " 0.7154687759130172,\n",
       " 0.7513171005303952,\n",
       " 0.86861652243075,\n",
       " 0.7460422834397347,\n",
       " 0.49048176713175845,\n",
       " 0.75582834785614,\n",
       " 0.8664762240107842,\n",
       " 0.7291276680892409,\n",
       " 0.6939602714146683,\n",
       " 0.6065828585895618,\n",
       " 0.607965767836233,\n",
       " 0.694422368276786,\n",
       " 0.798857546865938,\n",
       " 0.6090981558780985,\n",
       " 0.7358874949354185,\n",
       " 0.7355591463412667,\n",
       " 0.43387615073366803,\n",
       " 0.4028469101637836,\n",
       " 0.6119072825890493,\n",
       " 0.6844200869149629,\n",
       " 0.5737634294117028,\n",
       " 0.7149888639024798,\n",
       " 0.7120006206243231,\n",
       " 0.656933328651934,\n",
       " 0.7753160820488307,\n",
       " 0.612022080738061,\n",
       " 0.5795312503265673,\n",
       " 0.4894078670423303,\n",
       " 0.631617195200632,\n",
       " 0.5279555326073797,\n",
       " 0.42820447779808746,\n",
       " 0.6042742827481176,\n",
       " 0.6962479691964836,\n",
       " 0.3175241460922559,\n",
       " 0.7337447298616939,\n",
       " 0.7273760132017244,\n",
       " 0.5361463521820391,\n",
       " 0.7167880592953715,\n",
       " 0.7420270698547566,\n",
       " 0.573645375581225,\n",
       " 0.6502552775140307,\n",
       " 0.6754808860627758,\n",
       " 0.6986524508135551,\n",
       " 0.6729919389131342,\n",
       " 0.8212947150999037,\n",
       " 0.3953720155867195,\n",
       " 0.6991099884149474,\n",
       " 0.4755179050806786,\n",
       " 0.7768374549298848,\n",
       " 0.7650869829622211,\n",
       " 0.7477540057798477,\n",
       " 0.4237693683899786,\n",
       " 0.5520381729116516,\n",
       " 0.6155924938120971,\n",
       " 0.667395857734717,\n",
       " 0.6126458101472917,\n",
       " 0.4278831686970983,\n",
       " 0.5530254401467835,\n",
       " 0.8543919475335964,\n",
       " 0.8236360572913367,\n",
       " 0.7415432374038762,\n",
       " 0.7959864275547371,\n",
       " 0.7627781344704936,\n",
       " 0.6810276850760164,\n",
       " 0.5112902010876662,\n",
       " 0.6062258901030975,\n",
       " 0.65071648203241,\n",
       " 0.633394912472897,\n",
       " 0.8295127742612689,\n",
       " 0.7553307671824527,\n",
       " 0.746609583723129,\n",
       " 0.7742126853527255,\n",
       " 0.688383205458856,\n",
       " 0.6184026852661593,\n",
       " 0.8072534935387419,\n",
       " 0.6408973845495749,\n",
       " 0.557656642688654,\n",
       " 0.778422515470931,\n",
       " 0.8615560182890596,\n",
       " 0.31697088377119786,\n",
       " 0.3708527109801387,\n",
       " 0.6920984167994065,\n",
       " 0.7326356397605924,\n",
       " 0.5320715846652442,\n",
       " 0.5990341096522165,\n",
       " 0.5933732194865247,\n",
       " 0.36630457207103323,\n",
       " 0.729918014983914,\n",
       " 0.5892228544047915,\n",
       " 0.7212929501708951,\n",
       " 0.5619595338358164,\n",
       " 0.6217680768463412,\n",
       " 0.6401408686003006,\n",
       " 0.7997248343522205,\n",
       " 0.7662945179864442,\n",
       " 0.3631254848386586,\n",
       " 0.7190866827897766,\n",
       " 0.6175891687605751,\n",
       " 0.8308843150345184,\n",
       " 0.7059984133085042,\n",
       " 0.6687893369669584,\n",
       " 0.6358405187630749,\n",
       " 0.3697644211029192,\n",
       " 0.5858414104321574,\n",
       " 0.6235667557757447,\n",
       " 0.7919550592392423,\n",
       " 0.7576116743965344,\n",
       " 0.6926017505807343,\n",
       " 0.6780746932156381,\n",
       " 0.8635642800953276,\n",
       " 0.5311699243938259,\n",
       " 0.8516055802312856,\n",
       " 0.46591616314185874,\n",
       " 0.8047889454457713,\n",
       " 0.6866299394978667,\n",
       " 0.839026335045382,\n",
       " 0.6989015791001472,\n",
       " 0.8072431965718179,\n",
       " 0.7018051927521871,\n",
       " 0.6311386476333737,\n",
       " 0.38814638066718293,\n",
       " 0.7621239805103146,\n",
       " 0.5664954860450623,\n",
       " 0.7650868870864764,\n",
       " 0.8550833520497438,\n",
       " 0.7265223037961276,\n",
       " 0.6919371649241854,\n",
       " 0.795162326221187,\n",
       " 0.5401190154051041,\n",
       " 0.7967655308773552,\n",
       " 0.7331071949582506,\n",
       " 0.8341477400878894,\n",
       " 0.7638123031430442,\n",
       " 0.650086161863233,\n",
       " 0.47833540132490104,\n",
       " 0.7037805059131084,\n",
       " 0.8413753966722473,\n",
       " 0.5559075200358097,\n",
       " 0.7027499423808835,\n",
       " 0.605982023407128,\n",
       " 0.7248398963054863,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict\n",
    "submit_score.to_csv('predict_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(train_file,values = 'price', index=['lvl1','lvl2','lvl3'],columns=['type'],aggfunc=[min, max, np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
