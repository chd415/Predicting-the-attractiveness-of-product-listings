{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chd415/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "sns.set(color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                   \n",
    "import nltk                                         \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer  \n",
    "LA = np.linalg\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from gensim.models.word2vec import Word2Vec                                  \n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data text\n",
    "def load_data(filename):\n",
    "    load_file = pd.read_csv(filename,delimiter=',', header=0,\n",
    "                        dtype={'name':str, 'lvl1':str, 'lvl2':str, 'lvl3':str, 'descrption':str, 'type':str})\n",
    "    load_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "    load_file.duplicated(subset=None, keep='first')\n",
    "    load_file.set_index('id', inplace = True)\n",
    "    load_file.head()\n",
    "    return load_file\n",
    "#print(len(train_file))\n",
    "def load_label(filename):\n",
    "    load_label = pd.read_csv(filename,delimiter=',', header=0)\n",
    "    load_label.columns = ['id', 'score']\n",
    "    load_label.duplicated(subset=None, keep='first')\n",
    "    load_label.set_index('id', inplace = True)\n",
    "    return load_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def map_mathod(column):\n",
    "    values = []\n",
    "    indexs = []\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for count in range(len(train_file)):\n",
    "        value = train_file.get_value(count+1,column)\n",
    "        if value in values and value != np.nan:\n",
    "            continue\n",
    "        values.append(value)\n",
    "        indexs.append(len(values))\n",
    "    for j in range(len(indexs)):\n",
    "        mapping[values[j]] = indexs[j]\n",
    "    mapping[np.nan] = 0.0\n",
    "    return mapping\n",
    "#train_file['lvl3'] = train_file['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "#mapping_lvl3 = map_mathod('lvl3')\n",
    "#print(mapping_lvl3)\n",
    "'''\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def text_embedding(column,vecsize):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=10000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(max_features=10000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=1,size=vecsize, workers=4)\n",
    "    \n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(vecsize, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "\n",
    "        \n",
    "    tf_w_data = []\n",
    "    tf_data = tf_data.toarray()\n",
    "    i = 0\n",
    "    for row in splitted:\n",
    "        vec = [0 for i in range(vecsize)]\n",
    "    \n",
    "        temp_tfidf = []\n",
    "        for val in tf_data[i]:\n",
    "            if val != 0:\n",
    "                temp_tfidf.append(val)\n",
    "    \n",
    "        count = 0\n",
    "        tf_idf_sum = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                count += 1\n",
    "                tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "                vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        if (tf_idf_sum == 0):\n",
    "            tf_w_data.append(vec)\n",
    "        else:\n",
    "            tf_w_data.append(vec/tf_idf_sum)\n",
    "        i = i + 1\n",
    "    \n",
    "    return tf_w_data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "temp = load_data('train_data.csv')\n",
    "price_X = temp['price'].as_matrix(columns=None).reshape(1, -1)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(price_X)\n",
    "# outfile = scaler.transform(price_X)\n",
    "\n",
    "# est = KBinsDiscretizer(n_bins=15, encode='onehot-dense', strategy='uniform')\n",
    "# est.fit(price_X)\n",
    "# outfile = est.transform(price_X)\n",
    "\n",
    "transformer = Normalizer(norm='l2').fit(price_X)\n",
    "outfile = transformer.transform(price_X)\n",
    "\n",
    "\n",
    "# temp['price'] = temp['price']\n",
    "# hist = temp['price'].hist(bins=15)\n",
    "\n",
    "# price_X = temp['price'].clip(lower=0.).as_matrix(columns=None).tolist()\n",
    "# price_X.sort()\n",
    "\n",
    "#np.histogram(outfile,bins=10)\n",
    "\n",
    "outfilelist = outfile.tolist()\n",
    "outfilelist.sort()\n",
    "\n",
    "file = open('temp.dat','w')\n",
    "file.writelines([\"%s\\n\" % item  for item in outfilelist])\n",
    "file.close()\n",
    "#print(outfile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "\n",
    "temp = pd.read_csv('train_data.csv',delimiter=',', header=0)\n",
    "description_X = temp.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "#description_X.head()\n",
    "tfidf_n = TfidfVectorizer(max_features=1000,stop_words = 'english')\n",
    "tf_out = tfidf_n.fit_transform(description_X.astype('U')).toarray()\n",
    "#lg_array = csr_matrix(tf_out, dtype=np.int8).toarray()\n",
    "#lg_array = np.vstack( lg_array )\n",
    "    \n",
    "#U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "#assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "print(tf_out.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "    filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].str.lower()\n",
    "    filename['name'] = filename['name'].str.lower()\n",
    "\n",
    "    '''\n",
    "    \n",
    "    mapping_lvl1 = map_mathod('lvl1')\n",
    "    mapping_lvl2 = map_mathod('lvl2')\n",
    "    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "    filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "    filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    '''\n",
    "    #clean up data for lvl 1&2&3 and type\n",
    "    temp =  filename.drop(['price', 'descrption','name'], axis=1)\n",
    "    outfile = MultiColumnLabelEncoder(columns = ['lvl1','lvl2','lvl3','type']).fit_transform(temp.astype(str))\n",
    "#    outfile = outfile.as_matrix(columns=None).tolist()\n",
    "    \n",
    "    \n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(outfile)\n",
    "    outfile = enc.transform(outfile).toarray()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #normalize price\n",
    "#     maxp = filename.price.max()\n",
    "#     valuethred = 500.\n",
    "#     filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=0.)\n",
    "#     hist = filename['price'].hist(bins=10)\n",
    "#     #maxp\n",
    "#     ld = filename.price.as_matrix(columns=None).tolist()\n",
    "\n",
    "    price_X = filename['price'].as_matrix(columns=None).reshape(-1, 1)\n",
    "    transformer = Normalizer(copy=True,norm='l2')\n",
    "    ld = transformer.fit_transform(price_X)\n",
    "    outfile = np.column_stack((outfile,ld))\n",
    "\n",
    "    \n",
    "\n",
    "    #clean up text\n",
    "    description_X = filename.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    count_descrption = description_X.str.count('final').fillna(0).tolist()\n",
    "    outfile = np.column_stack((outfile,count_descrption))\n",
    "    \n",
    "    description_X = description_X.str.lower().replace('final ','',regex=True).replace('\\d+', '',regex=True)\n",
    "    descrption_Xstring = pd.Series(description_X.tolist()).astype(str)\n",
    "    count_wordcount = descrption_Xstring.apply(lambda x: len(x.split(' ')))\n",
    "    count_lettercount = descrption_Xstring.apply(lambda x: len(x))\n",
    "    outfile = np.column_stack((outfile,count_wordcount))\n",
    "    outfile = np.column_stack((outfile,count_lettercount))\n",
    "    \n",
    "    tfidf_n = HashingVectorizer(n_features=2**10,stop_words = 'english')\n",
    "    #tfidf_n = TfidfVectorizer(max_features=500,stop_words = 'english')\n",
    "    #tfidf_n = CountVectorizer(max_features=500,stop_words = 'english')\n",
    "    lg = tfidf_n.fit_transform(description_X.astype('U')).toarray()\n",
    "    lg_array = np.vstack( lg )\n",
    "#    print(lg)\n",
    "    \n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "    s_new = s[:30]\n",
    "    U_new = U[:, :30]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    outfile = np.column_stack((outfile,new_lg))\n",
    "    \n",
    "    \n",
    "    name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True).replace('\\d+', '',regex=True)\n",
    "    name_Xstring = pd.Series(name_X.tolist()).astype(str)\n",
    "    name_wordcount = name_Xstring.apply(lambda x: len(x.split(' ')))\n",
    "    name_lettercount = name_Xstring.apply(lambda x: len(x))\n",
    "    outfile = np.column_stack((outfile,name_wordcount))\n",
    "    outfile = np.column_stack((outfile,name_lettercount))\n",
    "\n",
    "\n",
    "    tfidf_f = HashingVectorizer(n_features=2**10,stop_words = 'english')\n",
    "    #tfidf_f = TfidfVectorizer(max_features=500,stop_words = 'english')\n",
    "    #tfidf_f = CountVectorizer(max_features=500,stop_words = 'english')\n",
    "    lf = tfidf_f.fit_transform(name_X.astype('U')).toarray()\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "    sf_new = sf[:15]\n",
    "    Uf_new = Uf[:, :15]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    outfile = np.column_stack((outfile,new_lf))\n",
    "    \n",
    "    '''\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(outfile)\n",
    "    outfile = enc.transform(outfile).toarray()\n",
    "    '''\n",
    "\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_file = pd.read_csv('train_data.csv',delimiter=',', header=0, nrows=10)\n",
    "train_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "description_X = train_file.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "count_descrption = description_X.str.count('final')\n",
    "description_X = train_file.descrption.str.lower().replace('final ','',regex=True)\n",
    "print(np.shape(count_descrption))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36283, 305)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = load_data('train_data.csv')\n",
    "test_file = load_data('test_data.csv')\n",
    "combined_file = pd.concat([train_file,test_file])\n",
    "cleaned_train = clean_data(combined_file)\n",
    "train_score = load_label('train_label.csv')\n",
    "np.shape(cleaned_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "s_gra = np.abs(np.gradient(s))\n",
    "slist = s.tolist()\n",
    "file = open('svd_secrption.dat','w')\n",
    "file.writelines([\"%s\\n\" % item  for item in s_gra])\n",
    "file.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_ylabel('Eign Values', color=color,fontweight='bold')\n",
    "ax1.plot(slist, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Gradient of Eign Value', color=color,fontweight='bold')  # we already handled the x-label with ax1\n",
    "ax2.plot(s_gra, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "plt.grid()\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('Eign_Values_descrption.png', dpi=fig.dpi)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         5.76567035e-03,  4.45632408e-04, -1.02727874e-01],\n",
       "       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "         2.00962191e-02, -3.55559138e-02,  1.22390325e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.28080643e-01, -7.27881469e-02,  8.21005722e-02],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         4.66897727e-03, -2.28484828e-03,  7.43972722e-03],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         7.42522646e-02,  1.07715144e-02,  1.06080415e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         3.28845575e-02,  5.90442695e-02,  4.15250582e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test cell\n",
    "cleaned_train['descrption'] = temp['descrption']\n",
    "'''\n",
    "cleaned_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pca = PCA(n_components=100, svd_solver='full')\n",
    "cleaned_train_temp = pca.fit_transform(cleaned_train)\n",
    "\n",
    "cleaned_train_min = np.min(cleaned_train_temp)\n",
    "cleaned_train_out = cleaned_train_temp - (cleaned_train_min)*np.ones_like(cleaned_train_temp.size)\n",
    "print(cleaned_train_out)\n",
    "cleaned_train = cleaned_train_out\n",
    "print(cleaned_train.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36283, 305)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "def rearrange(cleaned_data):\n",
    "    la = cleaned_data.lvl1.as_matrix(columns=None).tolist()\n",
    "    lb = cleaned_data.lvl2.as_matrix(columns=None).tolist()\n",
    "    lc = cleaned_data.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = la\n",
    "    X = np.column_stack((X,lb))\n",
    "    X = np.column_stack((X,lc))\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    \n",
    "    ld = cleaned_data.price.as_matrix(columns=None).tolist()\n",
    "    le = cleaned_data.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = np.column_stack((X,ld))\n",
    "    X = np.column_stack((X,le))\n",
    "\n",
    "   \n",
    "    lf = cleaned_data.name.as_matrix(columns=None).tolist()\n",
    "    lg = cleaned_data.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "# only use U \\cdot s\n",
    "    \n",
    "#    s[8:] = 0.\n",
    "#    new_lg = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "    s_new = s[:5]\n",
    "    U_new = U[:, :5]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    lg_min = np.min(new_lg)\n",
    "    lg_out = new_lg  - (lg_min-2)*np.ones_like(new_lg.size)\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "#    sf[5:] = 0.\n",
    "#    new_lf = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "    sf_new = sf[:5]\n",
    "    Uf_new = Uf[:, :5]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    lf_min = np.min(new_lf)\n",
    "    lf_out = new_lf  - (lf_min-2)*np.ones_like(new_lf.size)\n",
    "\n",
    "    \n",
    "    X = np.column_stack((X,lf_out))\n",
    "    X = np.column_stack((X,lg_out))\n",
    "    X = X.tolist()\n",
    "   \n",
    "    return X,lf_min,lg_min\n",
    "'''\n",
    "    \n",
    "    #print(len(X))\n",
    "X = cleaned_train#rearrange(cleaned_train)\n",
    "w,b = np.shape(np.array(X))\n",
    "print(np.shape(np.array(X)))\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18141\n",
      "(18141, 305)\n",
      "(18142, 305)\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_train[:18141]\n",
    "XX = cleaned_train[18141:]\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "print(np.size(Y))\n",
    "print(np.shape(X))\n",
    "print(np.shape(XX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    "#maxlen = b\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "#X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, dtype='float32')\n",
    "print(X_train[1400].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier  \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Classifier\n",
    "def linear_regression_classifier(train_x, train_y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    param_grid = {'alpha': [math.pow(10,-i) for i in range(11)]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = MultinomialNB(alpha = best_parameters['alpha'])  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    param_grid = {'n_neighbors': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = best_parameters['n_neighbors'], algorithm='kd_tree')\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    \n",
    "    return bagging\n",
    " \n",
    " \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(100,300,10))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    #for para, val in best_parameters.items():\n",
    "        #print para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "def feature_select(x,y):\n",
    "    clf = ExtraTreesClassifier()\n",
    "    clf = clf.fit(x, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "******************* KNN ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   40.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 40.773230s!\n",
      "accuracy: 68.81%\n",
      "loss: 10.77\n",
      "******************* LR ********************\n",
      "training took 0.582929s!\n",
      "accuracy: 79.58%\n",
      "loss: 7.05\n",
      "******************* RF ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   32.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 35.448478s!\n",
      "accuracy: 79.69%\n",
      "loss: 7.01\n",
      "******************* DT ********************\n",
      "training took 1.196019s!\n",
      "accuracy: 66.49%\n",
      "loss: 11.57\n"
     ]
    }
   ],
   "source": [
    "# just for my own record\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "    \n",
    "    test_classifiers = ['KNN','LR','RF','DT']    \n",
    "    classifiers = {\n",
    "                   'KNN':knn_classifier,\n",
    "                    'LR':logistic_regression_classifier,\n",
    "                    'RF':random_forest_classifier,\n",
    "                    'DT':decision_tree_classifier,\n",
    "    }\n",
    "    \n",
    "    '''\n",
    "    test_classifiers = ['SVC']    \n",
    "    classifiers = {'SVC':svm_cross_validation}\n",
    "    ''' \n",
    "    \n",
    "    print('reading training and testing data...')    \n",
    "\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_validation = select_model.transform(X_validation)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    '''\n",
    "    start_time = time.time()    \n",
    "    model = classifiers[classifier](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    regressor = DecisionTreeRegressor()\n",
    "    regressor.fit(train_x, train_y)\n",
    "    predict = model.predict(X_validation)\n",
    "    '''\n",
    "\n",
    "        \n",
    "    for classifier in test_classifiers:    \n",
    "        print('******************* %s ********************' % classifier)    \n",
    "        start_time = time.time()    \n",
    "        model = classifiers[classifier](X_train, y_train)   \n",
    "        print('training took %fs!' % (time.time() - start_time))    \n",
    "        predict = model.predict(X_validation)\n",
    "\n",
    "        precision = metrics.precision_score(y_validation, predict)    \n",
    "        recall = metrics.recall_score(y_validation, predict)    \n",
    "        accuracy = metrics.accuracy_score(y_validation, predict)    \n",
    "        print('accuracy: %.2f%%' % (100 * accuracy))\n",
    "        logloss = metrics.log_loss(y_validation, predict)\n",
    "        print('loss: %.2f' % (logloss))\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "#        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        , ...,  0.00719774,\n",
       "         0.08520765, -0.00848235],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.04223069,\n",
       "         0.05598058, -0.02465427],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.09019198,\n",
       "        -0.06775344,  0.01201529],\n",
       "       ...,\n",
       "       [ 0.        ,  1.        ,  0.        , ..., -0.02281718,\n",
       "        -0.03820481,  0.03000282],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  0.07781594,\n",
       "        -0.15327869,  0.07893756],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.09236642,\n",
       "        -0.03573242, -0.1795121 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "#X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cnn model\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# Embedding\n",
    "max_features = 5000\n",
    "maxlen = b\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 32\n",
    "pool_size = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 5\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "X_train = np.asarray(np.abs(X_train))\n",
    "X_validation = np.asarray(np.abs(X_validation))\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_validation), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, padding='post')\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_validation.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(max_features, embedding_size,input_length=maxlen))\n",
    "#model.add(Dense(32, activation='relu', input_dim=100))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "cnnmodel.add(Dense(32, activation='relu', input_dim=64))\n",
    "cnnmodel.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=pool_size))\n",
    "cnnmodel.add(LSTM(lstm_output_size))\n",
    "cnnmodel.add(Dense(1))\n",
    "cnnmodel.add(Activation('sigmoid'))\n",
    "\n",
    "cnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "training took 0.908805s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   49.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 54.554684s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 13.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 939.536996s!\n",
      "predict finished\n",
      "training took 942.306601s!\n",
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "     \n",
    "    test_classifiers = ['LR','RF','GBC','DT']    \n",
    "    classifiers = {\n",
    "                   'LR':logistic_regression_classifier,\n",
    "                   'RF':random_forest_classifier,\n",
    "                   'GBC':gradient_boosting_classifier,\n",
    "                   'DT':decision_tree_classifier\n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_test = select_model.transform(X_test)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = classifiers['LR'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_lr = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['RF'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))\n",
    "    regr = RandomForestRegressor(max_depth=2, max_features=1)\n",
    "    regr.fit(X_train, y_train)\n",
    "    Y_predict_rf = regr.predict(X_test)\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['GBC'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_gbc = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['DT'](X_train, y_train)  \n",
    "    print('training took %fs!' % (time.time() - start_time))\n",
    "    regressor = DecisionTreeRegressor(max_depth=2) \n",
    "    regressor.fit(X_train, y_train)\n",
    "    Y_predict_dt = regressor.predict(X_test)\n",
    "    print('predict finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn test\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "cnnmodel.fit(X_train, y_train, batch_size=128, epochs=8)\n",
    "\n",
    "Y_predict_cnn = cnnmodel.predict(X_test, verbose=0)\n",
    "Y_predict_cnn = np.squeeze(Y_predict_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18141/18141 [==============================] - 1s 50us/step - loss: 0.6373 - acc: 0.7101\n",
      "Epoch 2/20\n",
      "18141/18141 [==============================] - 1s 32us/step - loss: 0.5183 - acc: 0.7647\n",
      "Epoch 3/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4793 - acc: 0.7754\n",
      "Epoch 4/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4608 - acc: 0.7882\n",
      "Epoch 5/20\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.4493 - acc: 0.7958\n",
      "Epoch 6/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4476 - acc: 0.7989\n",
      "Epoch 7/20\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.4366 - acc: 0.8008\n",
      "Epoch 8/20\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.4365 - acc: 0.8020\n",
      "Epoch 9/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.4297 - acc: 0.8049\n",
      "Epoch 10/20\n",
      "18141/18141 [==============================] - 1s 28us/step - loss: 0.4290 - acc: 0.8072\n",
      "Epoch 11/20\n",
      "18141/18141 [==============================] - 1s 33us/step - loss: 0.4256 - acc: 0.8066\n",
      "Epoch 12/20\n",
      "18141/18141 [==============================] - 1s 31us/step - loss: 0.4265 - acc: 0.8046\n",
      "Epoch 13/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.4314 - acc: 0.8017\n",
      "Epoch 14/20\n",
      "18141/18141 [==============================] - 1s 30us/step - loss: 0.4195 - acc: 0.8093\n",
      "Epoch 15/20\n",
      "18141/18141 [==============================] - 1s 28us/step - loss: 0.4136 - acc: 0.8142\n",
      "Epoch 16/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.4248 - acc: 0.8070\n",
      "Epoch 17/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4249 - acc: 0.8062\n",
      "Epoch 18/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4237 - acc: 0.8064\n",
      "Epoch 19/20\n",
      "18141/18141 [==============================] - 1s 31us/step - loss: 0.4183 - acc: 0.8077\n",
      "Epoch 20/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4134 - acc: 0.8107\n",
      "18141/18141 [==============================] - 1s 38us/step\n",
      "\n",
      "acc: 80.46%\n"
     ]
    }
   ],
   "source": [
    "#nn train\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_array = np.asarray(X_train)\n",
    "Y_array = np.asarray(y_train)\n",
    "Xtest_array = np.asarray(X_test) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_array\n",
    "Y = Y_array\n",
    "# create model\n",
    "nnmodel = Sequential()\n",
    "nnmodel.add(Dense(100, input_dim=b, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "nnmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "nnmodel.fit(X, Y, epochs=20, batch_size=150)\n",
    "# evaluate the model\n",
    "scores = nnmodel.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (nnmodel.metrics_names[1], scores[1]*100))\n",
    "Y_predict_nn = nnmodel.predict(Xtest_array, verbose=0)\n",
    "Y_predict_nn = np.squeeze(Y_predict_nn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95243216, 0.9365295 , 0.4178319 , ..., 0.903523  , 0.45674112,\n",
       "       0.9220488 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8161227 , 0.7838163 , 0.10645147, ..., 0.64000625, 0.31511305,\n",
       "       0.85438897])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.718581  , 0.67580366, 0.69235922, ..., 0.7219229 , 0.67976975,\n",
       "       0.69461764])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83448276, 0.63103448, 0.31034483, ..., 0.82068966, 0.42068966,\n",
       "       0.80344828])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68512556, 0.56818182, 0.29733728, ..., 0.88516969, 0.56818182,\n",
       "       0.88516969])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_dt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict.append((  Y_predict_gbc[i] + Y_predict_nn[i] ) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8934574571149103,\n",
       " 0.7837819999661939,\n",
       " 0.3640883626609013,\n",
       " 0.9860923783532505,\n",
       " 0.9699075752291186,\n",
       " 0.8788595949781353,\n",
       " 0.27048415474850557,\n",
       " 0.9508815872258154,\n",
       " 0.5074864815021383,\n",
       " 0.9427687431203908,\n",
       " 0.9589718746727911,\n",
       " 0.36794885510000686,\n",
       " 0.7273575190840096,\n",
       " 0.461634309949546,\n",
       " 0.8999572431219035,\n",
       " 0.4839181934965068,\n",
       " 0.8940704368311783,\n",
       " 0.2863713100038726,\n",
       " 0.8583994680437548,\n",
       " 0.7220754467207811,\n",
       " 0.9656400372242105,\n",
       " 0.8316066195224894,\n",
       " 0.8467576762725567,\n",
       " 0.9232959009450057,\n",
       " 0.7762503778112346,\n",
       " 0.8372111680178806,\n",
       " 0.6645338712067439,\n",
       " 0.960339406440998,\n",
       " 0.9077297586819222,\n",
       " 0.8142866835511964,\n",
       " 0.9725532624228248,\n",
       " 0.7883731365203858,\n",
       " 0.938540298363258,\n",
       " 0.9120848148033537,\n",
       " 0.7890113520211187,\n",
       " 0.7091837946710915,\n",
       " 0.5720072898371467,\n",
       " 0.9359707552811195,\n",
       " 0.26196113752907724,\n",
       " 0.9566240933434716,\n",
       " 0.7979488619442644,\n",
       " 0.9542680446443886,\n",
       " 0.9619114557216908,\n",
       " 0.7234908196432837,\n",
       " 0.5297094569124025,\n",
       " 0.9354972818802143,\n",
       " 0.7645822150953885,\n",
       " 0.2292274456599663,\n",
       " 0.9806535299482017,\n",
       " 0.3812293901525695,\n",
       " 0.8167081779447095,\n",
       " 0.9636889052802119,\n",
       " 0.5483672520210003,\n",
       " 0.7100097596645355,\n",
       " 0.12705755833664845,\n",
       " 0.9022434705290301,\n",
       " 0.832799087721726,\n",
       " 0.9061948486443223,\n",
       " 0.5970709416373022,\n",
       " 0.495272637030174,\n",
       " 0.4950923155094015,\n",
       " 0.8403223050051722,\n",
       " 0.31721441786864707,\n",
       " 0.3965552346459751,\n",
       " 0.7648680773274652,\n",
       " 0.9695567161872469,\n",
       " 0.7753360657856383,\n",
       " 0.874673103669594,\n",
       " 0.4132112766134328,\n",
       " 0.4444913553780523,\n",
       " 0.33487313307564837,\n",
       " 0.8349486221527231,\n",
       " 0.8424155652523041,\n",
       " 0.7821633661615437,\n",
       " 0.47266556723364467,\n",
       " 0.44325710442559474,\n",
       " 0.7849827663651828,\n",
       " 0.8526606798171997,\n",
       " 0.25562465139504137,\n",
       " 0.6459571515691691,\n",
       " 0.1978737129219647,\n",
       " 0.2740493450699181,\n",
       " 0.08047876622656297,\n",
       " 0.8964892753239335,\n",
       " 0.9199659002238306,\n",
       " 0.6357852502115842,\n",
       " 0.8742994187207058,\n",
       " 0.2966785531105666,\n",
       " 0.9798293113708496,\n",
       " 0.8435884438712021,\n",
       " 0.7908604782203148,\n",
       " 0.8202406322133953,\n",
       " 0.7453654663316135,\n",
       " 0.7012254014097411,\n",
       " 0.9413122970482399,\n",
       " 0.5818279973391829,\n",
       " 0.6463972718551241,\n",
       " 0.9346012518323701,\n",
       " 0.9310750679723148,\n",
       " 0.858414045079001,\n",
       " 0.5785681125418893,\n",
       " 0.9853722909401204,\n",
       " 0.9453477522422528,\n",
       " 0.9421051432346476,\n",
       " 0.8468850306395826,\n",
       " 0.5972165758239811,\n",
       " 0.9643606375003684,\n",
       " 0.9692772384347587,\n",
       " 0.8351464425695354,\n",
       " 0.6989447990368152,\n",
       " 0.8711274169642349,\n",
       " 0.7905628461262275,\n",
       " 0.9509088181216141,\n",
       " 0.8370933750580096,\n",
       " 0.813795337800322,\n",
       " 0.2892431194412297,\n",
       " 0.5919311381619552,\n",
       " 0.540420403871043,\n",
       " 0.20284574437758018,\n",
       " 0.6748098640606321,\n",
       " 0.8250171593550978,\n",
       " 0.9521761442052907,\n",
       " 0.6214543909862124,\n",
       " 0.11071979562784064,\n",
       " 0.9245897864473277,\n",
       " 0.9252028808511537,\n",
       " 0.9256313356859931,\n",
       " 0.2671676246770497,\n",
       " 0.7647898908319144,\n",
       " 0.972590646661561,\n",
       " 0.9045811936773103,\n",
       " 0.8516523209111444,\n",
       " 0.8383520549741285,\n",
       " 0.21251383372421923,\n",
       " 0.9395868702181454,\n",
       " 0.903736729868527,\n",
       " 0.9251668190133983,\n",
       " 0.8892154317477654,\n",
       " 0.6042668605672903,\n",
       " 0.5225445225321014,\n",
       " 0.7822337199901712,\n",
       " 0.8827700686865839,\n",
       " 0.8458810232836624,\n",
       " 0.8747738856693794,\n",
       " 0.6138805403791625,\n",
       " 0.6172224285273716,\n",
       " 0.9111075156721575,\n",
       " 0.8022277264759459,\n",
       " 0.8821526007405642,\n",
       " 0.9407868784049462,\n",
       " 0.9272645309053618,\n",
       " 0.5618810589971214,\n",
       " 0.6245090903906987,\n",
       " 0.947367688499648,\n",
       " 0.5694006445079014,\n",
       " 0.11707640470101915,\n",
       " 0.9398261567641949,\n",
       " 0.6533778472193357,\n",
       " 0.7905294295015006,\n",
       " 0.9311678101276529,\n",
       " 0.5366710313435259,\n",
       " 0.5777225942447268,\n",
       " 0.38433346409222174,\n",
       " 0.5224296635594861,\n",
       " 0.7960934466329115,\n",
       " 0.18565319247286896,\n",
       " 0.8067486401261954,\n",
       " 0.2656618164531116,\n",
       " 0.48460750620940635,\n",
       " 0.5890647230477168,\n",
       " 0.734689810563778,\n",
       " 0.4758845545094589,\n",
       " 0.4300248825344546,\n",
       " 0.8472871254230367,\n",
       " 0.9573556240262656,\n",
       " 0.7955903116999001,\n",
       " 0.5945676327779374,\n",
       " 0.9263842122308139,\n",
       " 0.9103613214246158,\n",
       " 0.26617064095776655,\n",
       " 0.4320048111266103,\n",
       " 0.7208278487468588,\n",
       " 0.9009672121755008,\n",
       " 0.9503152014880345,\n",
       " 0.7455244894685417,\n",
       " 0.5033891573034484,\n",
       " 0.7817421306823862,\n",
       " 0.5971217763834986,\n",
       " 0.8706812492732344,\n",
       " 0.36306701672488245,\n",
       " 0.19518566732776577,\n",
       " 0.9464147434152406,\n",
       " 0.8517785664262443,\n",
       " 0.9235704333617769,\n",
       " 0.46459924414240084,\n",
       " 0.9134922239287147,\n",
       " 0.7853776261724275,\n",
       " 0.9318774358979587,\n",
       " 0.9404527394936002,\n",
       " 0.8542537368577102,\n",
       " 0.5484309576708695,\n",
       " 0.9726724223843937,\n",
       " 0.9561930130267966,\n",
       " 0.985159307718277,\n",
       " 0.6072110108260451,\n",
       " 0.4765065766614059,\n",
       " 0.8784697452495838,\n",
       " 0.5908130062037501,\n",
       " 0.6467471307721631,\n",
       " 0.25190949599290713,\n",
       " 0.7395150599808529,\n",
       " 0.5964984932850147,\n",
       " 0.7989893477538537,\n",
       " 0.5796889356498061,\n",
       " 0.7305924736220262,\n",
       " 0.5781843775305255,\n",
       " 0.5368595303132616,\n",
       " 0.6817051852571554,\n",
       " 0.9770341589533049,\n",
       " 0.8733268143801853,\n",
       " 0.7263375689243448,\n",
       " 0.7105792604643724,\n",
       " 0.5117074678684104,\n",
       " 0.6594581733489859,\n",
       " 0.7557362106339685,\n",
       " 0.9239034632156635,\n",
       " 0.8388016583590672,\n",
       " 0.8301303939572696,\n",
       " 0.980804222616656,\n",
       " 0.8400594261185876,\n",
       " 0.745747229354135,\n",
       " 0.15387342351263966,\n",
       " 0.9301432136831612,\n",
       " 0.17380000496732778,\n",
       " 0.9637417186950815,\n",
       " 0.6547531915122065,\n",
       " 0.3186376288019378,\n",
       " 0.34071267452733267,\n",
       " 0.8035175757161502,\n",
       " 0.6580505149117832,\n",
       " 0.9081264415691639,\n",
       " 0.8465435463806679,\n",
       " 0.7975641856933462,\n",
       " 0.20368963623869007,\n",
       " 0.29601883662158046,\n",
       " 0.6266814135271928,\n",
       " 0.7518244332280652,\n",
       " 0.628155447491284,\n",
       " 0.7877537108700852,\n",
       " 0.9660562667353401,\n",
       " 0.85417527659186,\n",
       " 0.17198399261828384,\n",
       " 0.9477459889033746,\n",
       " 0.8776981035183216,\n",
       " 0.9542282371685422,\n",
       " 0.9034095422974948,\n",
       " 0.9399897258857202,\n",
       " 0.7281932471127346,\n",
       " 0.8782192820105059,\n",
       " 0.15229374946191393,\n",
       " 0.9520417776601068,\n",
       " 0.19522926529933665,\n",
       " 0.8243727250345823,\n",
       " 0.9562825028238625,\n",
       " 0.855725931504677,\n",
       " 0.43921125007086786,\n",
       " 0.9821975864213088,\n",
       " 0.9301457598291594,\n",
       " 0.921937862141379,\n",
       " 0.8888576996737513,\n",
       " 0.3889045067902269,\n",
       " 0.2366785292995387,\n",
       " 0.921418603329823,\n",
       " 0.9541390564934961,\n",
       " 0.9181902443540507,\n",
       " 0.9714992893153224,\n",
       " 0.9092513906544653,\n",
       " 0.32214317249840707,\n",
       " 0.940017682313919,\n",
       " 0.24786367138911938,\n",
       " 0.9355114258568862,\n",
       " 0.7136467664406217,\n",
       " 0.9648930839423475,\n",
       " 0.9166686411561638,\n",
       " 0.7862488175260609,\n",
       " 0.6692991464302458,\n",
       " 0.7097321204070387,\n",
       " 0.912707907783574,\n",
       " 0.801361314387157,\n",
       " 0.8638517237942794,\n",
       " 0.38522835795221655,\n",
       " 0.5853787940123986,\n",
       " 0.57528782260829,\n",
       " 0.9168443799018859,\n",
       " 0.5481551178570452,\n",
       " 0.5131254215692651,\n",
       " 0.12457618988279638,\n",
       " 0.8501423052672682,\n",
       " 0.3329090065997222,\n",
       " 0.3553769147601621,\n",
       " 0.1547121194416079,\n",
       " 0.8590870014552412,\n",
       " 0.9464990895369958,\n",
       " 0.9114392360736584,\n",
       " 0.9022864724027699,\n",
       " 0.9018653666150981,\n",
       " 0.9472054902849526,\n",
       " 0.7723926392094842,\n",
       " 0.7345447051114049,\n",
       " 0.9525981134381787,\n",
       " 0.8331192263241471,\n",
       " 0.9058396170879233,\n",
       " 0.9801830267084055,\n",
       " 0.9955471456050873,\n",
       " 0.38018585834009894,\n",
       " 0.9784583802880912,\n",
       " 0.9075499267413698,\n",
       " 0.5473816489351206,\n",
       " 0.96446447906823,\n",
       " 0.9677250878564243,\n",
       " 0.9234029173851013,\n",
       " 0.7882899411793413,\n",
       " 0.7663939227317942,\n",
       " 0.5657099372354046,\n",
       " 0.880506679107403,\n",
       " 0.2992896127289739,\n",
       " 0.9067436966402778,\n",
       " 0.9533148732678643,\n",
       " 0.4766544652396235,\n",
       " 0.9417746494556296,\n",
       " 0.4062079141879904,\n",
       " 0.7687929720714175,\n",
       " 0.7333969800636686,\n",
       " 0.8017798900604248,\n",
       " 0.7185902644848001,\n",
       " 0.09473658101312045,\n",
       " 0.18245341192031728,\n",
       " 0.800943355519196,\n",
       " 0.18003675279946163,\n",
       " 0.518951869627525,\n",
       " 0.6912064527643138,\n",
       " 0.6692495163144736,\n",
       " 0.8733032333439794,\n",
       " 0.6984600502869178,\n",
       " 0.934426880293879,\n",
       " 0.09374919184323015,\n",
       " 0.8661042959525667,\n",
       " 0.44356526136398317,\n",
       " 0.968557353060821,\n",
       " 0.49129607410266485,\n",
       " 0.5817056451378197,\n",
       " 0.6079388338944007,\n",
       " 0.16145420858058435,\n",
       " 0.962608560816995,\n",
       " 0.6753381437268751,\n",
       " 0.6775669428808936,\n",
       " 0.8914529072827306,\n",
       " 0.8026014739069445,\n",
       " 0.8286961758958882,\n",
       " 0.9198092657944252,\n",
       " 0.8010407897932776,\n",
       " 0.916490206225165,\n",
       " 0.5877287285081272,\n",
       " 0.9498560175813477,\n",
       " 0.6163108842126255,\n",
       " 0.23646808929484467,\n",
       " 0.16584477794581448,\n",
       " 0.7516932201796565,\n",
       " 0.38774878125766227,\n",
       " 0.8068993578697072,\n",
       " 0.99063441157341,\n",
       " 0.9549231677219785,\n",
       " 0.6253679982547102,\n",
       " 0.9869634211063385,\n",
       " 0.6984952579284536,\n",
       " 0.7903311412909935,\n",
       " 0.8210935438501423,\n",
       " 0.886464753849753,\n",
       " 0.6798083087493634,\n",
       " 0.4858498217730687,\n",
       " 0.7143918055912544,\n",
       " 0.9442848226119732,\n",
       " 0.8135268211364746,\n",
       " 0.6323522797946273,\n",
       " 0.9721446884089503,\n",
       " 0.8935403811520544,\n",
       " 0.6308853190520713,\n",
       " 0.8230207603553246,\n",
       " 0.39351701911153464,\n",
       " 0.936736807946501,\n",
       " 0.11716365845038973,\n",
       " 0.9703359556609187,\n",
       " 0.7490249343987169,\n",
       " 0.6186187444062068,\n",
       " 0.9039524055760482,\n",
       " 0.6543783798299987,\n",
       " 0.986692875623703,\n",
       " 0.987169877619579,\n",
       " 0.8426293167574652,\n",
       " 0.8958015289799921,\n",
       " 0.05657316761027122,\n",
       " 0.6323464913614865,\n",
       " 0.6097082822487272,\n",
       " 0.7882274720175513,\n",
       " 0.4890909916368024,\n",
       " 0.8823040590204041,\n",
       " 0.875372314453125,\n",
       " 0.6240741462543093,\n",
       " 0.9417290928034947,\n",
       " 0.9338920496661087,\n",
       " 0.6611788916176763,\n",
       " 0.9546080593405099,\n",
       " 0.7973722988161547,\n",
       " 0.3536291753423625,\n",
       " 0.5840178970632882,\n",
       " 0.055883596831097684,\n",
       " 0.008012901060283184,\n",
       " 0.9321493551648896,\n",
       " 0.8038772356921229,\n",
       " 0.7661066170396476,\n",
       " 0.8942903463182779,\n",
       " 0.7688665727089191,\n",
       " 0.9708313596659693,\n",
       " 0.755888801402059,\n",
       " 0.8674265366176079,\n",
       " 0.3846017072940695,\n",
       " 0.6465539776045701,\n",
       " 0.3497131809078414,\n",
       " 0.40790157287285245,\n",
       " 0.9403829443043676,\n",
       " 0.7697788228248728,\n",
       " 0.976778512782064,\n",
       " 0.8886444677566661,\n",
       " 0.9712173015906893,\n",
       " 0.20378518515619737,\n",
       " 0.9571434407398618,\n",
       " 0.8116068965402143,\n",
       " 0.7802594550724687,\n",
       " 0.3213730646618481,\n",
       " 0.9392579206104936,\n",
       " 0.9731291010462004,\n",
       " 0.627971594292542,\n",
       " 0.9089053382133616,\n",
       " 0.7943590297781188,\n",
       " 0.061378744610681615,\n",
       " 0.5882208797438391,\n",
       " 0.6001045276378763,\n",
       " 0.32929670564059554,\n",
       " 0.8399283349514007,\n",
       " 0.8584516463608578,\n",
       " 0.5074716806411743,\n",
       " 0.6183640687630094,\n",
       " 0.8939093655553357,\n",
       " 0.9062810416879326,\n",
       " 0.895712227245857,\n",
       " 0.9334949988743355,\n",
       " 0.8722177423279861,\n",
       " 0.6150278636093798,\n",
       " 0.3295228273704134,\n",
       " 0.7222226105887315,\n",
       " 0.1536521951186246,\n",
       " 0.9118686614365413,\n",
       " 0.9158192274899318,\n",
       " 0.45769787266336637,\n",
       " 0.6247322680621312,\n",
       " 0.34123439213325235,\n",
       " 0.7692519327689862,\n",
       " 0.9576117283311383,\n",
       " 0.34654910913829146,\n",
       " 0.8623572035082455,\n",
       " 0.43335021652024364,\n",
       " 0.19336270047672863,\n",
       " 0.37482713049855726,\n",
       " 0.605728129888403,\n",
       " 0.8957518996863529,\n",
       " 0.6870316306064869,\n",
       " 0.6221230860414176,\n",
       " 0.674936618887145,\n",
       " 0.6784391133949674,\n",
       " 0.8926506459712982,\n",
       " 0.9341473289604845,\n",
       " 0.7083939924322326,\n",
       " 0.8908429077987013,\n",
       " 0.8883898087616624,\n",
       " 0.947851583258859,\n",
       " 0.6663454997128454,\n",
       " 0.9745292719068198,\n",
       " 0.91872339865257,\n",
       " 0.9047760392057485,\n",
       " 0.8594963042900481,\n",
       " 0.9248121238988021,\n",
       " 0.9231483728721224,\n",
       " 0.5846521630369383,\n",
       " 0.7080471965773352,\n",
       " 0.866044465426741,\n",
       " 0.8060192469892831,\n",
       " 0.9321307398121932,\n",
       " 0.8253009091163503,\n",
       " 0.4951726041991135,\n",
       " 0.5591133031351813,\n",
       " 0.44277430587801436,\n",
       " 0.21433036168073785,\n",
       " 0.9197565510355192,\n",
       " 0.7879160221280723,\n",
       " 0.9078846059996506,\n",
       " 0.6505949702756157,\n",
       " 0.9442365671026296,\n",
       " 0.06869129813115658,\n",
       " 0.23471686670492437,\n",
       " 0.6572396905257785,\n",
       " 0.8939481085744397,\n",
       " 0.6712662051463949,\n",
       " 0.6828208847292538,\n",
       " 0.238172044733475,\n",
       " 0.4724807804000789,\n",
       " 0.8442147565299067,\n",
       " 0.2308992841634257,\n",
       " 0.2907688875650537,\n",
       " 0.3628444797006147,\n",
       " 0.43589824808054956,\n",
       " 0.40446611838094115,\n",
       " 0.8462197990253053,\n",
       " 0.45859514885935293,\n",
       " 0.9436171165828047,\n",
       " 0.8940673318402521,\n",
       " 0.49829998489083915,\n",
       " 0.08926660310605478,\n",
       " 0.9294696168652896,\n",
       " 0.856408370157768,\n",
       " 0.42133610556865564,\n",
       " 0.8707364737987519,\n",
       " 0.9864571988582611,\n",
       " 0.7262386552218734,\n",
       " 0.9768786979132685,\n",
       " 0.9249477877699095,\n",
       " 0.3824205363105083,\n",
       " 0.8913547719347066,\n",
       " 0.908799743035744,\n",
       " 0.8745175914517764,\n",
       " 0.9813558812799125,\n",
       " 0.7772260515854277,\n",
       " 0.9430277123533446,\n",
       " 0.9578145871902335,\n",
       " 0.8937826162782209,\n",
       " 0.8242233800476995,\n",
       " 0.322074373991325,\n",
       " 0.6682171842147564,\n",
       " 0.9059978078151572,\n",
       " 0.9329418556443576,\n",
       " 0.6998891084358609,\n",
       " 0.893498831165248,\n",
       " 0.8970779020210793,\n",
       " 0.8918097781723944,\n",
       " 0.8455516412340361,\n",
       " 0.8802343539122878,\n",
       " 0.9330814581492852,\n",
       " 0.09692217525223207,\n",
       " 0.3906572087057706,\n",
       " 0.5918060576093608,\n",
       " 0.298446999644411,\n",
       " 0.9116906223625972,\n",
       " 0.8157399535179138,\n",
       " 0.09834903790776071,\n",
       " 0.3346384733915329,\n",
       " 0.7028293956970346,\n",
       " 0.7529616544986593,\n",
       " 0.24733743241121028,\n",
       " 0.3560968635411098,\n",
       " 0.7976094342511275,\n",
       " 0.714968236355946,\n",
       " 0.6090824949330297,\n",
       " 0.905544472768389,\n",
       " 0.42589381799615667,\n",
       " 0.4396861306552229,\n",
       " 0.9270728405179649,\n",
       " 0.8582902248563438,\n",
       " 0.8745257794857025,\n",
       " 0.867629075050354,\n",
       " 0.9374430066552655,\n",
       " 0.6206083110694227,\n",
       " 0.8603605122401796,\n",
       " 0.21094633222653947,\n",
       " 0.670419757530607,\n",
       " 0.2591500640943133,\n",
       " 0.26878494193841673,\n",
       " 0.8754465725915186,\n",
       " 0.3924108766276261,\n",
       " 0.7379657973503244,\n",
       " 0.8340564218060724,\n",
       " 0.5877829623633417,\n",
       " 0.7996297297806576,\n",
       " 0.21760133042417723,\n",
       " 0.902602888386825,\n",
       " 0.8896512516613664,\n",
       " 0.372795785090019,\n",
       " 0.32663967249722314,\n",
       " 0.5541453949336348,\n",
       " 0.4565021116158058,\n",
       " 0.5110834109372107,\n",
       " 0.020081108953033028,\n",
       " 0.7915771011648507,\n",
       " 0.7323990400495201,\n",
       " 0.9804887091291361,\n",
       " 0.5265677413036083,\n",
       " 0.9472921359127966,\n",
       " 0.9450006548700661,\n",
       " 0.8357300894013766,\n",
       " 0.5699822113431734,\n",
       " 0.14534863850166058,\n",
       " 0.9233151429686053,\n",
       " 0.93762047290802,\n",
       " 0.9308259606361389,\n",
       " 0.7928917243562896,\n",
       " 0.4853140200006551,\n",
       " 0.9446266655264229,\n",
       " 0.9181979826812087,\n",
       " 0.7994425245400133,\n",
       " 0.9452299800412408,\n",
       " 0.885294515305552,\n",
       " 0.8157558067091579,\n",
       " 0.8187245403898173,\n",
       " 0.30719097590652006,\n",
       " 0.9065171570613466,\n",
       " 0.8590287915591536,\n",
       " 0.9464073292140303,\n",
       " 0.953644191807714,\n",
       " 0.9757081167451267,\n",
       " 0.8447048016663256,\n",
       " 0.8694823092427747,\n",
       " 0.8148203757302515,\n",
       " 0.9019068709735213,\n",
       " 0.7403228741267631,\n",
       " 0.934622536239953,\n",
       " 0.8490442191732341,\n",
       " 0.8448558928637668,\n",
       " 0.8256904523948143,\n",
       " 0.3207914734708852,\n",
       " 0.2750168541895932,\n",
       " 0.5224126819906564,\n",
       " 0.694868821316752,\n",
       " 0.476653247660604,\n",
       " 0.9187398281590692,\n",
       " 0.9454291434123598,\n",
       " 0.2747070718428184,\n",
       " 0.781228446754916,\n",
       " 0.5606106266893189,\n",
       " 0.23241730733164426,\n",
       " 0.8154332892648105,\n",
       " 0.7030661303421546,\n",
       " 0.8949961138182673,\n",
       " 0.5847773607434897,\n",
       " 0.5983906129310871,\n",
       " 0.9733334890727339,\n",
       " 0.9312516150803402,\n",
       " 0.8618789935934132,\n",
       " 0.5905729959750998,\n",
       " 0.8495059660796461,\n",
       " 0.8973962736540827,\n",
       " 0.7066926411513624,\n",
       " 0.5655343931296776,\n",
       " 0.5795543695318288,\n",
       " 0.7721536208843363,\n",
       " 0.8695936153674948,\n",
       " 0.22497682967062654,\n",
       " 0.2604111476191159,\n",
       " 0.06217562421385584,\n",
       " 0.2960478371587293,\n",
       " 0.9757113688978656,\n",
       " 0.9039712815449156,\n",
       " 0.9265194728456694,\n",
       " 0.8693634183242402,\n",
       " 0.9688032380465803,\n",
       " 0.8443852471894231,\n",
       " 0.2857859064792765,\n",
       " 0.6930284697433997,\n",
       " 0.5659851479119268,\n",
       " 0.9584059462465089,\n",
       " 0.980822521242602,\n",
       " 0.8456535670264014,\n",
       " 0.9224276875627452,\n",
       " 0.8806210435669998,\n",
       " 0.8829424714219981,\n",
       " 0.910408380113799,\n",
       " 0.8765630327422043,\n",
       " 0.3549308381203947,\n",
       " 0.9174836964442812,\n",
       " 0.9213514381441577,\n",
       " 0.6454128701111366,\n",
       " 0.9433637214118037,\n",
       " 0.9656147369023027,\n",
       " 0.9583024372314585,\n",
       " 0.924935880817216,\n",
       " 0.9307244278233626,\n",
       " 0.2910155314823677,\n",
       " 0.6977820904090486,\n",
       " 0.9363900579255202,\n",
       " 0.35706759382938513,\n",
       " 0.7677885846845035,\n",
       " 0.8842857876728321,\n",
       " 0.9392619689990734,\n",
       " 0.8969878096005013,\n",
       " 0.8394373714923858,\n",
       " 0.21147731347330684,\n",
       " 0.4853745673237176,\n",
       " 0.992671803564861,\n",
       " 0.6781401533505012,\n",
       " 0.8192825118015552,\n",
       " 0.4045249044895172,\n",
       " 0.8638954037222368,\n",
       " 0.9791816370240574,\n",
       " 0.17947037106957928,\n",
       " 0.7651389074736628,\n",
       " 0.5336643291958447,\n",
       " 0.5930674739952745,\n",
       " 0.07189126892721859,\n",
       " 0.0798179993341709,\n",
       " 0.9477500029679002,\n",
       " 0.9415132224559783,\n",
       " 0.6297441918274451,\n",
       " 0.9230841079662586,\n",
       " 0.9348382333229328,\n",
       " 0.8565981786826562,\n",
       " 0.14809887897351692,\n",
       " 0.946301890447222,\n",
       " 0.96579464612336,\n",
       " 0.8912766078422809,\n",
       " 0.6352755464356521,\n",
       " 0.9199856153849898,\n",
       " 0.9759494997304061,\n",
       " 0.79472056339527,\n",
       " 0.5143040772142081,\n",
       " 0.9326420442811374,\n",
       " 0.7884140512038922,\n",
       " 0.8689220459296785,\n",
       " 0.8079118864289645,\n",
       " 0.9291061086901302,\n",
       " 0.6816314405408399,\n",
       " 0.5239262351701999,\n",
       " 0.8597689053107953,\n",
       " 0.5566939935601991,\n",
       " 0.9054704435940446,\n",
       " 0.6263521463706576,\n",
       " 0.969048781230532,\n",
       " 0.8969275166248454,\n",
       " 0.7926658694086404,\n",
       " 0.9606679090138139,\n",
       " 0.6713702298443893,\n",
       " 0.4552074159013814,\n",
       " 0.8879689952422833,\n",
       " 0.6757324751081137,\n",
       " 0.9236086093146225,\n",
       " 0.9218471364728336,\n",
       " 0.9911472378105952,\n",
       " 0.6780773541022991,\n",
       " 0.9952572584152222,\n",
       " 0.7523608594105162,\n",
       " 0.7653016217823686,\n",
       " 0.8462419635262983,\n",
       " 0.8660932793699462,\n",
       " 0.296305423512541,\n",
       " 0.6927959016684828,\n",
       " 0.2906561236957024,\n",
       " 0.7783920320971258,\n",
       " 0.7006525976904507,\n",
       " 0.24388693499154057,\n",
       " 0.2759084542249811,\n",
       " 0.6646515322142634,\n",
       " 0.8308326597871452,\n",
       " 0.4740230561330401,\n",
       " 0.7370084446052025,\n",
       " 0.9470042855575167,\n",
       " 0.5984746371877605,\n",
       " 0.5089188497641991,\n",
       " 0.5653215200736605,\n",
       " 0.6237246085857523,\n",
       " 0.6901626500590095,\n",
       " 0.9245413959026336,\n",
       " 0.6194722820972574,\n",
       " 0.8355309848127694,\n",
       " 0.9875520663014774,\n",
       " 0.9769291020672897,\n",
       " 0.9738766466749125,\n",
       " 0.907471705921765,\n",
       " 0.8438081792716322,\n",
       " 0.47817786294838477,\n",
       " 0.9593589817655497,\n",
       " 0.9369030724311698,\n",
       " 0.5085181131445129,\n",
       " 0.7373079867198549,\n",
       " 0.8905398350337457,\n",
       " 0.8731993954757165,\n",
       " 0.9513367665225062,\n",
       " 0.9489274088678689,\n",
       " 0.015732476500601604,\n",
       " 0.01980367676708205,\n",
       " 0.6625533683546658,\n",
       " 0.8814066878680525,\n",
       " 0.9688400048634102,\n",
       " 0.858231684873844,\n",
       " 0.9617817233348716,\n",
       " 0.18327583033463052,\n",
       " 0.6268196461529567,\n",
       " 0.6581658468164246,\n",
       " 0.7057746243887935,\n",
       " 0.9281437479216477,\n",
       " 0.8264872548909024,\n",
       " 0.7048339212762899,\n",
       " 0.1948907761738218,\n",
       " 0.4863299550681279,\n",
       " 0.7151438427382502,\n",
       " 0.8048154502079405,\n",
       " 0.9274835025442058,\n",
       " 0.1423975456120639,\n",
       " 0.6352812436120263,\n",
       " 0.7747150904145734,\n",
       " 0.1860812449249728,\n",
       " 0.964009735707579,\n",
       " 0.9402816135307839,\n",
       " 0.8818647392864885,\n",
       " 0.9501286771790735,\n",
       " 0.8390046427989828,\n",
       " 0.5275550305843353,\n",
       " 0.839827533425956,\n",
       " 0.6621842298014411,\n",
       " 0.8370963164444627,\n",
       " 0.6430187599412327,\n",
       " 0.11870981876192421,\n",
       " 0.6541418295482109,\n",
       " 0.9685066471839774,\n",
       " 0.9723707092219386,\n",
       " 0.6916145992690119,\n",
       " 0.7287810960720325,\n",
       " 0.9036610617719847,\n",
       " 0.832047968691793,\n",
       " 0.6148426608792668,\n",
       " 0.7309285696210532,\n",
       " 0.806083163721808,\n",
       " 0.4113147345082513,\n",
       " 0.42037187783882535,\n",
       " 0.7128462139902444,\n",
       " 0.7830982323350577,\n",
       " 0.6448833664943432,\n",
       " 0.6010021306317428,\n",
       " 0.2584118418652436,\n",
       " 0.5922202383649761,\n",
       " 0.8799846373755356,\n",
       " 0.2637632402880439,\n",
       " 0.7437195259949256,\n",
       " 0.7171415195382875,\n",
       " 0.8695547969176851,\n",
       " 0.1693618058130659,\n",
       " 0.4432011692688383,\n",
       " 0.6742285165293463,\n",
       " 0.7923419890732601,\n",
       " 0.9181611414613395,\n",
       " 0.966222025402661,\n",
       " 0.12114168431224494,\n",
       " 0.9144634238604842,\n",
       " 0.8316142815968086,\n",
       " 0.7761647937626674,\n",
       " 0.8888096667569259,\n",
       " 0.923085210652187,\n",
       " 0.9322334088128188,\n",
       " 0.8196058474738022,\n",
       " 0.8629606175011602,\n",
       " 0.9485934068416727,\n",
       " 0.8530488038885182,\n",
       " 0.9258698745020504,\n",
       " 0.915072363615036,\n",
       " 0.7371830003014926,\n",
       " 0.8551820317219043,\n",
       " 0.9125371805552779,\n",
       " 0.9191024077349695,\n",
       " 0.7353531656594112,\n",
       " 0.8822855768532589,\n",
       " 0.5922877455579824,\n",
       " 0.39090528282625925,\n",
       " 0.6850309246572954,\n",
       " 0.7407473905333157,\n",
       " 0.1888300708527195,\n",
       " 0.9580067385887278,\n",
       " 0.6120277661701728,\n",
       " 0.8833902800905293,\n",
       " 0.5548429012298584,\n",
       " 0.8955416089501874,\n",
       " 0.8946903084886485,\n",
       " 0.5501666408160637,\n",
       " 0.42524366902893984,\n",
       " 0.7325388123249186,\n",
       " 0.012095754972562708,\n",
       " 0.35098923506407903,\n",
       " 0.8007576068927502,\n",
       " 0.2229855680773998,\n",
       " 0.8447716809552291,\n",
       " 0.9548728455757273,\n",
       " 0.2751745417200286,\n",
       " 0.9834491489262416,\n",
       " 0.8344632968820375,\n",
       " 0.2856196932751557,\n",
       " 0.72371949574043,\n",
       " 0.26848462367879933,\n",
       " 0.815318815872587,\n",
       " 0.865992807519847,\n",
       " 0.9578662559903901,\n",
       " 0.8369284995670976,\n",
       " 0.9048332243130125,\n",
       " 0.4349773822159603,\n",
       " 0.5866008914750198,\n",
       " 0.9515660255119718,\n",
       " 0.9029843618129862,\n",
       " 0.3664532824836928,\n",
       " 0.1991310432039458,\n",
       " 0.3801924577047085,\n",
       " 0.9594740941606719,\n",
       " 0.8684868584419119,\n",
       " 0.31075920020711834,\n",
       " 0.9062560134920581,\n",
       " 0.9699516966425139,\n",
       " 0.9464007875014996,\n",
       " 0.7931355139304852,\n",
       " 0.9566506622166469,\n",
       " 0.9513202476090399,\n",
       " 0.908126082913629,\n",
       " 0.10904984583371673,\n",
       " 0.710607236623764,\n",
       " 0.7530600973244371,\n",
       " 0.1683132345563379,\n",
       " 0.433877598519983,\n",
       " 0.8954067460421858,\n",
       " 0.8578798795568532,\n",
       " 0.8815940754166964,\n",
       " 0.9397067594117132,\n",
       " 0.5812965652038311,\n",
       " 0.9694873429577926,\n",
       " 0.9168275405620707,\n",
       " 0.4174522059744802,\n",
       " 0.95254592402228,\n",
       " 0.9895641207695007,\n",
       " 0.32645910844720644,\n",
       " 0.2995338945553221,\n",
       " 0.8800021081135191,\n",
       " 0.5760624139473356,\n",
       " 0.8864069819450379,\n",
       " 0.5650666430078704,\n",
       " 0.9081917635325728,\n",
       " 0.3395442263833408,\n",
       " 0.810757646683989,\n",
       " 0.724767393695897,\n",
       " 0.9284438554582924,\n",
       " 0.3813629548097479,\n",
       " 0.44902339002181746,\n",
       " 0.8042756415646652,\n",
       " 0.6675457863972105,\n",
       " 0.8770393688103248,\n",
       " 0.43909761402113684,\n",
       " 0.5523914006249657,\n",
       " 0.27861604885808355,\n",
       " 0.7403580147644568,\n",
       " 0.716542582265262,\n",
       " 0.8337110313875922,\n",
       " 0.8312800316975035,\n",
       " 0.5388965875938021,\n",
       " 0.43528935107691535,\n",
       " 0.6829922014269336,\n",
       " 0.7661342086463139,\n",
       " 0.18975380951988285,\n",
       " 0.9557524508443371,\n",
       " 0.5512861759498202,\n",
       " 0.8263488352298737,\n",
       " 0.08861736693772776,\n",
       " 0.9072369762535752,\n",
       " 0.413419791131184,\n",
       " 0.6583208443789647,\n",
       " 0.8881439190486382,\n",
       " 0.8109445150556236,\n",
       " 0.5250250781404562,\n",
       " 0.9127596279670452,\n",
       " 0.5801041360559135,\n",
       " 0.7163069928514547,\n",
       " 0.3790669774187022,\n",
       " 0.8366134581894711,\n",
       " 0.5188086591917893,\n",
       " 0.9469875319250698,\n",
       " 0.9560857544685233,\n",
       " 0.9666150658295072,\n",
       " 0.8998481674440976,\n",
       " 0.9697698726736266,\n",
       " 0.5099896593340512,\n",
       " 0.9417494440900869,\n",
       " 0.7771456506745569,\n",
       " 0.8870439480090964,\n",
       " 0.9047692964816916,\n",
       " 0.8894435761303737,\n",
       " 0.4644482423519266,\n",
       " 0.9264483626546531,\n",
       " 0.8217146559008237,\n",
       " 0.6536140694700439,\n",
       " 0.9519697333204336,\n",
       " 0.8137737888714363,\n",
       " 0.7413636437777815,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict_temp.append((Y_predict_rf[i] + Y_predict_gbc[i]) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict\n",
    "submit_score.to_csv('predict_result_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict_temp\n",
    "submit_score.to_csv('predict_result_ncnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AA = load_label('predict_result_488.csv')\n",
    "aa = AA.score.as_matrix(columns=None).tolist()\n",
    "BB = load_label('predict_result_495.csv')\n",
    "bb = BB.score.as_matrix(columns=None).tolist()\n",
    "CC = load_label('predict_result_509.csv')\n",
    "cc = CC.score.as_matrix(columns=None).tolist()\n",
    "DD = load_label('predict_result_cnn.csv')\n",
    "dd = DD.score.as_matrix(columns=None).tolist()\n",
    "\n",
    "comb = []\n",
    "for i in range(len(cc)):\n",
    "    comb.append((aa[i] + bb[i] + cc[i] + dd[i]) / 4.0)\n",
    "\n",
    "#comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = dd\n",
    "submit_score.to_csv('combined.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(train_file,values = 'price', index=['lvl1','lvl2','lvl3'],columns=['type'],aggfunc=[min, max, np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
