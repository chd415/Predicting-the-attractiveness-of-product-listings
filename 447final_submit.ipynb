{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chd415/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "sns.set(color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                   \n",
    "import nltk                                         \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer  \n",
    "LA = np.linalg\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "from gensim.models.word2vec import Word2Vec                                  \n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data text\n",
    "def load_data(filename):\n",
    "    load_file = pd.read_csv(filename,delimiter=',', header=0,\n",
    "                        dtype={'name':str, 'lvl1':str, 'lvl2':str, 'lvl3':str, 'descrption':str, 'type':str})\n",
    "    load_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "    load_file.duplicated(subset=None, keep='first')\n",
    "    load_file.set_index('id', inplace = True)\n",
    "    load_file.head()\n",
    "    return load_file\n",
    "#print(len(train_file))\n",
    "def load_label(filename):\n",
    "    load_label = pd.read_csv(filename,delimiter=',', header=0)\n",
    "    load_label.columns = ['id', 'score']\n",
    "    load_label.duplicated(subset=None, keep='first')\n",
    "    load_label.set_index('id', inplace = True)\n",
    "    return load_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def map_mathod(column):\n",
    "    values = []\n",
    "    indexs = []\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for count in range(len(train_file)):\n",
    "        value = train_file.get_value(count+1,column)\n",
    "        if value in values and value != np.nan:\n",
    "            continue\n",
    "        values.append(value)\n",
    "        indexs.append(len(values))\n",
    "    for j in range(len(indexs)):\n",
    "        mapping[values[j]] = indexs[j]\n",
    "    mapping[np.nan] = 0.0\n",
    "    return mapping\n",
    "#train_file['lvl3'] = train_file['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "#mapping_lvl3 = map_mathod('lvl3')\n",
    "#print(mapping_lvl3)\n",
    "'''\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=10000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(max_features=10000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=1,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "    \n",
    "    return avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "temp = load_data('train_data.csv')\n",
    "temp['descrption'] = temp['descrption'].str.lower()\n",
    "description_X = temp.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "def test_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=5000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english',max_features=5000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=5,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "\n",
    "    tf_w_data = []\n",
    "    tf_data = tf_data.toarray()\n",
    "    i = 0\n",
    "    for row in splitted:\n",
    "        vec = [0 for i in range(50)]\n",
    "    \n",
    "        temp_tfidf = []\n",
    "        for val in tf_data[i]:\n",
    "            if val != 0:\n",
    "                temp_tfidf.append(val)\n",
    "    \n",
    "        count = 0\n",
    "        tf_idf_sum = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                count += 1\n",
    "                tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "                vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        if (tf_idf_sum == 0):\n",
    "            tf_w_data.append(vec)\n",
    "        else:\n",
    "            tf_w_data.append(vec/tf_idf_sum)\n",
    "#            vec = float(1/tf_idf_sum) * vec\n",
    "#        tf_w_data.append(vec)\n",
    "        i = i + 1\n",
    "    \n",
    "    return tf_w_data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "temp['descrption'] = test_embedding(description_X)\n",
    "temp.descrption.head()\n",
    "#tfidf_n = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "    filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].str.lower()\n",
    "    filename['name'] = filename['name'].str.lower()\n",
    "\n",
    "    '''\n",
    "    \n",
    "    mapping_lvl1 = map_mathod('lvl1')\n",
    "    mapping_lvl2 = map_mathod('lvl2')\n",
    "    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "    filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "    filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    '''\n",
    "    #clean up data for lvl 1&2&3\n",
    "    temp =  filename.drop(['price', 'descrption','name','type'], axis=1)\n",
    "    outfile = MultiColumnLabelEncoder(columns = ['lvl1','lvl2','lvl3']).fit_transform(temp.astype(str))\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(outfile)\n",
    "    outfile = enc.transform(outfile).toarray()\n",
    "    \n",
    "    #normalize price\n",
    "    maxp = filename.price.max()\n",
    "    valuethred = 1000.\n",
    "    filename['price'] = filename['price'].clip(lower=0.,upper=valuethred)\n",
    "    #filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    #hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "    ld = filename.price.as_matrix(columns=None).tolist()\n",
    "    outfile = np.column_stack((outfile,ld))\n",
    "\n",
    "    #clean up type \n",
    "    mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "    filename['type'] = filename['type'].map(mapping_type)\n",
    "    le = filename.type.as_matrix(columns=None).tolist()\n",
    "    outfile = np.column_stack((outfile,le))\n",
    "\n",
    "    #clean up text\n",
    "    description_X = filename.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    count_descrption = description_X.str.count('final').fillna(0).tolist()\n",
    "    outfile = np.column_stack((outfile,count_descrption))\n",
    "    \n",
    "    description_X = filename.descrption.str.lower().replace('final ','',regex=True)\n",
    "    filename['descrption'] = text_embedding(description_X)\n",
    " \n",
    "    lg = filename.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "    s_new = s[:8]\n",
    "    U_new = U[:, :8]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    lg_min = np.min(new_lg)\n",
    "    lg_out = new_lg  - (lg_min-2)*np.ones_like(new_lg.size)\n",
    "    outfile = np.column_stack((outfile,lg_out))\n",
    "    \n",
    "    \n",
    "    name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['name'] = text_embedding(name_X)\n",
    "    \n",
    "    lf = filename.name.as_matrix(columns=None).tolist()\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "    sf_new = sf[:8]\n",
    "    Uf_new = Uf[:, :8]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    lf_min = np.min(new_lf)\n",
    "    lf_out = new_lf  - (lf_min-2)*np.ones_like(new_lf.size)\n",
    "    outfile = np.column_stack((outfile,lf_out))\n",
    "   \n",
    "\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_file = pd.read_csv('train_data.csv',delimiter=',', header=0, nrows=10)\n",
    "train_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "description_X = train_file.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "count_descrption = description_X.str.count('final')\n",
    "description_X = train_file.descrption.str.lower().replace('final ','',regex=True)\n",
    "print(np.shape(count_descrption))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36283, 270)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = load_data('train_data.csv')\n",
    "test_file = load_data('test_data.csv')\n",
    "combined_file = pd.concat([train_file,test_file])\n",
    "cleaned_train = clean_data(combined_file)\n",
    "train_score = load_label('train_label.csv')\n",
    "np.shape(cleaned_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# test cell\n",
    "cleaned_train['descrption'] = temp['descrption']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ..., 10.13324131,\n",
       "        11.99554897, 11.35688418],\n",
       "       [ 0.        ,  1.        ,  0.        , ..., 11.9036486 ,\n",
       "         9.82575646, 10.58128699],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 11.73378475,\n",
       "        11.62101979, 11.51830028],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 12.33374908,\n",
       "        11.35996205, 11.5136934 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 11.17770531,\n",
       "        11.97945511, 11.61931454],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 10.76861269,\n",
       "        11.21852384, 11.04587675]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train\n",
    "#print(mapping_lvl1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36283, 270)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "def rearrange(cleaned_data):\n",
    "    la = cleaned_data.lvl1.as_matrix(columns=None).tolist()\n",
    "    lb = cleaned_data.lvl2.as_matrix(columns=None).tolist()\n",
    "    lc = cleaned_data.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = la\n",
    "    X = np.column_stack((X,lb))\n",
    "    X = np.column_stack((X,lc))\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    \n",
    "    ld = cleaned_data.price.as_matrix(columns=None).tolist()\n",
    "    le = cleaned_data.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = np.column_stack((X,ld))\n",
    "    X = np.column_stack((X,le))\n",
    "\n",
    "   \n",
    "    lf = cleaned_data.name.as_matrix(columns=None).tolist()\n",
    "    lg = cleaned_data.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "# only use U \\cdot s\n",
    "    \n",
    "#    s[8:] = 0.\n",
    "#    new_lg = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "    s_new = s[:5]\n",
    "    U_new = U[:, :5]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    lg_min = np.min(new_lg)\n",
    "    lg_out = new_lg  - (lg_min-2)*np.ones_like(new_lg.size)\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "#    sf[5:] = 0.\n",
    "#    new_lf = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "    sf_new = sf[:5]\n",
    "    Uf_new = Uf[:, :5]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    lf_min = np.min(new_lf)\n",
    "    lf_out = new_lf  - (lf_min-2)*np.ones_like(new_lf.size)\n",
    "\n",
    "    \n",
    "    X = np.column_stack((X,lf_out))\n",
    "    X = np.column_stack((X,lg_out))\n",
    "    X = X.tolist()\n",
    "   \n",
    "    return X,lf_min,lg_min\n",
    "'''\n",
    "    \n",
    "    #print(len(X))\n",
    "X = cleaned_train#rearrange(cleaned_train)\n",
    "print(np.shape(np.array(X)))\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "X[137,253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18141\n",
      "(18141, 270)\n",
      "(18142, 270)\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_train[:18141]\n",
    "XX = cleaned_train[18141:]\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "print(np.size(Y))\n",
    "print(np.shape(X))\n",
    "print(np.shape(XX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "maxlen = 270\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, dtype='float32')\n",
    "#X_train = np.any(np.isnan(X_train))\n",
    "#X_train = np.all(np.isfinite(X_train))\n",
    "print(X_train[1400].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Classifier\n",
    "def linear_regression_classifier(train_x, train_y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    param_grid = {'alpha': [math.pow(10,-i) for i in range(11)]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = MultinomialNB(alpha = best_parameters['alpha'])  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    param_grid = {'n_neighbors': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = best_parameters['n_neighbors'], algorithm='kd_tree')\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(100,300,10))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    #for para, val in best_parameters.items():\n",
    "        #print para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "def feature_select(x,y):\n",
    "    clf = ExtraTreesClassifier()\n",
    "    clf = clf.fit(x, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "******************* KNN ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 16.900737s!\n",
      "precision: 69.16%, recall: 100.00%\n",
      "accuracy: 69.17%\n",
      "[0.6920215  0.69588588 0.69051065]\n",
      "******************* LR ********************\n",
      "training took 0.241711s!\n",
      "precision: 77.36%, recall: 90.43%\n",
      "accuracy: 75.09%\n",
      "[0.72219926 0.7366136  0.73433947]\n",
      "******************* RF ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   19.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 21.534177s!\n",
      "precision: 79.11%, recall: 89.36%\n",
      "accuracy: 76.33%\n",
      "[0.73770153 0.7506719  0.74260906]\n",
      "******************* DT ********************\n",
      "training took 0.650436s!\n",
      "precision: 71.19%, recall: 92.07%\n",
      "accuracy: 68.75%\n",
      "[0.68561389 0.66694232 0.68658259]\n",
      "******************* GBC ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 410.052062s!\n",
      "precision: 79.57%, recall: 90.35%\n",
      "accuracy: 77.29%\n",
      "[0.75010335 0.76431673 0.76121563]\n"
     ]
    }
   ],
   "source": [
    "# just for my own record\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['KNN','LR','RF','DT','GBC']    \n",
    "    classifiers = {\n",
    "                   'KNN':knn_classifier,\n",
    "                    'LR':logistic_regression_classifier,\n",
    "                    'RF':random_forest_classifier,\n",
    "                    'DT':decision_tree_classifier,\n",
    "                    'GBC':gradient_boosting_classifier\n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_validation = select_model.transform(X_validation)\n",
    "\n",
    "    result = []\n",
    "        \n",
    "    for classifier in test_classifiers:    \n",
    "        print('******************* %s ********************' % classifier)    \n",
    "        start_time = time.time()    \n",
    "        model = classifiers[classifier](X_train, y_train)   \n",
    "        print('training took %fs!' % (time.time() - start_time))    \n",
    "        predict = model.predict(X_validation)\n",
    "\n",
    "        precision = metrics.precision_score(y_validation, predict)    \n",
    "        recall = metrics.recall_score(y_validation, predict)    \n",
    "        print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))    \n",
    "        accuracy = metrics.accuracy_score(y_validation, predict)    \n",
    "        print('accuracy: %.2f%%' % (100 * accuracy))\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  1.      ,  0.      , ..., 11.774629, 11.737314,\n",
       "        11.739768],\n",
       "       [ 0.      ,  0.      ,  0.      , ..., 11.737541, 12.21775 ,\n",
       "        11.256157],\n",
       "       [ 1.      ,  0.      ,  0.      , ..., 11.562332, 11.451252,\n",
       "        11.531164],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  1.      , ..., 11.248144, 11.919338,\n",
       "        12.642514],\n",
       "       [ 0.      ,  1.      ,  0.      , ..., 11.222754, 11.798913,\n",
       "        11.439027],\n",
       "       [ 0.      ,  1.      ,  0.      , ..., 11.568027, 11.701103,\n",
       "        11.861883]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "#X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "14512 train sequences\n",
      "3629 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (14512, 270)\n",
      "x_test shape: (3629, 270)\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#test cnn model\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Embedding\n",
    "max_features = 15000\n",
    "maxlen = 270\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 5\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "#X_train, X_validation, y_train, y_validation\n",
    "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "#X_train = np.asarray(np.abs(X))\n",
    "X_train = np.asarray(np.abs(X_train))\n",
    "X_validation = np.asarray(np.abs(X_validation))\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_validation), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, padding='post')\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_validation.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(max_features, embedding_size,input_length=maxlen))\n",
    "#model.add(Dense(32, activation='relu', input_dim=100))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "cnnmodel.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=pool_size))\n",
    "cnnmodel.add(LSTM(lstm_output_size))\n",
    "cnnmodel.add(Dense(1))\n",
    "cnnmodel.add(Activation('sigmoid'))\n",
    "\n",
    "cnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "training took 0.241209s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   25.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 27.739618s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  8.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 546.078951s!\n",
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['LR','RF','GBC']    \n",
    "    classifiers = {\n",
    "                   'LR':logistic_regression_classifier,\n",
    "                   'RF':random_forest_classifier,\n",
    "                   'GBC':gradient_boosting_classifier\n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "#    X_test = rearrange(Xt)\n",
    "#    X_test = Xt\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_test = select_model.transform(X_test)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = classifiers['LR'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_lr = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['RF'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_rf = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['GBC'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_gbc = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "        \n",
    "#    for classifier in test_classifiers:    \n",
    "#        print('******************* %s ********************' % classifier)    \n",
    "#        start_time = time.time()    \n",
    "#        model = classifiers[classifier](X_train, y_train)   \n",
    "#        print('training took %fs!' % (time.time() - start_time))    \n",
    "#        Y_predict = model.predict_proba(X_test)[:,1]\n",
    "#        print('predict finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18141/18141 [==============================] - 60s 3ms/step - loss: 0.5846 - acc: 0.6992\n",
      "Epoch 2/5\n",
      "18141/18141 [==============================] - 59s 3ms/step - loss: 0.5381 - acc: 0.7351\n",
      "Epoch 3/5\n",
      "18141/18141 [==============================] - 59s 3ms/step - loss: 0.5243 - acc: 0.7417\n",
      "Epoch 4/5\n",
      "18141/18141 [==============================] - 62s 3ms/step - loss: 0.5139 - acc: 0.7487\n",
      "Epoch 5/5\n",
      "18141/18141 [==============================] - 65s 4ms/step - loss: 0.5042 - acc: 0.7539\n"
     ]
    }
   ],
   "source": [
    "#cnn test\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "#X_test = np.asarray(Xt)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "cnnmodel.fit(X_train, y_train, batch_size=128, epochs=5)\n",
    "\n",
    "Y_predict_cnn = cnnmodel.predict(X_test, verbose=0)\n",
    "Y_predict_cnn = np.squeeze(Y_predict_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "18141/18141 [==============================] - 1s 59us/step - loss: 0.6737 - acc: 0.6873\n",
      "Epoch 2/25\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.5794 - acc: 0.7171\n",
      "Epoch 3/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5597 - acc: 0.7253\n",
      "Epoch 4/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5516 - acc: 0.7259\n",
      "Epoch 5/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5565 - acc: 0.7264\n",
      "Epoch 6/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5491 - acc: 0.7297\n",
      "Epoch 7/25\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.5310 - acc: 0.7361\n",
      "Epoch 8/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5338 - acc: 0.7349\n",
      "Epoch 9/25\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.5247 - acc: 0.7377\n",
      "Epoch 10/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5242 - acc: 0.7415\n",
      "Epoch 11/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5275 - acc: 0.7386\n",
      "Epoch 12/25\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.5212 - acc: 0.7438\n",
      "Epoch 13/25\n",
      "18141/18141 [==============================] - 0s 24us/step - loss: 0.5219 - acc: 0.7452\n",
      "Epoch 14/25\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.5212 - acc: 0.7422\n",
      "Epoch 15/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5218 - acc: 0.7409\n",
      "Epoch 16/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5178 - acc: 0.7452\n",
      "Epoch 17/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5130 - acc: 0.7465\n",
      "Epoch 18/25\n",
      "18141/18141 [==============================] - 1s 28us/step - loss: 0.5088 - acc: 0.7495\n",
      "Epoch 19/25\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.5174 - acc: 0.7449\n",
      "Epoch 20/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5054 - acc: 0.7480\n",
      "Epoch 21/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5094 - acc: 0.7462\n",
      "Epoch 22/25\n",
      "18141/18141 [==============================] - 0s 24us/step - loss: 0.5045 - acc: 0.7511\n",
      "Epoch 23/25\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.4998 - acc: 0.7534\n",
      "Epoch 24/25\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.5001 - acc: 0.7516\n",
      "Epoch 25/25\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.4994 - acc: 0.7551\n",
      "18141/18141 [==============================] - 1s 28us/step\n",
      "\n",
      "acc: 75.64%\n"
     ]
    }
   ],
   "source": [
    "#nn train\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_array = np.asarray(X_train)\n",
    "Y_array = np.asarray(y_train)\n",
    "Xtest_array = np.asarray(X_test) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_array\n",
    "Y = Y_array\n",
    "# create model\n",
    "nnmodel = Sequential()\n",
    "nnmodel.add(Dense(100, input_dim=270, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "nnmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "nnmodel.fit(X, Y, epochs=25, batch_size=150)\n",
    "# evaluate the model\n",
    "scores = nnmodel.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (nnmodel.metrics_names[1], scores[1]*100))\n",
    "Y_predict_nn = nnmodel.predict(Xtest_array, verbose=0)\n",
    "Y_predict_nn = np.squeeze(Y_predict_nn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8579925 , 0.94041866, 0.8313853 , ..., 0.70595163, 0.50523454,\n",
       "       0.61891323], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8295582 , 0.78621113, 0.6645141 , ..., 0.64757854, 0.41389206,\n",
       "       0.569912  ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73480094, 0.67819865, 0.68934352, ..., 0.62148617, 0.41585488,\n",
       "       0.49107215])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77777778, 0.83333333, 0.61111111, ..., 0.66666667, 0.27777778,\n",
       "       0.72222222])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68636364, 0.79545455, 0.71818182, ..., 0.68636364, 0.52272727,\n",
       "       0.58636364])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict.append(( Y_predict_rf[i] + Y_predict_gbc[i] + Y_predict_cnn[i] + Y_predict_nn[i]) / 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7879230195223683,\n",
       " 0.8388544181079576,\n",
       " 0.7062980920377404,\n",
       " 0.943190951750736,\n",
       " 0.768477741034344,\n",
       " 0.7789613797508105,\n",
       " 0.49860520630773875,\n",
       " 0.7843875995790115,\n",
       " 0.7212234910690423,\n",
       " 0.7845466396122267,\n",
       " 0.8631969074709247,\n",
       " 0.3807709932026237,\n",
       " 0.6003489641830174,\n",
       " 0.40475087120677483,\n",
       " 0.8834678248323575,\n",
       " 0.2311255061265194,\n",
       " 0.873183729010399,\n",
       " 0.6564867117489227,\n",
       " 0.8245094442307347,\n",
       " 0.5769025559527705,\n",
       " 0.9085451350970701,\n",
       " 0.8365589516632485,\n",
       " 0.8573541283005416,\n",
       " 0.8478584218506862,\n",
       " 0.9570992681113156,\n",
       " 0.6973043681997242,\n",
       " 0.7669518869633627,\n",
       " 0.9678706323558635,\n",
       " 0.8102602714239949,\n",
       " 0.9404498155670937,\n",
       " 0.8150021161394889,\n",
       " 0.7075678125926943,\n",
       " 0.8056072395558309,\n",
       " 0.6466429714602653,\n",
       " 0.720919211645319,\n",
       " 0.45648324354128406,\n",
       " 0.6798729751748268,\n",
       " 0.9523854328827425,\n",
       " 0.5096632492963714,\n",
       " 0.8012050654852029,\n",
       " 0.5585386865367793,\n",
       " 0.8966844571961297,\n",
       " 0.9577842205762863,\n",
       " 0.6449160896166406,\n",
       " 0.5598847478327125,\n",
       " 0.8654995611821762,\n",
       " 0.7667021310690678,\n",
       " 0.31053749369551437,\n",
       " 0.9412690380606988,\n",
       " 0.6922423969314556,\n",
       " 0.6682960966018715,\n",
       " 0.8878353593927442,\n",
       " 0.7403141237569577,\n",
       " 0.5666097553390446,\n",
       " 0.34242223821806184,\n",
       " 0.5745137103579261,\n",
       " 0.5056084916597665,\n",
       " 0.8443176256887841,\n",
       " 0.31077154433034887,\n",
       " 0.652228018490955,\n",
       " 0.7001406736747183,\n",
       " 0.5219168139377026,\n",
       " 0.40930632921782406,\n",
       " 0.656459810562206,\n",
       " 0.3910168477983186,\n",
       " 0.9005869375936912,\n",
       " 0.6207800335956342,\n",
       " 0.8155419999300831,\n",
       " 0.7215054362410247,\n",
       " 0.674287120471097,\n",
       " 0.4946316438792932,\n",
       " 0.44189750848695486,\n",
       " 0.38850753680624145,\n",
       " 0.670125709790172,\n",
       " 0.8459431837604503,\n",
       " 0.456926736356032,\n",
       " 0.649020043858374,\n",
       " 0.6364261954119711,\n",
       " 0.35317333382789534,\n",
       " 0.49661802042915365,\n",
       " 0.5756961399557614,\n",
       " 0.3289054932768899,\n",
       " 0.2922133699661553,\n",
       " 0.5969380459099105,\n",
       " 0.6982537371642662,\n",
       " 0.6672041084880781,\n",
       " 0.9109541324051944,\n",
       " 0.38655630711353184,\n",
       " 0.9205146239562468,\n",
       " 0.7611620288605642,\n",
       " 0.847835368550185,\n",
       " 0.7250494646002548,\n",
       " 0.6386729278347709,\n",
       " 0.8814101485591946,\n",
       " 0.9426997240143593,\n",
       " 0.8593327081865735,\n",
       " 0.8854973321009164,\n",
       " 0.761613302670344,\n",
       " 0.8395708713567618,\n",
       " 0.7394665759320211,\n",
       " 0.8789345564866307,\n",
       " 0.9001224914584497,\n",
       " 0.7608284189243509,\n",
       " 0.890749600379154,\n",
       " 0.8078777135020554,\n",
       " 0.8980867635120044,\n",
       " 0.6896643446852462,\n",
       " 0.9010148370506788,\n",
       " 0.5399896084359198,\n",
       " 0.9436106944626028,\n",
       " 0.7173233939240677,\n",
       " 0.5830297254853778,\n",
       " 0.910392844135111,\n",
       " 0.840965419947499,\n",
       " 0.7943909067698199,\n",
       " 0.6443230259900141,\n",
       " 0.2854903297592895,\n",
       " 0.7789211454114529,\n",
       " 0.5589566548063297,\n",
       " 0.47925539472789475,\n",
       " 0.7434961258461981,\n",
       " 0.8142665823601714,\n",
       " 0.6737935066524179,\n",
       " 0.32949335013676173,\n",
       " 0.7789133901848937,\n",
       " 0.8469757748974693,\n",
       " 0.8763380272520913,\n",
       " 0.5939704627099663,\n",
       " 0.534669560449894,\n",
       " 0.8890788649970836,\n",
       " 0.9061540941999416,\n",
       " 0.8587573018639979,\n",
       " 0.6894106894129455,\n",
       " 0.42708793359272407,\n",
       " 0.9092364098387535,\n",
       " 0.7932932148979168,\n",
       " 0.9141534354048546,\n",
       " 0.8778116136488288,\n",
       " 0.31902512615979317,\n",
       " 0.5606782776689289,\n",
       " 0.799687826181903,\n",
       " 0.5312301490945046,\n",
       " 0.8372125069300334,\n",
       " 0.6845006552910564,\n",
       " 0.7081792589389917,\n",
       " 0.42824005556076467,\n",
       " 0.8511270585084202,\n",
       " 0.8249551722196617,\n",
       " 0.8076245030068387,\n",
       " 0.9575832835652611,\n",
       " 0.8483461627755502,\n",
       " 0.6348126131928329,\n",
       " 0.4342086132728692,\n",
       " 0.910660274432163,\n",
       " 0.7441339467812066,\n",
       " 0.5487191449813169,\n",
       " 0.9428483106873252,\n",
       " 0.6458503601828007,\n",
       " 0.6478715577510872,\n",
       " 0.6748445736037361,\n",
       " 0.6227878210821537,\n",
       " 0.47721030353897753,\n",
       " 0.38173084201836827,\n",
       " 0.5624694144936523,\n",
       " 0.4064605542204597,\n",
       " 0.42283884106260355,\n",
       " 0.7142826438552201,\n",
       " 0.25735870097773245,\n",
       " 0.300363426979142,\n",
       " 0.8148475043701403,\n",
       " 0.7483688134737689,\n",
       " 0.18327714607420592,\n",
       " 0.187424812940034,\n",
       " 0.5018928777238335,\n",
       " 0.8774436646940732,\n",
       " 0.7461095380060601,\n",
       " 0.5821547227074402,\n",
       " 0.7574544534237698,\n",
       " 0.9182699016248337,\n",
       " 0.4044226799348388,\n",
       " 0.9030570583813118,\n",
       " 0.9534332047809254,\n",
       " 0.6053482233273862,\n",
       " 0.8338723847059288,\n",
       " 0.6746717732663107,\n",
       " 0.4710536087101156,\n",
       " 0.7165282924668958,\n",
       " 0.5253831539792244,\n",
       " 0.5434089610672961,\n",
       " 0.5374659979584241,\n",
       " 0.4923096076707647,\n",
       " 0.9242050391556036,\n",
       " 0.7847563390779977,\n",
       " 0.9297017475270262,\n",
       " 0.24365215433968437,\n",
       " 0.8239973632675228,\n",
       " 0.8743608000001521,\n",
       " 0.7637416302254706,\n",
       " 0.8781491487917273,\n",
       " 0.5730877148834141,\n",
       " 0.4451484339857342,\n",
       " 0.8534584199238305,\n",
       " 0.8504844195312924,\n",
       " 0.9633799940347672,\n",
       " 0.8727364252613048,\n",
       " 0.5896017647752858,\n",
       " 0.7880152231515056,\n",
       " 0.7866893572638733,\n",
       " 0.6372686437585138,\n",
       " 0.557448315635474,\n",
       " 0.5232809077308636,\n",
       " 0.6768979428392468,\n",
       " 0.41333027170463044,\n",
       " 0.6733820088885047,\n",
       " 0.3901863222471391,\n",
       " 0.37588769712231374,\n",
       " 0.6202792083073144,\n",
       " 0.6115061372819574,\n",
       " 0.9277016359146195,\n",
       " 0.7874259028470878,\n",
       " 0.5903055927970193,\n",
       " 0.674875012612102,\n",
       " 0.5501333484745989,\n",
       " 0.8095936993757884,\n",
       " 0.7760574921514048,\n",
       " 0.8920446934724094,\n",
       " 0.7165780002420599,\n",
       " 0.7187719467613432,\n",
       " 0.8814212735253151,\n",
       " 0.5807313810845818,\n",
       " 0.3672774839130315,\n",
       " 0.3771320255717846,\n",
       " 0.7178939505056902,\n",
       " 0.23993113250518688,\n",
       " 0.8084332549511785,\n",
       " 0.6688980896665593,\n",
       " 0.5472185995843676,\n",
       " 0.4059818581649751,\n",
       " 0.4196123341117242,\n",
       " 0.48654953126955514,\n",
       " 0.48960989256097814,\n",
       " 0.8181980276348615,\n",
       " 0.6550325908444145,\n",
       " 0.4042796959178616,\n",
       " 0.26801735141662636,\n",
       " 0.32224028688864875,\n",
       " 0.792819875689468,\n",
       " 0.5036875356929471,\n",
       " 0.41166821430728895,\n",
       " 0.9129034184446239,\n",
       " 0.6911988183103427,\n",
       " 0.682110450574846,\n",
       " 0.8094428915267039,\n",
       " 0.7789399079903208,\n",
       " 0.8764303402166174,\n",
       " 0.8697151045907627,\n",
       " 0.8709752200227795,\n",
       " 0.8973718220236325,\n",
       " 0.8913357615470886,\n",
       " 0.7886683804519249,\n",
       " 0.9222270407459953,\n",
       " 0.3868860996281258,\n",
       " 0.8841624048322139,\n",
       " 0.93529912693934,\n",
       " 0.7465166455567485,\n",
       " 0.42523248632146854,\n",
       " 0.9372259506974557,\n",
       " 0.9483359392243202,\n",
       " 0.8820688567077271,\n",
       " 0.7292473619935489,\n",
       " 0.7551258529376502,\n",
       " 0.6753964922644875,\n",
       " 0.866395427933847,\n",
       " 0.9497094652869484,\n",
       " 0.7964538020919067,\n",
       " 0.9300987685268576,\n",
       " 0.9003800843099151,\n",
       " 0.4278203835391035,\n",
       " 0.9267455072414995,\n",
       " 0.47436560932735,\n",
       " 0.8086494818781361,\n",
       " 0.7873489572845325,\n",
       " 0.8615791296116029,\n",
       " 0.7466026792923609,\n",
       " 0.689918143219418,\n",
       " 0.6607753328602723,\n",
       " 0.4456969882653217,\n",
       " 0.749690659268938,\n",
       " 0.7708888434099429,\n",
       " 0.734322137934993,\n",
       " 0.5580578808230583,\n",
       " 0.7534403522207279,\n",
       " 0.5241464212837845,\n",
       " 0.8705191810022701,\n",
       " 0.42413503350031495,\n",
       " 0.35287840304651646,\n",
       " 0.6580460749641813,\n",
       " 0.7663515042777014,\n",
       " 0.6625641684339504,\n",
       " 0.5929596339211319,\n",
       " 0.32459908910471985,\n",
       " 0.889376096924146,\n",
       " 0.9109267601762155,\n",
       " 0.9192031358829652,\n",
       " 0.8022899895304381,\n",
       " 0.9361279361476802,\n",
       " 0.7880824397007624,\n",
       " 0.5849414509053182,\n",
       " 0.8972358349898849,\n",
       " 0.8594265683130784,\n",
       " 0.8043501635091473,\n",
       " 0.7545945063684926,\n",
       " 0.9132316573099657,\n",
       " 0.9459113776683807,\n",
       " 0.4982894633454506,\n",
       " 0.9029795646366446,\n",
       " 0.8340057898350436,\n",
       " 0.44305764192884617,\n",
       " 0.8350959051137019,\n",
       " 0.8564420763591323,\n",
       " 0.8746259006285908,\n",
       " 0.7906884379760184,\n",
       " 0.8366959465874566,\n",
       " 0.5337753916780154,\n",
       " 0.8058989164203104,\n",
       " 0.6716750606442943,\n",
       " 0.809103346743969,\n",
       " 0.8884953575904924,\n",
       " 0.5002396085045555,\n",
       " 0.7149070091319807,\n",
       " 0.4350913967448051,\n",
       " 0.8789101107554003,\n",
       " 0.46671295701855364,\n",
       " 0.597493737183436,\n",
       " 0.7157944269252545,\n",
       " 0.30779319616279216,\n",
       " 0.31967834889738245,\n",
       " 0.3729706623036452,\n",
       " 0.32380542139513324,\n",
       " 0.4057966457924458,\n",
       " 0.4992691194168245,\n",
       " 0.47575669637834184,\n",
       " 0.5767367167906328,\n",
       " 0.2931530572699778,\n",
       " 0.8955279203677418,\n",
       " 0.384741752986053,\n",
       " 0.855808419621352,\n",
       " 0.42032031480110055,\n",
       " 0.9209269355643879,\n",
       " 0.6788103330616999,\n",
       " 0.47227998574574787,\n",
       " 0.4001841742884029,\n",
       " 0.5892985716462136,\n",
       " 0.9187423077195582,\n",
       " 0.6371719511169376,\n",
       " 0.8178846693099147,\n",
       " 0.7647785050399376,\n",
       " 0.8042988872889316,\n",
       " 0.757292119661967,\n",
       " 0.888607545573302,\n",
       " 0.8474724759658178,\n",
       " 0.9578534979711879,\n",
       " 0.4961389179783638,\n",
       " 0.7548413442240821,\n",
       " 0.4101055415892842,\n",
       " 0.6277019953486895,\n",
       " 0.5365940389157545,\n",
       " 0.3704177319551959,\n",
       " 0.5888153023792035,\n",
       " 0.5242236738554156,\n",
       " 0.9496873981422849,\n",
       " 0.8443300134605831,\n",
       " 0.6415797694463923,\n",
       " 0.9593277966434306,\n",
       " 0.6091647285403627,\n",
       " 0.8655457016795572,\n",
       " 0.7507855719689167,\n",
       " 0.7546051210225231,\n",
       " 0.7758263726728131,\n",
       " 0.3751059223907163,\n",
       " 0.3423158787416689,\n",
       " 0.8653505594441385,\n",
       " 0.7227092297992321,\n",
       " 0.43182052370875773,\n",
       " 0.8085773186852234,\n",
       " 0.6896097604373489,\n",
       " 0.6933330390188429,\n",
       " 0.6969068336817953,\n",
       " 0.3701540231855229,\n",
       " 0.9184647465294058,\n",
       " 0.13544006724700783,\n",
       " 0.9093469715479648,\n",
       " 0.5321388243424772,\n",
       " 0.4576946802964114,\n",
       " 0.727243339142414,\n",
       " 0.4438348039232119,\n",
       " 0.9637766320597042,\n",
       " 0.9689246118068695,\n",
       " 0.8169049958990078,\n",
       " 0.919564668668641,\n",
       " 0.20554686580794027,\n",
       " 0.6648080778242361,\n",
       " 0.5578568451633357,\n",
       " 0.6442159159918024,\n",
       " 0.4795824647551835,\n",
       " 0.743976118980032,\n",
       " 0.464428401655621,\n",
       " 0.32222290223445554,\n",
       " 0.7983155416719842,\n",
       " 0.7495141196732569,\n",
       " 0.49164667039206533,\n",
       " 0.8083746325789076,\n",
       " 0.7292885642762136,\n",
       " 0.6584220804048306,\n",
       " 0.3972718074917793,\n",
       " 0.1028292984447696,\n",
       " 0.05326840234073726,\n",
       " 0.7955834566944777,\n",
       " 0.7289211059760565,\n",
       " 0.7083548713212061,\n",
       " 0.8309462668919804,\n",
       " 0.8010857508339063,\n",
       " 0.9074121671493607,\n",
       " 0.7328024386757552,\n",
       " 0.7808678450608494,\n",
       " 0.6055613092400811,\n",
       " 0.8571539946276732,\n",
       " 0.4696546486832879,\n",
       " 0.28327577216456634,\n",
       " 0.9422403510170754,\n",
       " 0.7699714381586422,\n",
       " 0.8715417295092285,\n",
       " 0.9366658614440397,\n",
       " 0.9321278364670397,\n",
       " 0.6327915799437147,\n",
       " 0.9014507049863989,\n",
       " 0.6959251076585115,\n",
       " 0.595489101202199,\n",
       " 0.4356335046766984,\n",
       " 0.8150362147225274,\n",
       " 0.9268806893717159,\n",
       " 0.3257427946636171,\n",
       " 0.8197516848944655,\n",
       " 0.7197908465609406,\n",
       " 0.10365140590478074,\n",
       " 0.8157847303633738,\n",
       " 0.5447167629545385,\n",
       " 0.558878982217625,\n",
       " 0.9167099753413538,\n",
       " 0.7278378998089319,\n",
       " 0.7693455044067268,\n",
       " 0.3853273902427067,\n",
       " 0.734401288688785,\n",
       " 0.7713853182816746,\n",
       " 0.8355310353064778,\n",
       " 0.9070854442890244,\n",
       " 0.8348229946813198,\n",
       " 0.6997885297043155,\n",
       " 0.8324414808340747,\n",
       " 0.6628010556252316,\n",
       " 0.3103325757339145,\n",
       " 0.7936818902841722,\n",
       " 0.8793618994529802,\n",
       " 0.4081586702152936,\n",
       " 0.7434379329284032,\n",
       " 0.20183814443422085,\n",
       " 0.6864081867415496,\n",
       " 0.9084792191031004,\n",
       " 0.6237328369659607,\n",
       " 0.7192723036414446,\n",
       " 0.5657037662285747,\n",
       " 0.5498628740960901,\n",
       " 0.7007513774765862,\n",
       " 0.6705296686502418,\n",
       " 0.8905225707425011,\n",
       " 0.8143293269354888,\n",
       " 0.46385399267227967,\n",
       " 0.8660266120024401,\n",
       " 0.9071223375171122,\n",
       " 0.9121611044864462,\n",
       " 0.7801007724169529,\n",
       " 0.8090312107945934,\n",
       " 0.9398656336948125,\n",
       " 0.9264702871291324,\n",
       " 0.7470003828857884,\n",
       " 0.849475904305776,\n",
       " 0.8143595014858728,\n",
       " 0.9029330836101012,\n",
       " 0.9118548187041523,\n",
       " 0.8930209364252861,\n",
       " 0.8769379066698479,\n",
       " 0.8485732344966946,\n",
       " 0.4375465753102543,\n",
       " 0.8197648066763926,\n",
       " 0.9108869037844918,\n",
       " 0.6449862662288878,\n",
       " 0.9186910711755656,\n",
       " 0.658431378849829,\n",
       " 0.7887469777254144,\n",
       " 0.7775346564223068,\n",
       " 0.35678259522625894,\n",
       " 0.42267902166855453,\n",
       " 0.83986246649063,\n",
       " 0.7070821835838184,\n",
       " 0.894167574126311,\n",
       " 0.5859400462622595,\n",
       " 0.8905502291941884,\n",
       " 0.33554185865202335,\n",
       " 0.37886788098499025,\n",
       " 0.3440730490016215,\n",
       " 0.729179378621506,\n",
       " 0.6898022575510873,\n",
       " 0.2878511454345602,\n",
       " 0.7443106286453478,\n",
       " 0.2915862341571336,\n",
       " 0.8045872831585431,\n",
       " 0.35082990915486306,\n",
       " 0.7108257103146929,\n",
       " 0.6259022390902644,\n",
       " 0.4495347396291868,\n",
       " 0.741992192828294,\n",
       " 0.6463522977901227,\n",
       " 0.5807271829759232,\n",
       " 0.6020084450642268,\n",
       " 0.7261942840886838,\n",
       " 0.5173846330004509,\n",
       " 0.36835122415513705,\n",
       " 0.7860754937535585,\n",
       " 0.7891653306857505,\n",
       " 0.6723196595004111,\n",
       " 0.45558334614592366,\n",
       " 0.9599750445647673,\n",
       " 0.5629204741030028,\n",
       " 0.9503787873670309,\n",
       " 0.859867684768908,\n",
       " 0.8321789311640191,\n",
       " 0.9202518269269153,\n",
       " 0.8649194291745773,\n",
       " 0.7552853695069901,\n",
       " 0.7954299054061523,\n",
       " 0.8355315815017681,\n",
       " 0.9473099441540361,\n",
       " 0.7726764951992516,\n",
       " 0.8898822652872163,\n",
       " 0.6314235114388995,\n",
       " 0.3838155296715823,\n",
       " 0.5239775431276572,\n",
       " 0.8985324497776802,\n",
       " 0.8374498656903855,\n",
       " 0.7955200344926179,\n",
       " 0.7532314709039649,\n",
       " 0.8690880518971067,\n",
       " 0.8622094330161509,\n",
       " 0.8181074752952113,\n",
       " 0.8112900867305621,\n",
       " 0.8614388316568702,\n",
       " 0.38136518137021497,\n",
       " 0.48709615276317403,\n",
       " 0.5635666191878945,\n",
       " 0.42303621169894634,\n",
       " 0.6369227657715479,\n",
       " 0.6419386001247348,\n",
       " 0.24425826901287745,\n",
       " 0.7132078319185913,\n",
       " 0.8307586353836638,\n",
       " 0.7492448065317039,\n",
       " 0.6531701219503325,\n",
       " 0.6608176450386192,\n",
       " 0.49004978992120185,\n",
       " 0.5269612772597208,\n",
       " 0.5462466916050573,\n",
       " 0.9082844938593682,\n",
       " 0.6020829531731028,\n",
       " 0.7255329367488321,\n",
       " 0.7802642231035715,\n",
       " 0.7156682534952357,\n",
       " 0.6562530000251954,\n",
       " 0.9009111145530084,\n",
       " 0.9601631183515895,\n",
       " 0.5610611627047712,\n",
       " 0.9158598087050698,\n",
       " 0.41309251372862343,\n",
       " 0.4526897855930858,\n",
       " 0.4782074906007208,\n",
       " 0.43593470603227613,\n",
       " 0.6557854638256208,\n",
       " 0.30235017770619105,\n",
       " 0.32731815435067574,\n",
       " 0.38970013107314255,\n",
       " 0.6547851233169286,\n",
       " 0.5663105560974642,\n",
       " 0.26096761526182444,\n",
       " 0.6606358373706991,\n",
       " 0.8017281283934912,\n",
       " 0.47105137823206006,\n",
       " 0.5252332033232006,\n",
       " 0.3625335262128801,\n",
       " 0.47551747136043776,\n",
       " 0.45711930774980125,\n",
       " 0.05902095386250453,\n",
       " 0.787687869866689,\n",
       " 0.3052899274485882,\n",
       " 0.7819288205618811,\n",
       " 0.38294206514503015,\n",
       " 0.8429615298003862,\n",
       " 0.7520125616078425,\n",
       " 0.7890625485868166,\n",
       " 0.5200500780584836,\n",
       " 0.23488469391007616,\n",
       " 0.8324995105314736,\n",
       " 0.8896085299927778,\n",
       " 0.858211874118959,\n",
       " 0.8270981555033212,\n",
       " 0.4496875521811572,\n",
       " 0.9211600911737692,\n",
       " 0.8220193033567582,\n",
       " 0.78929760194186,\n",
       " 0.5177481925999275,\n",
       " 0.7844253030389247,\n",
       " 0.8484801931212647,\n",
       " 0.8001171421824079,\n",
       " 0.41480516333772677,\n",
       " 0.8618271171745627,\n",
       " 0.854580536454615,\n",
       " 0.8593842302910005,\n",
       " 0.8247756405310197,\n",
       " 0.9546056005087766,\n",
       " 0.8031388124131194,\n",
       " 0.9424343022433195,\n",
       " 0.8511320826080111,\n",
       " 0.827624569818227,\n",
       " 0.7866125786244267,\n",
       " 0.9148824332639425,\n",
       " 0.7902558701809006,\n",
       " 0.6598129751104297,\n",
       " 0.7823420165163097,\n",
       " 0.3218209906659945,\n",
       " 0.3871970508134726,\n",
       " 0.730774763045889,\n",
       " 0.5063675195129231,\n",
       " 0.7091709606575244,\n",
       " 0.8933122584615091,\n",
       " 0.8754273354706138,\n",
       " 0.31748977554867963,\n",
       " 0.7404434863064024,\n",
       " 0.5334188088624164,\n",
       " 0.564526966771092,\n",
       " 0.7394960466358397,\n",
       " 0.326452602551441,\n",
       " 0.7236637247632248,\n",
       " 0.5851711707283752,\n",
       " 0.6229759257550191,\n",
       " 0.8900834606452421,\n",
       " 0.9314379628258522,\n",
       " 0.8714868636143328,\n",
       " 0.6038995921611786,\n",
       " 0.8013794960999729,\n",
       " 0.7845880152902218,\n",
       " 0.40001558647914365,\n",
       " 0.5621602526367313,\n",
       " 0.8281177141148635,\n",
       " 0.43678953333033455,\n",
       " 0.7717404727983956,\n",
       " 0.3562752952328836,\n",
       " 0.4038624115815066,\n",
       " 0.34944907285348337,\n",
       " 0.5136231831077374,\n",
       " 0.9200076937675477,\n",
       " 0.9050677021645537,\n",
       " 0.8558018631104267,\n",
       " 0.9258658757113447,\n",
       " 0.9111968085019275,\n",
       " 0.8136884852792278,\n",
       " 0.5506491722031073,\n",
       " 0.7393131019792172,\n",
       " 0.3964356944868059,\n",
       " 0.9114408005066592,\n",
       " 0.9214841476895592,\n",
       " 0.7854384439160125,\n",
       " 0.6253934148285124,\n",
       " 0.9224556067977289,\n",
       " 0.8777651586014815,\n",
       " 0.936825275963003,\n",
       " 0.7993254716348166,\n",
       " 0.7260014947616693,\n",
       " 0.6477493209369255,\n",
       " 0.79232004673192,\n",
       " 0.7647254532936848,\n",
       " 0.9169420971111818,\n",
       " 0.8550469211255661,\n",
       " 0.864100040058897,\n",
       " 0.792993063336671,\n",
       " 0.8191558237328673,\n",
       " 0.2435109046297242,\n",
       " 0.7884945525665477,\n",
       " 0.847916167131578,\n",
       " 0.17336645784853685,\n",
       " 0.8055338214443187,\n",
       " 0.9100347671845946,\n",
       " 0.8663167132271661,\n",
       " 0.8811299311392236,\n",
       " 0.7913085504914775,\n",
       " 0.39716114316022755,\n",
       " 0.3771075626515379,\n",
       " 0.9341997802257538,\n",
       " 0.6458960713461193,\n",
       " 0.7908398753765857,\n",
       " 0.6274451381931401,\n",
       " 0.9022722799368579,\n",
       " 0.8700531182867108,\n",
       " 0.3597250626845793,\n",
       " 0.7462744343461413,\n",
       " 0.6671692872589284,\n",
       " 0.5943291255620995,\n",
       " 0.13912622679514114,\n",
       " 0.3686602446617502,\n",
       " 0.5297194616963166,\n",
       " 0.8720610153494459,\n",
       " 0.4793304104395587,\n",
       " 0.7536379430029128,\n",
       " 0.8493370020630384,\n",
       " 0.9153751092727738,\n",
       " 0.48565360783326506,\n",
       " 0.935922782258554,\n",
       " 0.9070489707017185,\n",
       " 0.7644820706109807,\n",
       " 0.4658168690975266,\n",
       " 0.7927372709669248,\n",
       " 0.9703926411542025,\n",
       " 0.7349012274934787,\n",
       " 0.4293954891387862,\n",
       " 0.9176871607098916,\n",
       " 0.5404468921549392,\n",
       " 0.9656298734925011,\n",
       " 0.5396649338831805,\n",
       " 0.6040047725643775,\n",
       " 0.44933132660208325,\n",
       " 0.527478818098704,\n",
       " 0.6002599843975269,\n",
       " 0.6500359291380102,\n",
       " 0.8344554177137337,\n",
       " 0.8135585780095572,\n",
       " 0.9083320323565993,\n",
       " 0.803929597350082,\n",
       " 0.8585041982055914,\n",
       " 0.8743599624344797,\n",
       " 0.6980289100697546,\n",
       " 0.7136686215497027,\n",
       " 0.6902662993079485,\n",
       " 0.5050955508995538,\n",
       " 0.8792023096120719,\n",
       " 0.7476489152872201,\n",
       " 0.9401367968983121,\n",
       " 0.7270474076873124,\n",
       " 0.9757437099109997,\n",
       " 0.5213362146206577,\n",
       " 0.6655629533709901,\n",
       " 0.5157555628906597,\n",
       " 0.8286846267454552,\n",
       " 0.6608413184230978,\n",
       " 0.8418127854966154,\n",
       " 0.33477355467550685,\n",
       " 0.4960052791568968,\n",
       " 0.3419775910901301,\n",
       " 0.2896699372415591,\n",
       " 0.3562328591190203,\n",
       " 0.48902776903576317,\n",
       " 0.7169425722324487,\n",
       " 0.8259694466085146,\n",
       " 0.5589403622379207,\n",
       " 0.9461627973751588,\n",
       " 0.5835951017309922,\n",
       " 0.589495322424354,\n",
       " 0.33038517508392384,\n",
       " 0.4673619196571485,\n",
       " 0.850788276544725,\n",
       " 0.6934460300688792,\n",
       " 0.4858258688841203,\n",
       " 0.7777762259497787,\n",
       " 0.976058808781884,\n",
       " 0.9675060776146975,\n",
       " 0.9027483421142655,\n",
       " 0.599894448572939,\n",
       " 0.7160207851968631,\n",
       " 0.44587824385274544,\n",
       " 0.8329481082432197,\n",
       " 0.9242391085082835,\n",
       " 0.7562096073771968,\n",
       " 0.6336719548762446,\n",
       " 0.7749986921597009,\n",
       " 0.9167070532386953,\n",
       " 0.8129135453941846,\n",
       " 0.8702816483348307,\n",
       " 0.10495512248440222,\n",
       " 0.033144038322974335,\n",
       " 0.5834954345768149,\n",
       " 0.7578388076538991,\n",
       " 0.893047513835358,\n",
       " 0.7188508684888031,\n",
       " 0.9511283324523405,\n",
       " 0.4256792176552493,\n",
       " 0.271090364584116,\n",
       " 0.44630487637989447,\n",
       " 0.7574264776827109,\n",
       " 0.6591650060180462,\n",
       " 0.7372200908684972,\n",
       " 0.9177856813777576,\n",
       " 0.2656013273003728,\n",
       " 0.7537123854413177,\n",
       " 0.6406325691276127,\n",
       " 0.6621558812531558,\n",
       " 0.8076169346920168,\n",
       " 0.30865597390767296,\n",
       " 0.842190174863796,\n",
       " 0.778767822336669,\n",
       " 0.273904467298828,\n",
       " 0.7715323046602384,\n",
       " 0.8602748858507233,\n",
       " 0.726620542130085,\n",
       " 0.6108839739753742,\n",
       " 0.7364188602777443,\n",
       " 0.6916003118259738,\n",
       " 0.863901778513735,\n",
       " 0.8597175928378346,\n",
       " 0.8432592916067201,\n",
       " 0.501890183714303,\n",
       " 0.21940075374688162,\n",
       " 0.3287073458234469,\n",
       " 0.9025375494451234,\n",
       " 0.9228275938467546,\n",
       " 0.6783093330384504,\n",
       " 0.7730840746200446,\n",
       " 0.736280726242547,\n",
       " 0.7294487932414719,\n",
       " 0.5917655890337145,\n",
       " 0.916839751842046,\n",
       " 0.4268235251608521,\n",
       " 0.21054683601615404,\n",
       " 0.42905028406718765,\n",
       " 0.6716274393327308,\n",
       " 0.7520833444113684,\n",
       " 0.5900950534777207,\n",
       " 0.520497067558645,\n",
       " 0.6018791823525622,\n",
       " 0.44106063686235986,\n",
       " 0.7724467981343317,\n",
       " 0.5778893949106486,\n",
       " 0.44469946661982873,\n",
       " 0.5417783844651598,\n",
       " 0.9042863611320052,\n",
       " 0.26785599322180553,\n",
       " 0.748206107243143,\n",
       " 0.4480677302437599,\n",
       " 0.31609608740216555,\n",
       " 0.8160831307521974,\n",
       " 0.9181353016032112,\n",
       " 0.12824774717893261,\n",
       " 0.8549439351366024,\n",
       " 0.629485568855748,\n",
       " 0.5954501293674864,\n",
       " 0.8088600411559597,\n",
       " 0.8499969636551057,\n",
       " 0.9267046536761101,\n",
       " 0.5099593178641917,\n",
       " 0.8970545065222364,\n",
       " 0.8929224230725356,\n",
       " 0.8085074072233354,\n",
       " 0.5513218885419344,\n",
       " 0.48961812626532836,\n",
       " 0.6030695152854679,\n",
       " 0.649836534984184,\n",
       " 0.8866916691414033,\n",
       " 0.8468568741974204,\n",
       " 0.7661621690398515,\n",
       " 0.7346226676545962,\n",
       " 0.3825571549397827,\n",
       " 0.4901130429873563,\n",
       " 0.43762050022681553,\n",
       " 0.5322285868302741,\n",
       " 0.651788144521039,\n",
       " 0.8478331111597293,\n",
       " 0.6329213119516469,\n",
       " 0.6350479079015328,\n",
       " 0.7124549667040507,\n",
       " 0.8463533117915645,\n",
       " 0.539778175965102,\n",
       " 0.4338415113362399,\n",
       " 0.618502124105439,\n",
       " 0.44856004728512333,\n",
       " 0.09647370129823685,\n",
       " 0.4791076360025791,\n",
       " 0.7272516473074152,\n",
       " 0.5179096732928296,\n",
       " 0.8310377510509106,\n",
       " 0.8222080749393713,\n",
       " 0.4570708885487884,\n",
       " 0.9518342633139003,\n",
       " 0.8716532677412033,\n",
       " 0.4343061544527911,\n",
       " 0.6532043250221194,\n",
       " 0.5621861278709739,\n",
       " 0.7148254750503433,\n",
       " 0.7891695371480903,\n",
       " 0.9193875507874922,\n",
       " 0.35620876559705444,\n",
       " 0.8274226472835348,\n",
       " 0.5733025091766106,\n",
       " 0.5985998908979724,\n",
       " 0.8921740058696631,\n",
       " 0.8444695000997697,\n",
       " 0.5334795136192833,\n",
       " 0.4432387126666127,\n",
       " 0.547464120674013,\n",
       " 0.8537881079045209,\n",
       " 0.7934244191706783,\n",
       " 0.41874417483505577,\n",
       " 0.5908398338791095,\n",
       " 0.9263035312144443,\n",
       " 0.898107785498253,\n",
       " 0.6112812922458456,\n",
       " 0.9010241892903742,\n",
       " 0.8553693280978636,\n",
       " 0.8964370949400795,\n",
       " 0.185333118836085,\n",
       " 0.534944370840535,\n",
       " 0.6686003845448446,\n",
       " 0.5835403515533968,\n",
       " 0.6973734078985272,\n",
       " 0.6942390414199444,\n",
       " 0.7148904488243237,\n",
       " 0.8220461128336011,\n",
       " 0.7831209435306414,\n",
       " 0.7586249789505294,\n",
       " 0.9199771710417488,\n",
       " 0.6223448736198021,\n",
       " 0.5099099472165107,\n",
       " 0.8874459596896412,\n",
       " 0.962796746600758,\n",
       " 0.3573729398124146,\n",
       " 0.387807725896739,\n",
       " 0.6822845463499878,\n",
       " 0.8120342117668402,\n",
       " 0.6132718926126307,\n",
       " 0.5349965954669799,\n",
       " 0.6653914597299364,\n",
       " 0.2882020725520572,\n",
       " 0.7743458351703605,\n",
       " 0.7350094733515171,\n",
       " 0.6838421782760908,\n",
       " 0.7569975134700235,\n",
       " 0.4411341812273469,\n",
       " 0.7142410216909466,\n",
       " 0.6375649080131993,\n",
       " 0.6912523763348357,\n",
       " 0.36259249076247213,\n",
       " 0.6160562602257488,\n",
       " 0.43535042019805525,\n",
       " 0.6808919955985715,\n",
       " 0.7052298425725012,\n",
       " 0.6327057378159628,\n",
       " 0.7820889674051843,\n",
       " 0.45044085926780797,\n",
       " 0.6856062317285875,\n",
       " 0.5227355614455059,\n",
       " 0.7840680001058964,\n",
       " 0.7814689537792494,\n",
       " 0.8521055931093717,\n",
       " 0.8143975616404504,\n",
       " 0.8166510367634321,\n",
       " 0.5592338114374815,\n",
       " 0.8636048674583435,\n",
       " 0.46555604450028354,\n",
       " 0.8021451380818782,\n",
       " 0.5838297293041692,\n",
       " 0.9007595030948369,\n",
       " 0.751490477088726,\n",
       " 0.9144833569875871,\n",
       " 0.5356153687142362,\n",
       " 0.8782710076582552,\n",
       " 0.3201451778110832,\n",
       " 0.8058657756357481,\n",
       " 0.6573308645775824,\n",
       " 0.7995697505245305,\n",
       " 0.9229980880262876,\n",
       " 0.9121274190719681,\n",
       " 0.8425156342561799,\n",
       " 0.8987830066921735,\n",
       " 0.4438310125861505,\n",
       " 0.8963282037865032,\n",
       " 0.7028695393993397,\n",
       " 0.867933125929399,\n",
       " 0.7304127310562616,\n",
       " 0.578213436404864,\n",
       " 0.3260228379733032,\n",
       " 0.8932427215154725,\n",
       " 0.9148329760390099,\n",
       " 0.5005388915839821,\n",
       " 0.8016761651544859,\n",
       " 0.8241777198483246,\n",
       " 0.8153525488846229,\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict_temp.append((Y_predict_rf[i] + Y_predict_gbc[i]) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict\n",
    "submit_score.to_csv('predict_result_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict_temp\n",
    "submit_score.to_csv('predict_result_ncnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(train_file,values = 'price', index=['lvl1','lvl2','lvl3'],columns=['type'],aggfunc=[min, max, np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
