{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chd415/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "sns.set(color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                   \n",
    "import nltk                                         \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer  \n",
    "LA = np.linalg\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          \n",
    "from gensim.models.word2vec import Word2Vec                                  \n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data text\n",
    "def load_data(filename):\n",
    "    load_file = pd.read_csv(filename,delimiter=',', header=0,\n",
    "                        dtype={'name':str, 'lvl1':str, 'lvl2':str, 'lvl3':str, 'descrption':str, 'type':str})\n",
    "    load_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "    load_file.duplicated(subset=None, keep='first')\n",
    "    load_file.set_index('id', inplace = True)\n",
    "    load_file.head()\n",
    "    return load_file\n",
    "#print(len(train_file))\n",
    "def load_label(filename):\n",
    "    load_label = pd.read_csv(filename,delimiter=',', header=0)\n",
    "    load_label.columns = ['id', 'score']\n",
    "    load_label.duplicated(subset=None, keep='first')\n",
    "    load_label.set_index('id', inplace = True)\n",
    "    return load_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mathod(column):\n",
    "    values = []\n",
    "    indexs = []\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for count in range(len(train_file)):\n",
    "        value = train_file.get_value(count+1,column)\n",
    "        if value in values and value != np.nan:\n",
    "            continue\n",
    "        values.append(value)\n",
    "        indexs.append(len(values))\n",
    "    for j in range(len(indexs)):\n",
    "        mapping[values[j]] = indexs[j]\n",
    "    mapping[np.nan] = 0.0\n",
    "    return mapping\n",
    "#train_file['lvl3'] = train_file['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "#mapping_lvl3 = map_mathod('lvl3')\n",
    "#print(mapping_lvl3)\n",
    "\n",
    "#from sktlearn import preprocessing\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=10000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(max_features=10000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=1,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "    \n",
    "    return avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "temp = load_data('train_data.csv')\n",
    "temp['descrption'] = temp['descrption'].str.lower()\n",
    "description_X = temp.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "def test_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=5000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english',max_features=5000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=5,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "\n",
    "    tf_w_data = []\n",
    "    tf_data = tf_data.toarray()\n",
    "    i = 0\n",
    "    for row in splitted:\n",
    "        vec = [0 for i in range(50)]\n",
    "    \n",
    "        temp_tfidf = []\n",
    "        for val in tf_data[i]:\n",
    "            if val != 0:\n",
    "                temp_tfidf.append(val)\n",
    "    \n",
    "        count = 0\n",
    "        tf_idf_sum = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                count += 1\n",
    "                tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "                vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        if (tf_idf_sum == 0):\n",
    "            tf_w_data.append(vec)\n",
    "        else:\n",
    "            tf_w_data.append(vec/tf_idf_sum)\n",
    "#            vec = float(1/tf_idf_sum) * vec\n",
    "#        tf_w_data.append(vec)\n",
    "        i = i + 1\n",
    "    \n",
    "    return tf_w_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "temp['descrption'] = test_embedding(description_X)\n",
    "temp.descrption.head()\n",
    "#tfidf_n = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    #clean up data for lvl 1&2&3\n",
    "    filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "    filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].str.lower()\n",
    "    filename['name'] = filename['name'].str.lower()\n",
    "    \n",
    "    mapping_lvl1 = map_mathod('lvl1')\n",
    "    mapping_lvl2 = map_mathod('lvl2')\n",
    "    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "    filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "    filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    #normalize price\n",
    "    maxp = filename.price.max()\n",
    "    valuethred = 1000.\n",
    "    filename['price'] = filename['price'].clip(lower=0.,upper=valuethred)\n",
    "    #filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    #hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "\n",
    "    #clean up type \n",
    "    mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "    filename['type'] = filename['type'].map(mapping_type)\n",
    "    \n",
    "    #clean up text\n",
    "    description_X = filename.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['descrption'] = text_embedding(description_X)\n",
    "    \n",
    "    name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['name'] = text_embedding(name_X)\n",
    "\n",
    "    return filename,mapping_lvl1,mapping_lvl2,mapping_lvl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = load_data('train_data.csv')\n",
    "cleaned_train,mapping_lvl1,mapping_lvl2,mapping_lvl3 = clean_data(train_file)\n",
    "train_score = load_label('train_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell\n",
    "cleaned_train['descrption'] = temp['descrption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lvl1</th>\n",
       "      <th>lvl2</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>descrption</th>\n",
       "      <th>price</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.07032404059363115, -0.29758781334385276, -...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.06992685375735164, -0.16184532611320415, -0...</td>\n",
       "      <td>128.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.8263886688011033, -0.3248656073618414, -1.3...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0.016663137124851346, -0.06203396766795777, 0...</td>\n",
       "      <td>14.69</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.4067267007406123, 0.039988674101716075, -2....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[-0.09603992232587188, 0.3617549417540431, 0.4...</td>\n",
       "      <td>14.10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.6705077759483282, 0.06829343867652557, -2.7...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.10783753912065826, -0.06589401812981004, 0....</td>\n",
       "      <td>17.94</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1.0321451897376819, 0.016257136293193874, -1....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0.3669052943587303, -0.943045073085361, -0.82...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[-0.08606704908351485, -0.44580006927404053, -...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[-0.3062274492560671, -0.22138636959537578, -0...</td>\n",
       "      <td>388.99</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.6027595897150391, -0.12192046743653276, -1....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.1515675337545367, 0.02379527411216663, 0.11...</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.09512590244412422, -0.850517663690779, -1.7...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[-0.4776106401631195, -0.12558511003024048, -0...</td>\n",
       "      <td>21.74</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.14016459241393023, -0.13052392018726094, -1...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[0.16502526154131084, -0.2545379170147266, -0....</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[-0.1933838913350233, -0.4144515814924879, -0....</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[-1.1603592104381986, 0.1397100786368052, -1.0...</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  lvl1  lvl2  lvl3  \\\n",
       "id                                                                        \n",
       "1   [-0.07032404059363115, -0.29758781334385276, -...   1.0   1.0   1.0   \n",
       "2   [0.8263886688011033, -0.3248656073618414, -1.3...   2.0   2.0   2.0   \n",
       "3   [1.4067267007406123, 0.039988674101716075, -2....   3.0   3.0   3.0   \n",
       "4   [1.6705077759483282, 0.06829343867652557, -2.7...   3.0   3.0   3.0   \n",
       "5   [1.0321451897376819, 0.016257136293193874, -1....   3.0   3.0   3.0   \n",
       "6   [-0.08606704908351485, -0.44580006927404053, -...   4.0   4.0   4.0   \n",
       "7   [0.6027595897150391, -0.12192046743653276, -1....   2.0   5.0   5.0   \n",
       "8   [0.09512590244412422, -0.850517663690779, -1.7...   5.0   6.0   6.0   \n",
       "9   [0.14016459241393023, -0.13052392018726094, -1...   6.0   7.0   7.0   \n",
       "10  [-0.1933838913350233, -0.4144515814924879, -0....   6.0   7.0   8.0   \n",
       "\n",
       "                                           descrption    price  type  \n",
       "id                                                                    \n",
       "1   [0.06992685375735164, -0.16184532611320415, -0...   128.00   1.0  \n",
       "2   [0.016663137124851346, -0.06203396766795777, 0...    14.69   1.0  \n",
       "3   [-0.09603992232587188, 0.3617549417540431, 0.4...    14.10   1.0  \n",
       "4   [0.10783753912065826, -0.06589401812981004, 0....    17.94   1.0  \n",
       "5   [0.3669052943587303, -0.943045073085361, -0.82...     6.80   1.0  \n",
       "6   [-0.3062274492560671, -0.22138636959537578, -0...   388.99   1.0  \n",
       "7   [0.1515675337545367, 0.02379527411216663, 0.11...  1000.00   2.0  \n",
       "8   [-0.4776106401631195, -0.12558511003024048, -0...    21.74   1.0  \n",
       "9   [0.16502526154131084, -0.2545379170147266, -0....    25.00   2.0  \n",
       "10  [-1.1603592104381986, 0.1397100786368052, -1.0...     9.48   1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train.head(10)\n",
    "#print(mapping_lvl1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18141, 263)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def rearrange(cleaned_data):\n",
    "    la = cleaned_data.lvl1.as_matrix(columns=None).tolist()\n",
    "    lb = cleaned_data.lvl2.as_matrix(columns=None).tolist()\n",
    "    lc = cleaned_data.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = la\n",
    "    X = np.column_stack((X,lb))\n",
    "    X = np.column_stack((X,lc))\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    \n",
    "    ld = cleaned_data.price.as_matrix(columns=None).tolist()\n",
    "    le = cleaned_data.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = np.column_stack((X,ld))\n",
    "    X = np.column_stack((X,le))\n",
    "\n",
    "   \n",
    "    lf = cleaned_data.name.as_matrix(columns=None).tolist()\n",
    "    lg = cleaned_data.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "# only use U \\cdot s\n",
    "    \n",
    "#    s[8:] = 0.\n",
    "#    new_lg = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "    s_new = s[:5]\n",
    "    U_new = U[:, :5]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    lg_min = np.min(new_lg)\n",
    "    lg_out = new_lg  - (lg_min-2)*np.ones_like(new_lg.size)\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "#    sf[5:] = 0.\n",
    "#    new_lf = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "    sf_new = sf[:5]\n",
    "    Uf_new = Uf[:, :5]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    lf_min = np.min(new_lf)\n",
    "    lf_out = new_lf  - (lf_min-2)*np.ones_like(new_lf.size)\n",
    "\n",
    "    \n",
    "    X = np.column_stack((X,lf_out))\n",
    "    X = np.column_stack((X,lg_out))\n",
    "    X = X.tolist()\n",
    "   \n",
    "    return X,lf_min,lg_min\n",
    "\n",
    "    \n",
    "    #print(len(X))\n",
    "X,lf_min,lg_min = rearrange(cleaned_train)\n",
    "print(np.shape(np.array(X)))\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "maxlen = 263\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, dtype='float32')\n",
    "#X_train = np.any(np.isnan(X_train))\n",
    "#X_train = np.all(np.isfinite(X_train))\n",
    "print(X_train[1400].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Classifier\n",
    "def linear_regression_classifier(train_x, train_y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    param_grid = {'alpha': [math.pow(10,-i) for i in range(11)]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = MultinomialNB(alpha = best_parameters['alpha'])  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    param_grid = {'n_neighbors': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = best_parameters['n_neighbors'], algorithm='kd_tree')\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(100,300,10))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    #for para, val in best_parameters.items():\n",
    "        #print para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "def feature_select(x,y):\n",
    "    clf = ExtraTreesClassifier()\n",
    "    clf = clf.fit(x, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for my own record\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['KNN','LR','RF','DT']    \n",
    "    classifiers = {\n",
    "                   'KNN':knn_classifier,\n",
    "                    'LR':logistic_regression_classifier,\n",
    "                    'RF':random_forest_classifier,\n",
    "                    'DT':decision_tree_classifier,                 \n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_validation = select_model.transform(X_validation)\n",
    "\n",
    "    result = []\n",
    "        \n",
    "    for classifier in test_classifiers:    \n",
    "        print('******************* %s ********************' % classifier)    \n",
    "        start_time = time.time()    \n",
    "        model = classifiers[classifier](X_train, y_train)   \n",
    "        print('training took %fs!' % (time.time() - start_time))    \n",
    "        predict = model.predict(X_validation)\n",
    "\n",
    "        precision = metrics.precision_score(y_validation, predict)    \n",
    "        recall = metrics.recall_score(y_validation, predict)    \n",
    "        print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))    \n",
    "        accuracy = metrics.accuracy_score(y_validation, predict)    \n",
    "        print('accuracy: %.2f%%' % (100 * accuracy))\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       , ...,  6.121397 ,  8.0434885,\n",
       "         8.24141  ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  8.813916 ,  7.5686784,\n",
       "         9.206636 ],\n",
       "       [ 0.       ,  0.       ,  1.       , ...,  8.532886 ,  8.417407 ,\n",
       "         7.542454 ],\n",
       "       ...,\n",
       "       [ 1.       ,  0.       ,  0.       , ...,  8.556288 ,  7.9783664,\n",
       "         9.911194 ],\n",
       "       [ 0.       ,  0.       ,  1.       , ...,  7.0517106,  9.395992 ,\n",
       "         8.676114 ],\n",
       "       [ 0.       ,  0.       ,  1.       , ..., 10.234994 , 12.946437 ,\n",
       "         8.970349 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "#X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "14512 train sequences\n",
      "3629 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (14512, 263)\n",
      "x_test shape: (3629, 263)\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#test cnn model\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Embedding\n",
    "max_features = 15000\n",
    "maxlen = 263\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 5\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "#X_train, X_validation, y_train, y_validation\n",
    "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "#X_train = np.asarray(np.abs(X))\n",
    "X_train = np.asarray(np.abs(X_train))\n",
    "X_validation = np.asarray(np.abs(X_validation))\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_validation), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, padding='post')\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_validation.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(max_features, embedding_size,input_length=maxlen))\n",
    "#model.add(Dense(32, activation='relu', input_dim=100))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "cnnmodel.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=pool_size))\n",
    "cnnmodel.add(LSTM(lstm_output_size))\n",
    "cnnmodel.add(Dense(1))\n",
    "cnnmodel.add(Activation('sigmoid'))\n",
    "\n",
    "cnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = load_data('test_data.csv')\n",
    "#cleaned_test = clean_data(test_file)\n",
    "X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_file\n",
    "#clean up data for lvl 1&2&3\n",
    "filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "filename['descrption'] = filename['descrption'].str.lower()\n",
    "filename['name'] = filename['name'].str.lower()\n",
    "    \n",
    "#    mapping_lvl1 = map_mathod('lvl1')\n",
    "#    mapping_lvl2 = map_mathod('lvl2')\n",
    "#    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    #normalize price\n",
    "maxp = filename.price.max()\n",
    "valuethred = 1000.\n",
    "filename['price'] = filename['price'].clip(lower=0.,upper=valuethred)\n",
    "#filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    #hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "\n",
    "    #clean up type \n",
    "mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "filename['type'] = filename['type'].map(mapping_type)\n",
    "    \n",
    "    #clean up text\n",
    "description_X = filename.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "filename['descrption'] = text_embedding(description_X)\n",
    "    \n",
    "name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "filename['name'] = text_embedding(name_X)\n",
    "\n",
    "#    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = filename\n",
    "lat = cleaned_test.lvl1.as_matrix(columns=None).tolist()\n",
    "lbt = cleaned_test.lvl2.as_matrix(columns=None).tolist()\n",
    "lct = cleaned_test.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "Xt = lat\n",
    "Xt = np.column_stack((Xt,lbt))\n",
    "Xt = np.column_stack((Xt,lct))\n",
    "\n",
    "enct = preprocessing.OneHotEncoder()\n",
    "enct.fit(Xt)\n",
    "Xt = enct.transform(Xt).toarray()\n",
    "\n",
    "ldt = cleaned_test.price.as_matrix(columns=None).tolist()\n",
    "let = cleaned_test.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "Xt = np.column_stack((Xt,ldt))\n",
    "Xt = np.column_stack((Xt,let))\n",
    "\n",
    "lft = cleaned_test.name.as_matrix(columns=None).tolist()\n",
    "lgt = cleaned_test.descrption.as_matrix(columns=None).tolist()\n",
    "lg_arrayt = np.vstack( lgt )\n",
    "lf_arrayt = np.vstack( lft )\n",
    "\n",
    "Ut, st, Vht = LA.svd(lg_arrayt, full_matrices=False)\n",
    "assert np.allclose(lg_arrayt, np.dot(Ut, np.dot(np.diag(st), Vht)))\n",
    "\n",
    "st_new = st[:5] \n",
    "Ut_new = Ut[:, :5]\n",
    "new_lgt = np.dot(Ut_new, np.diag(st_new))\n",
    "lgt_out = new_lgt  - (lg_min-2)*np.ones_like(new_lgt.size)\n",
    "\n",
    "Uf, sf, Vhf = LA.svd(lf_arrayt, full_matrices=False)\n",
    "assert np.allclose(lf_arrayt, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "sf_new = sf[:5] \n",
    "Uf_new = Uf[:, :5]\n",
    "new_lft = np.dot(Uf_new, np.diag(sf_new))\n",
    "lft_out = new_lft  - (lf_min-2)*np.ones_like(new_lft.size)\n",
    "\n",
    "\n",
    "#X = la\n",
    "#X = np.column_stack((X,lb))\n",
    "#X = np.column_stack((X,lc))\n",
    "#X = np.column_stack((X,ld))\n",
    "#X = np.column_stack((X,le))\n",
    "Xt = np.column_stack((Xt,lft_out))\n",
    "Xt = np.column_stack((Xt,lgt_out))\n",
    "Xt = Xt.tolist()\n",
    "#cleaned_test = Xt\n",
    "#return X,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaned_test = clean_test(test_file)\n",
    "#cleaned_test = filename\n",
    "cleaned_test.head(10)\n",
    "np.min(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing import sequence\n",
    "#maxlen = 150\n",
    "#X_test = rearrange(cleaned_test)\n",
    "X_test = np.asarray(Xt)\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=maxlen, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "training took 0.346484s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   17.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 19.723191s!\n",
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['LR','RF']    \n",
    "    classifiers = {\n",
    "                   'LR':logistic_regression_classifier,\n",
    "                   'RF':random_forest_classifier         \n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "#    X_test = rearrange(Xt)\n",
    "#    X_test = Xt\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_test = select_model.transform(X_test)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = classifiers['LR'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_lr = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['RF'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_rf = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "        \n",
    "#    for classifier in test_classifiers:    \n",
    "#        print('******************* %s ********************' % classifier)    \n",
    "#        start_time = time.time()    \n",
    "#        model = classifiers[classifier](X_train, y_train)   \n",
    "#        print('training took %fs!' % (time.time() - start_time))    \n",
    "#        Y_predict = model.predict_proba(X_test)[:,1]\n",
    "#        print('predict finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18141/18141 [==============================] - 55s 3ms/step - loss: 0.5784 - acc: 0.7033\n",
      "Epoch 2/5\n",
      "18141/18141 [==============================] - 54s 3ms/step - loss: 0.5353 - acc: 0.7355\n",
      "Epoch 3/5\n",
      "18141/18141 [==============================] - 54s 3ms/step - loss: 0.5236 - acc: 0.7430\n",
      "Epoch 4/5\n",
      "18141/18141 [==============================] - 54s 3ms/step - loss: 0.5132 - acc: 0.7480\n",
      "Epoch 5/5\n",
      "18141/18141 [==============================] - 55s 3ms/step - loss: 0.5044 - acc: 0.7502\n"
     ]
    }
   ],
   "source": [
    "#cnn test\n",
    "X_train, y_train = X,Y\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "#X_test = np.asarray(Xt)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "cnnmodel.fit(X_train, y_train, batch_size=128, epochs=5)\n",
    "\n",
    "Y_predict_cnn = cnnmodel.predict(X_test, verbose=0)\n",
    "Y_predict_cnn = np.squeeze(Y_predict_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5098013 , 0.5097091 , 0.50959545, ..., 0.5096364 , 0.5095682 ,\n",
       "       0.5094789 ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70960149, 0.70658239, 0.72379223, ..., 0.70881897, 0.43334579,\n",
       "       0.38980332])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70588235, 0.58823529, 0.47058824, ..., 0.70588235, 0.41176471,\n",
       "       0.23529412])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict.append((Y_predict_lr[i] + Y_predict_rf[i] + Y_predict_cnn[i]) / 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6417617229753266,\n",
       " 0.601508934407967,\n",
       " 0.5679919742959664,\n",
       " 0.7383062791580081,\n",
       " 0.6171568150835252,\n",
       " 0.6697510249074924,\n",
       " 0.6363602936944943,\n",
       " 0.5993328340575735,\n",
       " 0.6324364086201705,\n",
       " 0.6024202375325002,\n",
       " 0.7412061216070346,\n",
       " 0.5039924644737895,\n",
       " 0.6911224905837831,\n",
       " 0.5497889798337453,\n",
       " 0.7285973515173213,\n",
       " 0.4322579983611576,\n",
       " 0.7597256810459195,\n",
       " 0.5693371582091432,\n",
       " 0.6584914961098802,\n",
       " 0.5813136034607614,\n",
       " 0.5060994748528079,\n",
       " 0.6728534855827433,\n",
       " 0.7699855760930033,\n",
       " 0.6794054164001916,\n",
       " 0.7245350858787151,\n",
       " 0.6663921197853707,\n",
       " 0.6474549129019334,\n",
       " 0.7260616324030026,\n",
       " 0.7449465674147385,\n",
       " 0.7080520196835809,\n",
       " 0.6602019190695558,\n",
       " 0.6189760945354279,\n",
       " 0.8018967271664402,\n",
       " 0.6533012547860929,\n",
       " 0.6731650656933224,\n",
       " 0.44273150031352965,\n",
       " 0.6556826477302232,\n",
       " 0.6946488275468113,\n",
       " 0.49863724811481697,\n",
       " 0.7189814859771816,\n",
       " 0.49013529480044477,\n",
       " 0.7680072962577363,\n",
       " 0.5581324252460774,\n",
       " 0.48171598721118625,\n",
       " 0.6879753460645732,\n",
       " 0.6069928273612174,\n",
       " 0.6541088264043867,\n",
       " 0.3565465587099747,\n",
       " 0.6936143318361765,\n",
       " 0.6758636557103888,\n",
       " 0.6806810980105915,\n",
       " 0.6449504690793318,\n",
       " 0.590395847307052,\n",
       " 0.47985966175929357,\n",
       " 0.4024610148749464,\n",
       " 0.730674186475751,\n",
       " 0.4096963137148008,\n",
       " 0.6500514787867336,\n",
       " 0.4747385330444384,\n",
       " 0.7083259100097514,\n",
       " 0.5454362700271012,\n",
       " 0.48258636821394174,\n",
       " 0.4789944008601059,\n",
       " 0.6480998003859126,\n",
       " 0.5455830181252653,\n",
       " 0.7590717329385862,\n",
       " 0.5759143337125662,\n",
       " 0.5255331890912273,\n",
       " 0.685531986027278,\n",
       " 0.6096314410602023,\n",
       " 0.49227939404984383,\n",
       " 0.3646904786201011,\n",
       " 0.5693322407956938,\n",
       " 0.529993844829141,\n",
       " 0.7777066654374215,\n",
       " 0.615744142476517,\n",
       " 0.6450106829670199,\n",
       " 0.6919608458139485,\n",
       " 0.4033779092331038,\n",
       " 0.426694838303672,\n",
       " 0.645920920790089,\n",
       " 0.3295014519818668,\n",
       " 0.44577725937416907,\n",
       " 0.5869636802878002,\n",
       " 0.6370860440628217,\n",
       " 0.6455598626103628,\n",
       " 0.6402384956792716,\n",
       " 0.4233529351339367,\n",
       " 0.646359868122082,\n",
       " 0.6253070133568666,\n",
       " 0.6805768354379608,\n",
       " 0.5782486453421073,\n",
       " 0.5596189525982215,\n",
       " 0.8061031474982224,\n",
       " 0.8101651449581141,\n",
       " 0.7692785838693051,\n",
       " 0.7530335572460364,\n",
       " 0.7645587743751912,\n",
       " 0.7705261421493809,\n",
       " 0.6661190519630761,\n",
       " 0.7408094717089053,\n",
       " 0.6278939375586509,\n",
       " 0.7040238562485097,\n",
       " 0.6519769270349255,\n",
       " 0.7808918176895455,\n",
       " 0.744183806871442,\n",
       " 0.6655743595764854,\n",
       " 0.7890966069789581,\n",
       " 0.6149001733579413,\n",
       " 0.7250058246442709,\n",
       " 0.6758890603484587,\n",
       " 0.6212155496959633,\n",
       " 0.655198442008663,\n",
       " 0.7015990636574928,\n",
       " 0.6521635126827369,\n",
       " 0.5651727992244777,\n",
       " 0.41027333780904973,\n",
       " 0.6892711170511646,\n",
       " 0.5551858453848276,\n",
       " 0.6026204595375305,\n",
       " 0.648626276377005,\n",
       " 0.6119145769942148,\n",
       " 0.6445737372474613,\n",
       " 0.40939621335141574,\n",
       " 0.600714209389911,\n",
       " 0.741960719528541,\n",
       " 0.6906018537448176,\n",
       " 0.5068979295984977,\n",
       " 0.7159167698704337,\n",
       " 0.7471927750308486,\n",
       " 0.6854369626376343,\n",
       " 0.6631635741796272,\n",
       " 0.6729775491004095,\n",
       " 0.6056226763927758,\n",
       " 0.7178943651871613,\n",
       " 0.7582106052639982,\n",
       " 0.730096593058572,\n",
       " 0.7209432873240978,\n",
       " 0.41391708060558435,\n",
       " 0.6509829178776546,\n",
       " 0.5565578121099687,\n",
       " 0.5293174593139457,\n",
       " 0.5984324022186591,\n",
       " 0.7059367619686691,\n",
       " 0.6737768176335823,\n",
       " 0.3950561627840852,\n",
       " 0.7213743996936897,\n",
       " 0.6861346263851792,\n",
       " 0.5715925032148542,\n",
       " 0.7477521118358929,\n",
       " 0.5496017410549093,\n",
       " 0.5284611157817394,\n",
       " 0.4788363046597439,\n",
       " 0.7423904655045667,\n",
       " 0.6382986738896801,\n",
       " 0.5039811888034685,\n",
       " 0.6967388661373631,\n",
       " 0.41126917022119797,\n",
       " 0.5971243520117953,\n",
       " 0.6540590062154793,\n",
       " 0.615088123191736,\n",
       " 0.6496982844674712,\n",
       " 0.44658619282986356,\n",
       " 0.6324033261607666,\n",
       " 0.3984890626358934,\n",
       " 0.4081385196838761,\n",
       " 0.5608699537449003,\n",
       " 0.4646565488770526,\n",
       " 0.4223415407211881,\n",
       " 0.5916409263171708,\n",
       " 0.5842804727445073,\n",
       " 0.43399324073025114,\n",
       " 0.3319037272193259,\n",
       " 0.4203844583183572,\n",
       " 0.7410621620546173,\n",
       " 0.536702037641951,\n",
       " 0.6127323686621665,\n",
       " 0.6880926735671195,\n",
       " 0.7786123255850379,\n",
       " 0.4278597609401575,\n",
       " 0.7299467316639703,\n",
       " 0.7523084124074902,\n",
       " 0.5922076140159414,\n",
       " 0.7138789068187936,\n",
       " 0.5980110314477972,\n",
       " 0.4617545873300264,\n",
       " 0.6285503186290531,\n",
       " 0.56902171725482,\n",
       " 0.5437454151723983,\n",
       " 0.4771917460487449,\n",
       " 0.43675915585147074,\n",
       " 0.7373834285691929,\n",
       " 0.6288557724153593,\n",
       " 0.6512938451817141,\n",
       " 0.3971820968259228,\n",
       " 0.5768790932760229,\n",
       " 0.7351947250209169,\n",
       " 0.52891718265253,\n",
       " 0.6327533287529347,\n",
       " 0.6206715744914927,\n",
       " 0.4197246256316602,\n",
       " 0.7601998036093844,\n",
       " 0.527848545766699,\n",
       " 0.6743088237535687,\n",
       " 0.7210359254761175,\n",
       " 0.6865248589197978,\n",
       " 0.749770855595499,\n",
       " 0.589065061429145,\n",
       " 0.6462024313986013,\n",
       " 0.6553217036390099,\n",
       " 0.7050160871823904,\n",
       " 0.5854339156089914,\n",
       " 0.46099641850201173,\n",
       " 0.6819770572776845,\n",
       " 0.4229109002557839,\n",
       " 0.42872120095298777,\n",
       " 0.5837777276508412,\n",
       " 0.6225458049736404,\n",
       " 0.6960905813317977,\n",
       " 0.6778989872566085,\n",
       " 0.613647972415441,\n",
       " 0.6368809245162195,\n",
       " 0.38401897051683287,\n",
       " 0.7085948743018265,\n",
       " 0.6563036543362787,\n",
       " 0.6369065898977445,\n",
       " 0.6367195817928368,\n",
       " 0.6422581688196162,\n",
       " 0.6633960489815197,\n",
       " 0.5511104414851861,\n",
       " 0.49120507022351245,\n",
       " 0.41086241904035264,\n",
       " 0.6977781510246507,\n",
       " 0.4584660649924251,\n",
       " 0.6300494985020585,\n",
       " 0.6332870378776904,\n",
       " 0.6437879930109606,\n",
       " 0.39940009713755487,\n",
       " 0.5684774004154658,\n",
       " 0.6660143005265781,\n",
       " 0.5737017107088336,\n",
       " 0.7782530532384886,\n",
       " 0.5910492993700293,\n",
       " 0.5390985322482192,\n",
       " 0.3908746062103265,\n",
       " 0.4883331070230063,\n",
       " 0.5946733973362495,\n",
       " 0.5540246933715385,\n",
       " 0.3636584252493389,\n",
       " 0.7011658281905105,\n",
       " 0.6355007575423125,\n",
       " 0.6423191926419337,\n",
       " 0.7883893842969654,\n",
       " 0.7144073266486212,\n",
       " 0.7533596917370949,\n",
       " 0.6994705161237621,\n",
       " 0.568865841189674,\n",
       " 0.7664777968317219,\n",
       " 0.6943144720189399,\n",
       " 0.7130442842265157,\n",
       " 0.6801659956394818,\n",
       " 0.6014246958690984,\n",
       " 0.7211783911698744,\n",
       " 0.7376230188363123,\n",
       " 0.685261306527821,\n",
       " 0.5507876291449261,\n",
       " 0.7103802685317614,\n",
       " 0.771251950574781,\n",
       " 0.7304529986783524,\n",
       " 0.6318933251764725,\n",
       " 0.7008767784679778,\n",
       " 0.6278539730015701,\n",
       " 0.7491900231788081,\n",
       " 0.7436894078145596,\n",
       " 0.6435771008741917,\n",
       " 0.8055860786754013,\n",
       " 0.6664431280097619,\n",
       " 0.5197283986774298,\n",
       " 0.7564851146820487,\n",
       " 0.4609649247163448,\n",
       " 0.7607123017071385,\n",
       " 0.576353518127493,\n",
       " 0.7732453828374024,\n",
       " 0.6514581019519358,\n",
       " 0.547796781482237,\n",
       " 0.6808654481141833,\n",
       " 0.45846113848582887,\n",
       " 0.6863282728924643,\n",
       " 0.6984446947909128,\n",
       " 0.6504727115739718,\n",
       " 0.5438357409823982,\n",
       " 0.6080469558297219,\n",
       " 0.623230160222133,\n",
       " 0.6479101148889547,\n",
       " 0.3827304973813326,\n",
       " 0.42930990908080274,\n",
       " 0.5348798560460047,\n",
       " 0.6490609126323243,\n",
       " 0.574771354637745,\n",
       " 0.5054047253399759,\n",
       " 0.4470688863754086,\n",
       " 0.6037544128855735,\n",
       " 0.7033810098496648,\n",
       " 0.7373470953416761,\n",
       " 0.7134076750892137,\n",
       " 0.7139206705126689,\n",
       " 0.7878587391688567,\n",
       " 0.6331100025282346,\n",
       " 0.6982035901781297,\n",
       " 0.6977458349116152,\n",
       " 0.7621744860526198,\n",
       " 0.7396752943479076,\n",
       " 0.7063595764687096,\n",
       " 0.7443772817802188,\n",
       " 0.5064176335193024,\n",
       " 0.7594808342432114,\n",
       " 0.6319849731213396,\n",
       " 0.5391483015019717,\n",
       " 0.7173553108768566,\n",
       " 0.7805392796932757,\n",
       " 0.7298298687554552,\n",
       " 0.573308734374788,\n",
       " 0.7087372873015892,\n",
       " 0.44064192000879543,\n",
       " 0.6580898980367628,\n",
       " 0.6794833691182379,\n",
       " 0.5882837951612455,\n",
       " 0.5360257853500939,\n",
       " 0.5388669894829005,\n",
       " 0.7112847888638099,\n",
       " 0.4407020683313922,\n",
       " 0.5908325652675305,\n",
       " 0.46916600783484297,\n",
       " 0.6594611068736008,\n",
       " 0.6597355052499122,\n",
       " 0.398660372377803,\n",
       " 0.3935509964451022,\n",
       " 0.4491499712819147,\n",
       " 0.4334627458850768,\n",
       " 0.5884499643600128,\n",
       " 0.5535310242128023,\n",
       " 0.4980707639875239,\n",
       " 0.5782634679146453,\n",
       " 0.4734907152779184,\n",
       " 0.7627918049768247,\n",
       " 0.43415384609729646,\n",
       " 0.7664167439275283,\n",
       " 0.5115892366746531,\n",
       " 0.6276305185638573,\n",
       " 0.6577144501765732,\n",
       " 0.489159025172028,\n",
       " 0.36926019634755897,\n",
       " 0.5910893989732117,\n",
       " 0.7044783169020826,\n",
       " 0.45264193016722337,\n",
       " 0.7544206258409639,\n",
       " 0.7155291521013334,\n",
       " 0.706024493347989,\n",
       " 0.6730148915356199,\n",
       " 0.6412924252163013,\n",
       " 0.7398634187997164,\n",
       " 0.7403378940301204,\n",
       " 0.5837020432900335,\n",
       " 0.7614900387946237,\n",
       " 0.4404302136633211,\n",
       " 0.694437154636872,\n",
       " 0.6683323975741051,\n",
       " 0.41814977690227245,\n",
       " 0.6660894353006395,\n",
       " 0.600706916116926,\n",
       " 0.7499972037487873,\n",
       " 0.5795913640239134,\n",
       " 0.520850315171204,\n",
       " 0.6512836145182386,\n",
       " 0.6010587592266715,\n",
       " 0.4935104506526797,\n",
       " 0.5731693052704787,\n",
       " 0.618792119432719,\n",
       " 0.709130753892676,\n",
       " 0.47327539327189405,\n",
       " 0.4800643358440538,\n",
       " 0.7385829297027829,\n",
       " 0.7261018191854492,\n",
       " 0.43514534641672825,\n",
       " 0.6266907168201072,\n",
       " 0.5121008573261415,\n",
       " 0.6563389900578618,\n",
       " 0.6283254169536446,\n",
       " 0.41402487570624164,\n",
       " 0.6704865039248036,\n",
       " 0.44755973115789693,\n",
       " 0.7118814320869934,\n",
       " 0.4138931616431254,\n",
       " 0.3959715971743754,\n",
       " 0.617579460715916,\n",
       " 0.6163429557289005,\n",
       " 0.6515593396646884,\n",
       " 0.727774598350448,\n",
       " 0.624888828525309,\n",
       " 0.7418064918786517,\n",
       " 0.42648790222503075,\n",
       " 0.6894202666293562,\n",
       " 0.6783389542445311,\n",
       " 0.5754054661252361,\n",
       " 0.6747864678980614,\n",
       " 0.7185798181560532,\n",
       " 0.5576704130574319,\n",
       " 0.3918364234749631,\n",
       " 0.6733294835746362,\n",
       " 0.7209496492557413,\n",
       " 0.44353567059694204,\n",
       " 0.6265909100395826,\n",
       " 0.585241285957509,\n",
       " 0.6077036271101729,\n",
       " 0.4829977680555344,\n",
       " 0.5120299586793732,\n",
       " 0.503643350859943,\n",
       " 0.6613547698893638,\n",
       " 0.6251657736081538,\n",
       " 0.5355722350315576,\n",
       " 0.7500027198575365,\n",
       " 0.5517475209168511,\n",
       " 0.7806983224204118,\n",
       " 0.6589964838001047,\n",
       " 0.5541878822687809,\n",
       " 0.6070651418242569,\n",
       " 0.7099042862566786,\n",
       " 0.504119875957841,\n",
       " 0.521646415982893,\n",
       " 0.7675273655587601,\n",
       " 0.646439474762382,\n",
       " 0.7575444395018378,\n",
       " 0.7272963836087633,\n",
       " 0.7837381195133096,\n",
       " 0.43478887214928236,\n",
       " 0.7557986459453927,\n",
       " 0.5485222538475433,\n",
       " 0.43398585279327334,\n",
       " 0.5054353649107598,\n",
       " 0.7838927588866157,\n",
       " 0.7443942510894628,\n",
       " 0.4867606796691421,\n",
       " 0.6094199961805407,\n",
       " 0.5425286026170917,\n",
       " 0.4306210223610376,\n",
       " 0.7473355024170426,\n",
       " 0.4860380022731546,\n",
       " 0.6034363967451192,\n",
       " 0.7674316703743349,\n",
       " 0.6476008510081767,\n",
       " 0.6670333652869598,\n",
       " 0.4127252895697844,\n",
       " 0.6388536930660799,\n",
       " 0.6556232408904121,\n",
       " 0.6513830223868702,\n",
       " 0.693273920144041,\n",
       " 0.5467131072769243,\n",
       " 0.6277618803873004,\n",
       " 0.6635929644878943,\n",
       " 0.651882262583368,\n",
       " 0.41580538442955106,\n",
       " 0.6373550388032875,\n",
       " 0.5293156871834743,\n",
       " 0.44531128619348376,\n",
       " 0.6650381891375274,\n",
       " 0.42084584317528684,\n",
       " 0.5962969990579142,\n",
       " 0.6797287943020135,\n",
       " 0.6168405247542849,\n",
       " 0.6861138767345801,\n",
       " 0.6274097105318004,\n",
       " 0.5626667679200213,\n",
       " 0.6379975396666043,\n",
       " 0.6002182952887329,\n",
       " 0.7206504086484293,\n",
       " 0.669344857858075,\n",
       " 0.42427935483310436,\n",
       " 0.6965805044406334,\n",
       " 0.6447023748318905,\n",
       " 0.7157039690738801,\n",
       " 0.7568940118163227,\n",
       " 0.7675493856752852,\n",
       " 0.7399667273720222,\n",
       " 0.692447939318729,\n",
       " 0.7599576378983913,\n",
       " 0.6971429939053663,\n",
       " 0.716498187210535,\n",
       " 0.7415091158675288,\n",
       " 0.7657256445162793,\n",
       " 0.6763692104801698,\n",
       " 0.669783033661974,\n",
       " 0.7149029808119317,\n",
       " 0.49713656830840985,\n",
       " 0.7968814586660681,\n",
       " 0.6504241901888589,\n",
       " 0.6248128609622123,\n",
       " 0.6436343889491535,\n",
       " 0.6539387362994509,\n",
       " 0.6293488951236593,\n",
       " 0.6290440013675257,\n",
       " 0.4521452730602156,\n",
       " 0.4458028104650123,\n",
       " 0.7414641385158994,\n",
       " 0.6151259818903893,\n",
       " 0.7368274151623551,\n",
       " 0.647872181368316,\n",
       " 0.7466513139155305,\n",
       " 0.46732136912315697,\n",
       " 0.42597356809880355,\n",
       " 0.4035810443849703,\n",
       " 0.6722287280280509,\n",
       " 0.6060689345150597,\n",
       " 0.4313979819727101,\n",
       " 0.6056728683463298,\n",
       " 0.3751317924063562,\n",
       " 0.6856349769127356,\n",
       " 0.40245976710416537,\n",
       " 0.6385209727050117,\n",
       " 0.6443531302592708,\n",
       " 0.5102433846258047,\n",
       " 0.6347850735486534,\n",
       " 0.5085177830047697,\n",
       " 0.6071900865083292,\n",
       " 0.6094002301770688,\n",
       " 0.5984062428859259,\n",
       " 0.3314999559825272,\n",
       " 0.3692253842110664,\n",
       " 0.648159696477312,\n",
       " 0.617659857731799,\n",
       " 0.5698274581784893,\n",
       " 0.4595551172552507,\n",
       " 0.7474226395737175,\n",
       " 0.47206122019335517,\n",
       " 0.7524542697260763,\n",
       " 0.6642159372547427,\n",
       " 0.6619643345324729,\n",
       " 0.7178665294272908,\n",
       " 0.7259868912232034,\n",
       " 0.620979810622554,\n",
       " 0.6458384324778753,\n",
       " 0.6424012928313267,\n",
       " 0.6887480333411212,\n",
       " 0.6170546266768041,\n",
       " 0.7614166587195935,\n",
       " 0.4574994806946507,\n",
       " 0.40928045246977013,\n",
       " 0.49654545057764404,\n",
       " 0.7810749011146253,\n",
       " 0.6131888405852214,\n",
       " 0.6294890777832195,\n",
       " 0.5909590964978954,\n",
       " 0.6630448901497695,\n",
       " 0.6398902953782205,\n",
       " 0.6810530535157465,\n",
       " 0.6701061358423251,\n",
       " 0.6192234383431955,\n",
       " 0.4244764564311441,\n",
       " 0.5450558441574967,\n",
       " 0.6529385507882145,\n",
       " 0.4889052044193695,\n",
       " 0.5808174834277299,\n",
       " 0.6092957091671735,\n",
       " 0.3570287207168452,\n",
       " 0.7362556586784456,\n",
       " 0.6349838611751722,\n",
       " 0.5990425306613004,\n",
       " 0.621315958936192,\n",
       " 0.6696079675232579,\n",
       " 0.6198730619086044,\n",
       " 0.6331642753509769,\n",
       " 0.5530440575899975,\n",
       " 0.609697319746764,\n",
       " 0.6191640333447812,\n",
       " 0.549864844318138,\n",
       " 0.6570187251065388,\n",
       " 0.6175246592911544,\n",
       " 0.6088472728336641,\n",
       " 0.6819440316025673,\n",
       " 0.7452638208842965,\n",
       " 0.516819889296305,\n",
       " 0.6319217167887444,\n",
       " 0.685596439576789,\n",
       " 0.3941183235319994,\n",
       " 0.6752414852851119,\n",
       " 0.6039166470663888,\n",
       " 0.6970922429153488,\n",
       " 0.5396896327650545,\n",
       " 0.4082285030065038,\n",
       " 0.4716822758660046,\n",
       " 0.5856971461848061,\n",
       " 0.513796187579227,\n",
       " 0.3932400146170678,\n",
       " 0.6164404567131033,\n",
       " 0.6383868690933973,\n",
       " 0.6103864473581586,\n",
       " 0.5764335368380588,\n",
       " 0.39971414498298774,\n",
       " 0.5150899172930914,\n",
       " 0.439087333459802,\n",
       " 0.4503032954115406,\n",
       " 0.6244799406935369,\n",
       " 0.44563004876557005,\n",
       " 0.5438670752035724,\n",
       " 0.5316790701814557,\n",
       " 0.7987461760582985,\n",
       " 0.66134748021737,\n",
       " 0.6604687431771051,\n",
       " 0.6542971953098937,\n",
       " 0.5013631522237432,\n",
       " 0.5559757145711247,\n",
       " 0.762328680656624,\n",
       " 0.7807024454146813,\n",
       " 0.7048216691070944,\n",
       " 0.489185291067396,\n",
       " 0.7085988769173787,\n",
       " 0.7616751582424447,\n",
       " 0.7500774680996302,\n",
       " 0.6537300582150541,\n",
       " 0.6151540366056967,\n",
       " 0.7194574305221956,\n",
       " 0.6380834389620075,\n",
       " 0.5096481473151698,\n",
       " 0.7085909453309798,\n",
       " 0.7884303151463611,\n",
       " 0.7745657670646565,\n",
       " 0.7338788895179755,\n",
       " 0.7223385114924374,\n",
       " 0.5801124653762298,\n",
       " 0.7444888933956263,\n",
       " 0.6915058514116956,\n",
       " 0.599728945207389,\n",
       " 0.6310697052118185,\n",
       " 0.750501398305333,\n",
       " 0.6802864751865658,\n",
       " 0.6195576338710347,\n",
       " 0.6358021850925473,\n",
       " 0.5187701082412596,\n",
       " 0.4759723397690295,\n",
       " 0.6349958024020398,\n",
       " 0.6476886201826962,\n",
       " 0.6606141431229716,\n",
       " 0.6409980958163178,\n",
       " 0.7200252365280243,\n",
       " 0.48721340352902537,\n",
       " 0.5753651214426948,\n",
       " 0.5386631590144699,\n",
       " 0.6130390098316715,\n",
       " 0.6428956294098435,\n",
       " 0.41294179120072694,\n",
       " 0.6061395032954808,\n",
       " 0.5759112881512626,\n",
       " 0.5828937402291294,\n",
       " 0.6566223550597728,\n",
       " 0.6472074214938691,\n",
       " 0.7065406758411488,\n",
       " 0.6647597528758988,\n",
       " 0.7387675890446227,\n",
       " 0.7537426851262082,\n",
       " 0.4504624236781076,\n",
       " 0.5931944491030815,\n",
       " 0.5557487911903689,\n",
       " 0.4744636071250028,\n",
       " 0.731678874758293,\n",
       " 0.5490211573334696,\n",
       " 0.44860269699128913,\n",
       " 0.5165435023939028,\n",
       " 0.41260700884645257,\n",
       " 0.755910705907794,\n",
       " 0.7317148404652016,\n",
       " 0.7205779075874018,\n",
       " 0.7334158589820672,\n",
       " 0.6893868180972653,\n",
       " 0.7138629671190859,\n",
       " 0.5513718733162052,\n",
       " 0.772300766032466,\n",
       " 0.5152861230027835,\n",
       " 0.7476837337562302,\n",
       " 0.6099782392815288,\n",
       " 0.6109081023317147,\n",
       " 0.5951127751036894,\n",
       " 0.7083368438716997,\n",
       " 0.7142815413813924,\n",
       " 0.7518341364482094,\n",
       " 0.6314438833453074,\n",
       " 0.5681622850704596,\n",
       " 0.57094764869657,\n",
       " 0.5801130374575804,\n",
       " 0.5643168016411634,\n",
       " 0.7817120914133385,\n",
       " 0.6932704421960869,\n",
       " 0.7474070600071535,\n",
       " 0.634654794543233,\n",
       " 0.6454352754589597,\n",
       " 0.41193139054635863,\n",
       " 0.7471142847051322,\n",
       " 0.68864962200841,\n",
       " 0.5652120332463636,\n",
       " 0.6109686359273044,\n",
       " 0.7742435068881798,\n",
       " 0.6603587521223608,\n",
       " 0.7742528298102184,\n",
       " 0.6366409720239873,\n",
       " 0.4342576942945804,\n",
       " 0.4226838242781976,\n",
       " 0.6520767113891617,\n",
       " 0.5306815195972809,\n",
       " 0.48523285858808163,\n",
       " 0.6940869846917685,\n",
       " 0.7823356372958751,\n",
       " 0.5078667489479348,\n",
       " 0.46793785603550103,\n",
       " 0.6815040020952683,\n",
       " 0.6083308788123748,\n",
       " 0.5960300371757866,\n",
       " 0.4951423982844434,\n",
       " 0.5805496324066036,\n",
       " 0.5873522478582718,\n",
       " 0.7485780246789661,\n",
       " 0.6511882684924483,\n",
       " 0.6778023903860889,\n",
       " 0.5804550611807325,\n",
       " 0.7623551586731948,\n",
       " 0.3518038462190561,\n",
       " 0.6395354547582264,\n",
       " 0.7576050598750209,\n",
       " 0.5647954475808816,\n",
       " 0.3782148695136755,\n",
       " 0.657210326804111,\n",
       " 0.6728565384001953,\n",
       " 0.6462951207914971,\n",
       " 0.576198509406252,\n",
       " 0.6368742331356062,\n",
       " 0.6087126921014606,\n",
       " 0.7883572280663026,\n",
       " 0.6626756335137948,\n",
       " 0.7357197180279976,\n",
       " 0.5396458720565467,\n",
       " 0.5505030913805699,\n",
       " 0.4414172869973381,\n",
       " 0.6720820306613123,\n",
       " 0.7285311848959505,\n",
       " 0.7579003285570809,\n",
       " 0.7446791893265718,\n",
       " 0.6076153422433451,\n",
       " 0.7001339460822212,\n",
       " 0.6869884994476013,\n",
       " 0.6556866983677553,\n",
       " 0.682711188958363,\n",
       " 0.6011625954062129,\n",
       " 0.6488451597019796,\n",
       " 0.7570016630503252,\n",
       " 0.590988955952623,\n",
       " 0.7488110389466872,\n",
       " 0.700190894501889,\n",
       " 0.7040418089822023,\n",
       " 0.503321035565687,\n",
       " 0.7180206132626058,\n",
       " 0.5765144636122423,\n",
       " 0.6217579977933186,\n",
       " 0.6245145081495282,\n",
       " 0.6088503517909302,\n",
       " 0.43670973757243114,\n",
       " 0.7063449734451194,\n",
       " 0.4244030201438307,\n",
       " 0.35316771148413917,\n",
       " 0.5613787010216824,\n",
       " 0.580726958133701,\n",
       " 0.49008009573890315,\n",
       " 0.6858263706690444,\n",
       " 0.6284238766194444,\n",
       " 0.7683643246524733,\n",
       " 0.5627499436228667,\n",
       " 0.4516573914500917,\n",
       " 0.5509495445677776,\n",
       " 0.6218534544286095,\n",
       " 0.7424583687204422,\n",
       " 0.6086991096040059,\n",
       " 0.4288248992442934,\n",
       " 0.7219051182553095,\n",
       " 0.7279108592861707,\n",
       " 0.7080775211326805,\n",
       " 0.639020110570869,\n",
       " 0.6115570823162516,\n",
       " 0.6132500908900407,\n",
       " 0.5204882015564701,\n",
       " 0.719351472002329,\n",
       " 0.7080341701951554,\n",
       " 0.6699266139081881,\n",
       " 0.5923708690661629,\n",
       " 0.733437064582881,\n",
       " 0.6862539752424439,\n",
       " 0.6625289194890801,\n",
       " 0.7259618461792247,\n",
       " 0.4403590719986581,\n",
       " 0.44857768056129627,\n",
       " 0.5712815513735381,\n",
       " 0.6130155284142887,\n",
       " 0.7101814580603754,\n",
       " 0.5228285570143277,\n",
       " 0.7605181355702593,\n",
       " 0.4398512042105578,\n",
       " 0.4602057253077983,\n",
       " 0.5242662394958927,\n",
       " 0.702779240774373,\n",
       " 0.5553554046969181,\n",
       " 0.6559244496672005,\n",
       " 0.7298967587653818,\n",
       " 0.4301732772027549,\n",
       " 0.6087242246720609,\n",
       " 0.6520959204274189,\n",
       " 0.6708030863838882,\n",
       " 0.7102224523553762,\n",
       " 0.4121084188829083,\n",
       " 0.652398051642617,\n",
       " 0.7730839663300424,\n",
       " 0.4064655834907162,\n",
       " 0.7300447901613486,\n",
       " 0.7539020339912043,\n",
       " 0.6217576745110404,\n",
       " 0.42183194104607696,\n",
       " 0.5959972481766115,\n",
       " 0.6295183353971444,\n",
       " 0.6195453721806395,\n",
       " 0.7602500693478721,\n",
       " 0.7447598436039079,\n",
       " 0.5640725489613528,\n",
       " 0.41305812831374694,\n",
       " 0.44900322934143394,\n",
       " 0.7876174953456759,\n",
       " 0.7712858330906891,\n",
       " 0.5921144035364965,\n",
       " 0.629412262926069,\n",
       " 0.5320803365421782,\n",
       " 0.6580968897239586,\n",
       " 0.6472459582467751,\n",
       " 0.7682279470339072,\n",
       " 0.4280562151769303,\n",
       " 0.4449922630220738,\n",
       " 0.561534856422108,\n",
       " 0.5758533458039813,\n",
       " 0.6488151463651325,\n",
       " 0.613205958888667,\n",
       " 0.4952458760741374,\n",
       " 0.578185790599717,\n",
       " 0.5648628554460852,\n",
       " 0.6528913604573806,\n",
       " 0.6269871955347668,\n",
       " 0.4914841032167781,\n",
       " 0.48674793644335623,\n",
       " 0.760965995020127,\n",
       " 0.3669476456024255,\n",
       " 0.5274591655634442,\n",
       " 0.5202762930666093,\n",
       " 0.45958157496412394,\n",
       " 0.6941107663595463,\n",
       " 0.7903321757633014,\n",
       " 0.42618835259248405,\n",
       " 0.6229068005428816,\n",
       " 0.6632707742619118,\n",
       " 0.6052324479729585,\n",
       " 0.6804663868367936,\n",
       " 0.6605654531986044,\n",
       " 0.7043548559747781,\n",
       " 0.46297912114479584,\n",
       " 0.745866107179871,\n",
       " 0.7477239217127724,\n",
       " 0.6948630176714975,\n",
       " 0.6117798032297538,\n",
       " 0.6116687880550392,\n",
       " 0.5809592979022105,\n",
       " 0.627238482013618,\n",
       " 0.7835273739731363,\n",
       " 0.6208089647372587,\n",
       " 0.5850627955946707,\n",
       " 0.5925103660863922,\n",
       " 0.4483604674082778,\n",
       " 0.4880119229078244,\n",
       " 0.5007383748915873,\n",
       " 0.5145518999826969,\n",
       " 0.6481840983877847,\n",
       " 0.6609402436396833,\n",
       " 0.6380729657572329,\n",
       " 0.5345501522894092,\n",
       " 0.6349739004016055,\n",
       " 0.606177938799762,\n",
       " 0.5541391363764934,\n",
       " 0.42842317277791747,\n",
       " 0.6785744803599508,\n",
       " 0.43387148839095396,\n",
       " 0.5407271974517994,\n",
       " 0.6969163205320067,\n",
       " 0.6420720014908609,\n",
       " 0.5396416387065139,\n",
       " 0.7242199650845097,\n",
       " 0.7566496365067622,\n",
       " 0.41000666487771814,\n",
       " 0.721977842455328,\n",
       " 0.6867586362993522,\n",
       " 0.521778101285836,\n",
       " 0.6625297227086225,\n",
       " 0.6496403182366229,\n",
       " 0.710982931231741,\n",
       " 0.7045011188818447,\n",
       " 0.7620396583073865,\n",
       " 0.427587187992267,\n",
       " 0.6474391484479957,\n",
       " 0.5663123792519668,\n",
       " 0.6216369781420638,\n",
       " 0.7366922085950555,\n",
       " 0.7794429457239881,\n",
       " 0.42989548134785593,\n",
       " 0.46461239082604006,\n",
       " 0.508081754087336,\n",
       " 0.6430736118531427,\n",
       " 0.6402890383462745,\n",
       " 0.4271894744568727,\n",
       " 0.647803839695059,\n",
       " 0.7603588630061098,\n",
       " 0.784890596210846,\n",
       " 0.5936271248191521,\n",
       " 0.7094362998816255,\n",
       " 0.661251067543002,\n",
       " 0.5194360934275802,\n",
       " 0.4821267314310243,\n",
       " 0.6071785108272981,\n",
       " 0.5805691077272184,\n",
       " 0.4161812870981422,\n",
       " 0.6053093670646028,\n",
       " 0.6569110563992817,\n",
       " 0.7178092746632632,\n",
       " 0.6383963546391856,\n",
       " 0.5917975100836155,\n",
       " 0.7107795264482153,\n",
       " 0.6021158245035344,\n",
       " 0.5985915764890066,\n",
       " 0.6121591699258395,\n",
       " 0.7697953447993657,\n",
       " 0.6151729652197285,\n",
       " 0.5033091589884776,\n",
       " 0.41897769139489766,\n",
       " 0.5976333271784722,\n",
       " 0.6885482792756804,\n",
       " 0.4408463670147467,\n",
       " 0.5945137600115129,\n",
       " 0.530663882511079,\n",
       " 0.3430587060575887,\n",
       " 0.620465080513635,\n",
       " 0.5770086511870448,\n",
       " 0.6438556522465352,\n",
       " 0.6338042959043406,\n",
       " 0.643934313216379,\n",
       " 0.5812011513845778,\n",
       " 0.6888440950895475,\n",
       " 0.6792746648214899,\n",
       " 0.4166409173685464,\n",
       " 0.5843590225097697,\n",
       " 0.7252940923171965,\n",
       " 0.5797373518257002,\n",
       " 0.721219070947059,\n",
       " 0.5882880325304021,\n",
       " 0.6120874198906945,\n",
       " 0.35452643418311974,\n",
       " 0.6691202985541748,\n",
       " 0.6117743121644699,\n",
       " 0.687903778562493,\n",
       " 0.6262015675220479,\n",
       " 0.6533262317113204,\n",
       " 0.6432870559853155,\n",
       " 0.6530838187722142,\n",
       " 0.4292741279432746,\n",
       " 0.782997176248791,\n",
       " 0.4119681052430844,\n",
       " 0.747602520532134,\n",
       " 0.6275714120114174,\n",
       " 0.7509131400599475,\n",
       " 0.6379720626294217,\n",
       " 0.7313843090528289,\n",
       " 0.6165320001213862,\n",
       " 0.6959924244930717,\n",
       " 0.4727099010724827,\n",
       " 0.7314886739035558,\n",
       " 0.5861303625988734,\n",
       " 0.6860564247737698,\n",
       " 0.6702539691188593,\n",
       " 0.6891450147153885,\n",
       " 0.7223935034156542,\n",
       " 0.6889494211991612,\n",
       " 0.5805765168699825,\n",
       " 0.7815422134045988,\n",
       " 0.5402154380160002,\n",
       " 0.6326663116804084,\n",
       " 0.7350332398599514,\n",
       " 0.6604212960299823,\n",
       " 0.441781356720156,\n",
       " 0.7862023563994939,\n",
       " 0.6690520528655979,\n",
       " 0.5211622436122979,\n",
       " 0.6647562667206984,\n",
       " 0.5802605969678439,\n",
       " 0.6592466656219846,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict_temp.append((Y_predict_lr[i] + Y_predict_rf[i]) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7077419203718798,\n",
       " 0.6474088417135742,\n",
       " 0.5971902345743665,\n",
       " 0.8525318431487858,\n",
       " 0.6709162466998422,\n",
       " 0.749755824604128,\n",
       " 0.6996963114562007,\n",
       " 0.6441616785117448,\n",
       " 0.69376536312862,\n",
       " 0.648728768335646,\n",
       " 0.8568564238599079,\n",
       " 0.501205334560492,\n",
       " 0.7817839956565584,\n",
       " 0.5698564770004502,\n",
       " 0.8380179831475639,\n",
       " 0.39355046804840443,\n",
       " 0.8846866057802287,\n",
       " 0.5991760921568544,\n",
       " 0.7329030690549571,\n",
       " 0.6172307630478926,\n",
       " 0.5042821950100773,\n",
       " 0.754448616263977,\n",
       " 0.9001043433246093,\n",
       " 0.7642034371956548,\n",
       " 0.831840601745166,\n",
       " 0.7446599782410597,\n",
       " 0.7163125208631387,\n",
       " 0.8341837080840266,\n",
       " 0.8625470522024302,\n",
       " 0.8071699446559866,\n",
       " 0.7355361759523562,\n",
       " 0.6736070487073624,\n",
       " 0.9479326249388846,\n",
       " 0.7251243529871685,\n",
       " 0.7549185792318933,\n",
       " 0.4093566546527285,\n",
       " 0.7286298044105051,\n",
       " 0.7871072075277212,\n",
       " 0.49309657370458937,\n",
       " 0.8235617003059519,\n",
       " 0.48044732601261786,\n",
       " 0.8971174030505449,\n",
       " 0.582346104528662,\n",
       " 0.46780528140920186,\n",
       " 0.777134953463086,\n",
       " 0.655595729508089,\n",
       " 0.7262703241192907,\n",
       " 0.2800641920745904,\n",
       " 0.7854986607352981,\n",
       " 0.7589757029774331,\n",
       " 0.7662198670304595,\n",
       " 0.7125476594905797,\n",
       " 0.6306779076850499,\n",
       " 0.46499602750145863,\n",
       " 0.34894574089075825,\n",
       " 0.8411579119081456,\n",
       " 0.35977216508361476,\n",
       " 0.7201990846447153,\n",
       " 0.4573397558101726,\n",
       " 0.8076100460258271,\n",
       " 0.5633408531378807,\n",
       " 0.469028211073354,\n",
       " 0.4637239747661873,\n",
       " 0.7172656065790063,\n",
       " 0.5636341697889112,\n",
       " 0.8837044915263333,\n",
       " 0.6090514815621202,\n",
       " 0.5334312464492645,\n",
       " 0.7734454158981405,\n",
       " 0.659606519376482,\n",
       " 0.4836047345092903,\n",
       " 0.29229503270561863,\n",
       " 0.599213508927229,\n",
       " 0.540179867747633,\n",
       " 0.911658708216252,\n",
       " 0.6687542032358021,\n",
       " 0.7126838162939442,\n",
       " 0.7830755925562956,\n",
       " 0.3503640573768773,\n",
       " 0.3853209139382045,\n",
       " 0.7140159136367632,\n",
       " 0.23947823335648244,\n",
       " 0.41391316369847586,\n",
       " 0.6255982322999865,\n",
       " 0.7008183752144107,\n",
       " 0.713448428149019,\n",
       " 0.7055095315151996,\n",
       " 0.3803004297890703,\n",
       " 0.7146702815189079,\n",
       " 0.6831685450139375,\n",
       " 0.7659883117144515,\n",
       " 0.6124802909444506,\n",
       " 0.5846047442049493,\n",
       " 0.9542276224962656,\n",
       " 0.9602894752592079,\n",
       " 0.8990178971662623,\n",
       " 0.8746201376764582,\n",
       " 0.8919006021965605,\n",
       " 0.9009128976303518,\n",
       " 0.7443036928623693,\n",
       " 0.8563144375419194,\n",
       " 0.6869320764105318,\n",
       " 0.8011624192089615,\n",
       " 0.7231117545260057,\n",
       " 0.916428837002229,\n",
       " 0.8613413993940771,\n",
       " 0.7435050423153964,\n",
       " 0.9287822741406937,\n",
       " 0.6675982201275492,\n",
       " 0.8326347757561554,\n",
       " 0.7589860937747175,\n",
       " 0.676935982090942,\n",
       " 0.7278993533053528,\n",
       " 0.7974742385488307,\n",
       " 0.7233890998003198,\n",
       " 0.5929057416242685,\n",
       " 0.36062819428414655,\n",
       " 0.7790902924533493,\n",
       " 0.5779827697423567,\n",
       " 0.6490952326965699,\n",
       " 0.7181066997694412,\n",
       " 0.6630052654547011,\n",
       " 0.712023927366248,\n",
       " 0.359268727986108,\n",
       " 0.646257642972806,\n",
       " 0.8580335606675521,\n",
       " 0.7810194316754235,\n",
       " 0.5054971326732745,\n",
       " 0.8190491157297991,\n",
       " 0.8658654016553122,\n",
       " 0.7732330837746431,\n",
       " 0.7397684330353405,\n",
       " 0.7545907829172769,\n",
       " 0.6535534968679875,\n",
       " 0.8219290223653215,\n",
       " 0.8824121145610364,\n",
       " 0.8401718654579904,\n",
       " 0.8264886368977831,\n",
       " 0.3660377503105372,\n",
       " 0.7216952167937464,\n",
       " 0.5800048676362359,\n",
       " 0.5391958666576098,\n",
       " 0.6427916890461436,\n",
       " 0.8039966110487831,\n",
       " 0.7558258359340924,\n",
       " 0.33780392186281905,\n",
       " 0.8271668959139018,\n",
       " 0.7743424622961983,\n",
       " 0.6025625071301732,\n",
       " 0.8666864956671236,\n",
       " 0.5695538333345308,\n",
       " 0.5378012019757554,\n",
       " 0.46349699305757847,\n",
       " 0.8586856898168322,\n",
       " 0.702632760199373,\n",
       " 0.5012367902242458,\n",
       " 0.7902306127054955,\n",
       " 0.36216539468841014,\n",
       " 0.6408669560457996,\n",
       " 0.7262356779595409,\n",
       " 0.6678708763559786,\n",
       " 0.7196600842482039,\n",
       " 0.4151166993133073,\n",
       " 0.6938037754998474,\n",
       " 0.34298456407903843,\n",
       " 0.357456246255932,\n",
       " 0.5864540066023054,\n",
       " 0.44223692592902797,\n",
       " 0.37879695783127865,\n",
       " 0.6325418903168969,\n",
       " 0.6215737488105934,\n",
       " 0.39624357303423474,\n",
       " 0.24311010743055134,\n",
       " 0.3758025938496061,\n",
       " 0.8567374314860092,\n",
       " 0.5502168845974633,\n",
       " 0.6642861931833743,\n",
       " 0.7772535156416642,\n",
       " 0.9130439907255031,\n",
       " 0.3870272600950048,\n",
       " 0.8399642991084278,\n",
       " 0.8735261321286152,\n",
       " 0.6334593645206161,\n",
       " 0.8159320012518264,\n",
       " 0.6422021608038979,\n",
       " 0.43788449024796927,\n",
       " 0.6879826601602239,\n",
       " 0.5986494951613285,\n",
       " 0.560842121782035,\n",
       " 0.4609265920707828,\n",
       " 0.4003760842410733,\n",
       " 0.8511952509813834,\n",
       " 0.6883643978827587,\n",
       " 0.7220712173060336,\n",
       " 0.3410154130859454,\n",
       " 0.6104394632973656,\n",
       " 0.8479059073689454,\n",
       " 0.5385296779399765,\n",
       " 0.6942700390106733,\n",
       " 0.6761613253030654,\n",
       " 0.374853465384975,\n",
       " 0.8854052104037008,\n",
       " 0.5369710684669431,\n",
       " 0.7565713930266694,\n",
       " 0.8266776024227791,\n",
       " 0.7749108237743652,\n",
       " 0.8698348338750448,\n",
       " 0.6286653673931988,\n",
       " 0.7145115528672501,\n",
       " 0.7281138990616489,\n",
       " 0.8026715378284868,\n",
       " 0.6233951380161482,\n",
       " 0.4366939802558405,\n",
       " 0.7681536731414871,\n",
       " 0.37956033846846887,\n",
       " 0.3882403545529558,\n",
       " 0.6208613246191146,\n",
       " 0.6789717769566155,\n",
       " 0.7892114554556433,\n",
       " 0.7619420649455818,\n",
       " 0.6656246704914477,\n",
       " 0.7005137059290686,\n",
       " 0.321267385762203,\n",
       " 0.8079592224348718,\n",
       " 0.7296270880450979,\n",
       " 0.7005105105523358,\n",
       " 0.7002390881033025,\n",
       " 0.7084837281153649,\n",
       " 0.740251881537694,\n",
       " 0.5718243345605427,\n",
       " 0.4819795397014949,\n",
       " 0.3615635231604435,\n",
       " 0.791772403701054,\n",
       " 0.43289949307274417,\n",
       " 0.6902316981906336,\n",
       " 0.6951250813431316,\n",
       " 0.7108610168353953,\n",
       " 0.34433061183372793,\n",
       " 0.5979951445360863,\n",
       " 0.7441814344248159,\n",
       " 0.6057950127242459,\n",
       " 0.9125068703450224,\n",
       " 0.6317819144290369,\n",
       " 0.5538383591900474,\n",
       " 0.33147836005439657,\n",
       " 0.4776860192247713,\n",
       " 0.6371599170473887,\n",
       " 0.5762763276721301,\n",
       " 0.29077913915765397,\n",
       " 0.7968933181200402,\n",
       " 0.6984775493250196,\n",
       " 0.7086458953528999,\n",
       " 0.9276334040573714,\n",
       " 0.8167100874632425,\n",
       " 0.8751767820686094,\n",
       " 0.794335717079625,\n",
       " 0.5984739446038774,\n",
       " 0.8947818180903715,\n",
       " 0.7866018595386486,\n",
       " 0.8146367943913024,\n",
       " 0.765313550057886,\n",
       " 0.6473139551577125,\n",
       " 0.8269068875780233,\n",
       " 0.8514962238301826,\n",
       " 0.77307590449388,\n",
       " 0.5714142046236086,\n",
       " 0.8105977064933207,\n",
       " 0.901966860760548,\n",
       " 0.8408324483043939,\n",
       " 0.6929417674640342,\n",
       " 0.7965193779833388,\n",
       " 0.6869952727710167,\n",
       " 0.8688486972972038,\n",
       " 0.8606144039467234,\n",
       " 0.7104568511861655,\n",
       " 0.9534700794694009,\n",
       " 0.7447756210269414,\n",
       " 0.5248339419912118,\n",
       " 0.8798357400124224,\n",
       " 0.436591307257699,\n",
       " 0.8861743747828452,\n",
       " 0.6097021817551433,\n",
       " 0.9049444623767549,\n",
       " 0.722326811378984,\n",
       " 0.5668933922379276,\n",
       " 0.7664190253569285,\n",
       " 0.432955463050246,\n",
       " 0.774567039122327,\n",
       " 0.7928149856830733,\n",
       " 0.7208527491255604,\n",
       " 0.5609529937787425,\n",
       " 0.6572100325910184,\n",
       " 0.6799638881469965,\n",
       " 0.7170007478663452,\n",
       " 0.31933336475676755,\n",
       " 0.3891564377178838,\n",
       " 0.5474824201106483,\n",
       " 0.7187166030755312,\n",
       " 0.6073559076222821,\n",
       " 0.5033802906676634,\n",
       " 0.41580551328656257,\n",
       " 0.6507436212242648,\n",
       " 0.8001543699991065,\n",
       " 0.8511117534804249,\n",
       " 0.8151859039988719,\n",
       " 0.8159716692020784,\n",
       " 0.9268938819638106,\n",
       " 0.6948181328931514,\n",
       " 0.792407462989744,\n",
       " 0.7917240189384678,\n",
       " 0.888352183895748,\n",
       " 0.8546613916580461,\n",
       " 0.8046068717316441,\n",
       " 0.861668268613779,\n",
       " 0.5047976395871201,\n",
       " 0.8843173984252115,\n",
       " 0.69311175371506,\n",
       " 0.5539549449382755,\n",
       " 0.8211485444898758,\n",
       " 0.9159340344576687,\n",
       " 0.8398085252668193,\n",
       " 0.60514516871802,\n",
       " 0.8082483610194462,\n",
       " 0.4061581495978489,\n",
       " 0.7322324246270131,\n",
       " 0.7643614338729745,\n",
       " 0.6275248200345013,\n",
       " 0.5491459413517858,\n",
       " 0.5534558783016518,\n",
       " 0.8120650536074517,\n",
       " 0.4063135199584835,\n",
       " 0.6313857645387164,\n",
       " 0.4489930379363465,\n",
       " 0.7343344480053322,\n",
       " 0.7347126073640804,\n",
       " 0.34321697157825537,\n",
       " 0.3355749613977711,\n",
       " 0.4189874009421897,\n",
       " 0.3954407378137449,\n",
       " 0.6278231880599472,\n",
       " 0.5755260487722826,\n",
       " 0.49232588327262194,\n",
       " 0.6125006770592696,\n",
       " 0.4554507140110854,\n",
       " 0.8892591782026944,\n",
       " 0.3964353668575078,\n",
       " 0.8947274618347434,\n",
       " 0.5126193577318587,\n",
       " 0.6865468124870518,\n",
       " 0.7317409870267159,\n",
       " 0.4789091608220831,\n",
       " 0.2991265124969549,\n",
       " 0.6318203379407898,\n",
       " 0.8017583689078114,\n",
       " 0.4242152064805409,\n",
       " 0.8767822797229022,\n",
       " 0.8184009318740003,\n",
       " 0.8041060947945513,\n",
       " 0.7545717245199979,\n",
       " 0.7070373180822491,\n",
       " 0.8548634696931812,\n",
       " 0.8556352640207209,\n",
       " 0.6207509273217537,\n",
       " 0.8873253937994642,\n",
       " 0.40590356238684255,\n",
       " 0.7867621608169258,\n",
       " 0.7476071113853429,\n",
       " 0.37246937699090565,\n",
       " 0.7442663012167978,\n",
       " 0.6462166027177169,\n",
       " 0.8701199370642974,\n",
       " 0.6145731365052305,\n",
       " 0.5264772094454199,\n",
       " 0.7220511625438831,\n",
       " 0.6467377214644427,\n",
       " 0.4854387428334964,\n",
       " 0.6049526547574484,\n",
       " 0.6733907205003967,\n",
       " 0.8088053313166143,\n",
       " 0.455108448899464,\n",
       " 0.4652963033037394,\n",
       " 0.8529771279278164,\n",
       " 0.834276532393744,\n",
       " 0.39796311869278034,\n",
       " 0.685205237980405,\n",
       " 0.5132476118635408,\n",
       " 0.7296868269523322,\n",
       " 0.6875411187280438,\n",
       " 0.3662821742084713,\n",
       " 0.750849983224089,\n",
       " 0.416607732999739,\n",
       " 0.8129064636688966,\n",
       " 0.36611884598222044,\n",
       " 0.33922970434959104,\n",
       " 0.6715508707995209,\n",
       " 0.6696709005542578,\n",
       " 0.7224388520454029,\n",
       " 0.836746183261756,\n",
       " 0.682472454204208,\n",
       " 0.8577816853925931,\n",
       " 0.3849156490280826,\n",
       " 0.7792684490697057,\n",
       " 0.7626609048165036,\n",
       " 0.6082468443599733,\n",
       " 0.7573793225708165,\n",
       " 0.8229753514329934,\n",
       " 0.581725356877484,\n",
       " 0.3329542857384914,\n",
       " 0.7552074061423649,\n",
       " 0.8265836528558563,\n",
       " 0.41052804136065363,\n",
       " 0.6850444115433674,\n",
       " 0.6229957759344783,\n",
       " 0.6566624655733252,\n",
       " 0.4696992828415872,\n",
       " 0.5133169485838638,\n",
       " 0.5007527128762946,\n",
       " 0.7371536934731143,\n",
       " 0.6828602450756218,\n",
       " 0.5485557380968817,\n",
       " 0.870170560327534,\n",
       " 0.57282313727318,\n",
       " 0.9162115203814112,\n",
       " 0.7336535172422103,\n",
       " 0.5764653806751288,\n",
       " 0.6557257482817344,\n",
       " 0.8100232377517933,\n",
       " 0.5013367279325043,\n",
       " 0.5276598569665116,\n",
       " 0.8963234482309472,\n",
       " 0.7148245899988738,\n",
       " 0.8814195416380106,\n",
       " 0.8360072842678201,\n",
       " 0.9206899450876067,\n",
       " 0.3973863264124001,\n",
       " 0.8787697078764479,\n",
       " 0.5679379999882338,\n",
       " 0.3962017053297415,\n",
       " 0.503352966113088,\n",
       " 0.9209509614118938,\n",
       " 0.8616666918712395,\n",
       " 0.47536177007401037,\n",
       " 0.659266165850172,\n",
       " 0.5588689344184201,\n",
       " 0.39119880553710085,\n",
       " 0.8660927547680657,\n",
       " 0.4742620183538085,\n",
       " 0.6503511163998748,\n",
       " 0.8961866407791691,\n",
       " 0.7165603958798645,\n",
       " 0.745730207737645,\n",
       " 0.36434009657277044,\n",
       " 0.7034273506075732,\n",
       " 0.728564029369218,\n",
       " 0.7222382127032301,\n",
       " 0.7850019608816499,\n",
       " 0.5651993057861446,\n",
       " 0.6867839095435055,\n",
       " 0.7405427844488977,\n",
       " 0.7230195575293794,\n",
       " 0.3689471556428923,\n",
       " 0.701100840093893,\n",
       " 0.5390496506749612,\n",
       " 0.4131975444292332,\n",
       " 0.7427596462436752,\n",
       " 0.3764592957783264,\n",
       " 0.639673282505252,\n",
       " 0.7646143260679648,\n",
       " 0.6704768587371097,\n",
       " 0.7743152717268548,\n",
       " 0.6863353163679978,\n",
       " 0.5891322107389034,\n",
       " 0.7021509585191479,\n",
       " 0.6454629886636902,\n",
       " 0.8260776906951934,\n",
       " 0.7490631275273162,\n",
       " 0.3816639525034102,\n",
       " 0.7899464593281864,\n",
       " 0.7121685741783015,\n",
       " 0.8186398221143907,\n",
       " 0.8804241411699949,\n",
       " 0.8964095563419072,\n",
       " 0.8549627320110973,\n",
       " 0.7837436777387747,\n",
       " 0.8850378193143991,\n",
       " 0.7908510498676014,\n",
       " 0.8199136123454198,\n",
       " 0.8573011996935509,\n",
       " 0.8936504007687123,\n",
       " 0.7597235447146242,\n",
       " 0.7497920956208297,\n",
       " 0.8174695129506857,\n",
       " 0.49091430795272706,\n",
       " 0.940418305257174,\n",
       " 0.7207760629436581,\n",
       " 0.6823801095432941,\n",
       " 0.7105692477608885,\n",
       " 0.7260650482472415,\n",
       " 0.6891281026170533,\n",
       " 0.6887467877072639,\n",
       " 0.4234887578645544,\n",
       " 0.4139059225838099,\n",
       " 0.8573159582735745,\n",
       " 0.6678330718326999,\n",
       " 0.8503527074069237,\n",
       " 0.7169710869080497,\n",
       " 0.865031752310216,\n",
       " 0.4461901680703403,\n",
       " 0.3842517640248838,\n",
       " 0.3506412525611132,\n",
       " 0.7534502063571094,\n",
       " 0.6543326460047674,\n",
       " 0.39234940339806057,\n",
       " 0.6537198606955354,\n",
       " 0.30795715239661314,\n",
       " 0.7735750470894556,\n",
       " 0.3488674560799846,\n",
       " 0.702952648365684,\n",
       " 0.7116814237805534,\n",
       " 0.5106127688084428,\n",
       " 0.697303082868604,\n",
       " 0.5079254822712078,\n",
       " 0.6559375138052336,\n",
       " 0.6592479907390834,\n",
       " 0.6427436583619395,\n",
       " 0.24253243495607535,\n",
       " 0.29906958552527874,\n",
       " 0.7174274233246716,\n",
       " 0.6716712279047663,\n",
       " 0.5999320759109988,\n",
       " 0.43458480829864754,\n",
       " 0.8662141323761707,\n",
       " 0.4533597579366697,\n",
       " 0.8737251889690736,\n",
       " 0.7414419874517857,\n",
       " 0.7380503378582797,\n",
       " 0.821897669736029,\n",
       " 0.8340079087513852,\n",
       " 0.6765736414003686,\n",
       " 0.7139357819660959,\n",
       " 0.7087629957655448,\n",
       " 0.778234945977258,\n",
       " 0.6707000513872001,\n",
       " 0.887210018873534,\n",
       " 0.43150031037646397,\n",
       " 0.3591694732603192,\n",
       " 0.4900334130071215,\n",
       " 0.9167061145465413,\n",
       " 0.664991077240213,\n",
       " 0.6893540526279696,\n",
       " 0.6316656538048416,\n",
       " 0.7397502964501639,\n",
       " 0.7049884231565183,\n",
       " 0.7667483257913504,\n",
       " 0.7502213463740377,\n",
       " 0.6739882866155927,\n",
       " 0.38191657034694204,\n",
       " 0.5628024306439753,\n",
       " 0.7246204109162848,\n",
       " 0.47853799623858184,\n",
       " 0.6163698174992303,\n",
       " 0.6590732384238407,\n",
       " 0.2807558148208428,\n",
       " 0.8495344117466113,\n",
       " 0.6976597364649069,\n",
       " 0.6437282201729352,\n",
       " 0.6770933610784671,\n",
       " 0.7495991146378531,\n",
       " 0.6749608742197181,\n",
       " 0.694951219354285,\n",
       " 0.574738676402315,\n",
       " 0.6596880818616622,\n",
       " 0.673916524069601,\n",
       " 0.5700041887699163,\n",
       " 0.7307385564288817,\n",
       " 0.6714089448083137,\n",
       " 0.6584454364187702,\n",
       " 0.768051980564633,\n",
       " 0.8630340784753401,\n",
       " 0.520448766573089,\n",
       " 0.6929980639507405,\n",
       " 0.7735234399685922,\n",
       " 0.33636473805793254,\n",
       " 0.7580021545996496,\n",
       " 0.6510290831770216,\n",
       " 0.7907452402694771,\n",
       " 0.5547636039727923,\n",
       " 0.35760195007593315,\n",
       " 0.4527675889947008,\n",
       " 0.6237308262699306,\n",
       " 0.5158575730615742,\n",
       " 0.33514186725679374,\n",
       " 0.6698353314472185,\n",
       " 0.7026745433518573,\n",
       " 0.660792643201392,\n",
       " 0.6098926029064716,\n",
       " 0.34484814542247966,\n",
       " 0.5178651036484536,\n",
       " 0.40393573370448876,\n",
       " 0.42075773948114153,\n",
       " 0.681848304213523,\n",
       " 0.4136714265552613,\n",
       " 0.5608850475530545,\n",
       " 0.5427346470755434,\n",
       " 0.9432558528993221,\n",
       " 0.7372373813387045,\n",
       " 0.7358193783936637,\n",
       " 0.7265534437216765,\n",
       " 0.4973228781788307,\n",
       " 0.5791183102828956,\n",
       " 0.8886021916602135,\n",
       " 0.9161991976306125,\n",
       " 0.8023866162380799,\n",
       " 0.4790378474230087,\n",
       " 0.8079308344781645,\n",
       " 0.8876122818888135,\n",
       " 0.8702653675413672,\n",
       " 0.7257887177795057,\n",
       " 0.6679040919606996,\n",
       " 0.8243055088528278,\n",
       " 0.7022595716853512,\n",
       " 0.5096994088446878,\n",
       " 0.807974727046076,\n",
       " 0.9277700213931714,\n",
       " 0.9069184226020659,\n",
       " 0.8458695990398417,\n",
       " 0.8285696714306271,\n",
       " 0.615398359529036,\n",
       " 0.8618175364225562,\n",
       " 0.7823371619937947,\n",
       " 0.6446805943724391,\n",
       " 0.6917127450163667,\n",
       " 0.87085855612194,\n",
       " 0.7655785503462245,\n",
       " 0.6744496745976745,\n",
       " 0.6988408797296565,\n",
       " 0.5233205700195126,\n",
       " 0.4592182416615246,\n",
       " 0.6976965429776019,\n",
       " 0.7166720522833216,\n",
       " 0.7360939835157104,\n",
       " 0.7066037812023516,\n",
       " 0.8251349555267471,\n",
       " 0.4760307826788682,\n",
       " 0.6082229543903759,\n",
       " 0.5531709048177101,\n",
       " 0.6647255615328618,\n",
       " 0.709508434539875,\n",
       " 0.3645877504111673,\n",
       " 0.654377315007537,\n",
       " 0.609004266096828,\n",
       " 0.6194993124787802,\n",
       " 0.7300636244952531,\n",
       " 0.7158976531510668,\n",
       " 0.8049335954820754,\n",
       " 0.7422866489385583,\n",
       " 0.853263415265161,\n",
       " 0.8757085654242978,\n",
       " 0.42092949586490924,\n",
       " 0.6348820092621509,\n",
       " 0.5787296454494938,\n",
       " 0.45695269890504875,\n",
       " 0.8425796500852849,\n",
       " 0.5687341581422332,\n",
       " 0.4181265841965742,\n",
       " 0.5200549584381899,\n",
       " 0.3641077796099346,\n",
       " 0.8789678385610166,\n",
       " 0.8426557715735044,\n",
       " 0.8259867906947623,\n",
       " 0.845185722467394,\n",
       " 0.7791620988938684,\n",
       " 0.8159037405631961,\n",
       " 0.5722299231544683,\n",
       " 0.9035227687977683,\n",
       " 0.5181930888372899,\n",
       " 0.8665978462321846,\n",
       " 0.660071463202765,\n",
       " 0.6615180542143536,\n",
       " 0.6378444050795453,\n",
       " 0.8075385001653834,\n",
       " 0.8164910707982086,\n",
       " 0.8727824124722257,\n",
       " 0.6922832807388626,\n",
       " 0.5973570984316476,\n",
       " 0.6015711152739351,\n",
       " 0.6153609514691158,\n",
       " 0.5916155761685628,\n",
       " 0.9176934010493749,\n",
       " 0.7850574512904225,\n",
       " 0.8662450628577152,\n",
       " 0.6971453941006162,\n",
       " 0.7133134332651913,\n",
       " 0.36312904206305296,\n",
       " 0.8657769022449999,\n",
       " 0.7781594505983693,\n",
       " 0.5931074649906238,\n",
       " 0.6616033411780966,\n",
       " 0.906444420068903,\n",
       " 0.7356745381814813,\n",
       " 0.9064623979631607,\n",
       " 0.7001344056811682,\n",
       " 0.39664987953085984,\n",
       " 0.37922565516424467,\n",
       " 0.723204001982119,\n",
       " 0.5412228239916398,\n",
       " 0.47301105965641443,\n",
       " 0.7862449525263155,\n",
       " 0.9185686085889236,\n",
       " 0.5069627594635434,\n",
       " 0.44709975885908354,\n",
       " 0.7674424810424537,\n",
       " 0.6576332050536604,\n",
       " 0.6392418750690998,\n",
       " 0.487876054654372,\n",
       " 0.616023622298794,\n",
       " 0.6262214658025291,\n",
       " 0.867966849764497,\n",
       " 0.7219425651875555,\n",
       " 0.7618612446329359,\n",
       " 0.615819776629421,\n",
       " 0.8886420278943594,\n",
       " 0.2729840979857344,\n",
       " 0.7044246909740858,\n",
       " 0.881521052022233,\n",
       " 0.5924265879286345,\n",
       " 0.31260653378749625,\n",
       " 0.7310023853382313,\n",
       " 0.754402620949063,\n",
       " 0.7146407223878835,\n",
       " 0.6095299885737944,\n",
       " 0.7004480577245733,\n",
       " 0.6582688972944943,\n",
       " 0.9275775999214906,\n",
       " 0.7391509331522381,\n",
       " 0.8487254641784558,\n",
       " 0.5546871446670039,\n",
       " 0.5709225183212363,\n",
       " 0.4073552045305073,\n",
       " 0.7532209215870613,\n",
       " 0.8379032062055438,\n",
       " 0.8819808827644391,\n",
       " 0.8620928177270403,\n",
       " 0.6565670229551667,\n",
       " 0.7952890195566815,\n",
       " 0.7756378750278015,\n",
       " 0.7286508411326418,\n",
       " 0.7692162766550128,\n",
       " 0.646885190688131,\n",
       " 0.7184408362097688,\n",
       " 0.8805906844158046,\n",
       " 0.6316543848185219,\n",
       " 0.8683082351320669,\n",
       " 0.7953749488256637,\n",
       " 0.8011731358461214,\n",
       " 0.5001701770152938,\n",
       " 0.8221892942034484,\n",
       " 0.6099027708005962,\n",
       " 0.6777848805820372,\n",
       " 0.6819505511246677,\n",
       " 0.6583989736740967,\n",
       " 0.4003280934592479,\n",
       " 0.8046684435012669,\n",
       " 0.3818639642005025,\n",
       " 0.27499159989909083,\n",
       " 0.587307339147346,\n",
       " 0.6162680042057089,\n",
       " 0.48035069914271755,\n",
       " 0.773871227432247,\n",
       " 0.6878581152202281,\n",
       " 0.8976243652154805,\n",
       " 0.5893635473980299,\n",
       " 0.4227765751798221,\n",
       " 0.5716070396585968,\n",
       " 0.6778946273292547,\n",
       " 0.8588012239066216,\n",
       " 0.65812704928226,\n",
       " 0.38853785045144684,\n",
       " 0.8281089455313863,\n",
       " 0.8369374616434895,\n",
       " 0.8071941897381139,\n",
       " 0.7036909243516344,\n",
       " 0.6624720930769623,\n",
       " 0.6650087449146966,\n",
       " 0.5258927330044898,\n",
       " 0.8241759263605797,\n",
       " 0.8071530051897363,\n",
       " 0.7500842367726215,\n",
       " 0.6337050219563305,\n",
       " 0.8453159381371388,\n",
       " 0.7744600033910096,\n",
       " 0.73893053428962,\n",
       " 0.8340314061439895,\n",
       " 0.4058189334107373,\n",
       " 0.41814908142887375,\n",
       " 0.602138130445088,\n",
       " 0.664643847783863,\n",
       " 0.8103095937735312,\n",
       " 0.5294196574685893,\n",
       " 0.8858473329883387,\n",
       " 0.4050204152674053,\n",
       " 0.4355256132416962,\n",
       " 0.531620854872196,\n",
       " 0.7993035724271237,\n",
       " 0.5782136542827735,\n",
       " 0.7289846097005384,\n",
       " 0.8399228971755539,\n",
       " 0.3905721893064822,\n",
       " 0.6582035543104137,\n",
       " 0.7232892909404296,\n",
       " 0.7513575798626975,\n",
       " 0.8104569457068314,\n",
       " 0.36344295373711266,\n",
       " 0.7237098542202121,\n",
       " 0.9047328253915174,\n",
       " 0.354878654252569,\n",
       " 0.8401391328166384,\n",
       " 0.8758973420062461,\n",
       " 0.6777975682851153,\n",
       " 0.3778901628222436,\n",
       " 0.6391193778572635,\n",
       " 0.6894550402985516,\n",
       " 0.6744627831168455,\n",
       " 0.8855000997302737,\n",
       " 0.8622268823625727,\n",
       " 0.5913660520549291,\n",
       " 0.36478377335746126,\n",
       " 0.41870413691032904,\n",
       " 0.9265405694955646,\n",
       " 0.9019709246905838,\n",
       " 0.6333405296364097,\n",
       " 0.6892194588326918,\n",
       " 0.5433608653116957,\n",
       " 0.7323042751396031,\n",
       " 0.7160171490877683,\n",
       " 0.8974275771937745,\n",
       " 0.38730924566082703,\n",
       " 0.4126979096278678,\n",
       " 0.5875156740298295,\n",
       " 0.6089497477003414,\n",
       " 0.718355374453018,\n",
       " 0.6649783096995013,\n",
       " 0.48806435191676334,\n",
       " 0.612388840051492,\n",
       " 0.59252233530841,\n",
       " 0.7244934480423331,\n",
       " 0.6856163986373858,\n",
       " 0.48252629878230524,\n",
       " 0.4754016638336592,\n",
       " 0.8865301192201015,\n",
       " 0.29563801684647883,\n",
       " 0.5363411919925509,\n",
       " 0.5256724728755182,\n",
       " 0.4346107857936591,\n",
       " 0.7862942446893133,\n",
       " 0.9305841289041226,\n",
       " 0.3845165414924858,\n",
       " 0.6795551723757542,\n",
       " 0.740022901858032,\n",
       " 0.6530304708943743,\n",
       " 0.7658374505669274,\n",
       " 0.7359588405893036,\n",
       " 0.8016000592116483,\n",
       " 0.4397028135302431,\n",
       " 0.8638537037881476,\n",
       " 0.8667164811142329,\n",
       " 0.7874638680714248,\n",
       " 0.6628349634906421,\n",
       " 0.6627000311903011,\n",
       " 0.6165957416397689,\n",
       " 0.6860425915945694,\n",
       " 0.9203526373261289,\n",
       " 0.6764344658970868,\n",
       " 0.6227629389097367,\n",
       " 0.6339291686478683,\n",
       " 0.41773492761578884,\n",
       " 0.47723708521126973,\n",
       " 0.4962539561133209,\n",
       " 0.5170567663806769,\n",
       " 0.7175105476156278,\n",
       " 0.7366036382885808,\n",
       " 0.70229616495398,\n",
       " 0.5469744534306805,\n",
       " 0.6976070059597691,\n",
       " 0.6544239414046753,\n",
       " 0.5764392600991028,\n",
       " 0.3878937759191193,\n",
       " 0.7630444731491793,\n",
       " 0.39603683444647725,\n",
       " 0.5563899566581985,\n",
       " 0.7905234673759978,\n",
       " 0.7082806518582547,\n",
       " 0.5547005535816978,\n",
       " 0.8314282106520483,\n",
       " 0.8800799001451225,\n",
       " 0.3602735142195008,\n",
       " 0.8280885407406398,\n",
       " 0.7752690894359058,\n",
       " 0.527959368468137,\n",
       " 0.7388729391368627,\n",
       " 0.7196562237767484,\n",
       " 0.8116363772381603,\n",
       " 0.8019049564351785,\n",
       " 0.8881477369060413,\n",
       " 0.38668855534006985,\n",
       " 0.7163244879574855,\n",
       " 0.5946118334100394,\n",
       " 0.6776130964645757,\n",
       " 0.850153801660207,\n",
       " 0.9142657512504719,\n",
       " 0.3900462104079381,\n",
       " 0.44202209447308416,\n",
       " 0.5072363913639295,\n",
       " 0.7098483642900291,\n",
       " 0.7056068329901454,\n",
       " 0.3860510962506625,\n",
       " 0.7168727467232983,\n",
       " 0.8856480314285677,\n",
       " 0.922439551561905,\n",
       " 0.6356083896651753,\n",
       " 0.8092409111280565,\n",
       " 0.73701384577747,\n",
       " 0.5243140641716744,\n",
       " 0.4684399943881289,\n",
       " 0.6559493267573046,\n",
       " 0.6160029461920391,\n",
       " 0.36944446105988726,\n",
       " 0.6531209049880022,\n",
       " 0.7305752950309752,\n",
       " 0.8218107743110263,\n",
       " 0.7027209283763963,\n",
       " 0.6328328241349752,\n",
       " 0.8113374689459283,\n",
       " 0.6482986132544777,\n",
       " 0.6430180228832292,\n",
       " 0.663311566730724,\n",
       " 0.8997922637009712,\n",
       " 0.6678728206323272,\n",
       " 0.5001754589493302,\n",
       " 0.37362976918043544,\n",
       " 0.6416618900482564,\n",
       " 0.7779812700602187,\n",
       " 0.4065047552187754,\n",
       " 0.6370503991858942,\n",
       " 0.5412349027651842,\n",
       " 0.2597751032300598,\n",
       " 0.6758421071977596,\n",
       " 0.6106434561163521,\n",
       " 0.7109510913992828,\n",
       " 0.6958602882130478,\n",
       " 0.7111472841479938,\n",
       " 0.6169173052514578,\n",
       " 0.7784200465955029,\n",
       " 0.7639819480512504,\n",
       " 0.3702306746062864,\n",
       " 0.621665138798529,\n",
       " 0.8331228480037642,\n",
       " 0.6147061981124669,\n",
       " 0.8269064250527143,\n",
       " 0.6276456766108497,\n",
       " 0.6633369196405003,\n",
       " 0.2770498005151734,\n",
       " 0.7488799791480194,\n",
       " 0.662859539249665,\n",
       " 0.7769979787015122,\n",
       " 0.6844039521684631,\n",
       " 0.7250430561202948,\n",
       " 0.7101261813881754,\n",
       " 0.7247345114034079,\n",
       " 0.3891324789270121,\n",
       " 0.91955558240259,\n",
       " 0.36316620291238666,\n",
       " 0.8665007325212998,\n",
       " 0.6865427912539731,\n",
       " 0.8715067459366315,\n",
       " 0.7021703806548717,\n",
       " 0.8421176551571546,\n",
       " 0.6699842098607293,\n",
       " 0.7891971683577259,\n",
       " 0.45423642834708156,\n",
       " 0.8423693612486289,\n",
       " 0.6243498650920052,\n",
       " 0.7741576576188762,\n",
       " 0.7504281653253226,\n",
       " 0.7787718266728448,\n",
       " 0.8286861339629741,\n",
       " 0.77851059310436,\n",
       " 0.6159943903734093,\n",
       " 0.9174456769889937,\n",
       " 0.5554332217689497,\n",
       " 0.6941695243405285,\n",
       " 0.8476475565710855,\n",
       " 0.7357863546456357,\n",
       " 0.40789078889493124,\n",
       " 0.9244065361937843,\n",
       " 0.7486807530673941,\n",
       " 0.5269209622259268,\n",
       " 0.7422295338624807,\n",
       " 0.615569207514983,\n",
       " 0.7340071832912991,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict\n",
    "submit_score.to_csv('predict_result_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict_temp\n",
    "submit_score.to_csv('predict_result_ncnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(train_file,values = 'price', index=['lvl1','lvl2','lvl3'],columns=['type'],aggfunc=[min, max, np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
