{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chd415/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "sns.set(color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                   \n",
    "import nltk                                         \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer  \n",
    "LA = np.linalg\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "from gensim.models.word2vec import Word2Vec                                  \n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data text\n",
    "def load_data(filename):\n",
    "    load_file = pd.read_csv(filename,delimiter=',', header=0,\n",
    "                        dtype={'name':str, 'lvl1':str, 'lvl2':str, 'lvl3':str, 'descrption':str, 'type':str})\n",
    "    load_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "    load_file.duplicated(subset=None, keep='first')\n",
    "    load_file.set_index('id', inplace = True)\n",
    "    load_file.head()\n",
    "    return load_file\n",
    "#print(len(train_file))\n",
    "def load_label(filename):\n",
    "    load_label = pd.read_csv(filename,delimiter=',', header=0)\n",
    "    load_label.columns = ['id', 'score']\n",
    "    load_label.duplicated(subset=None, keep='first')\n",
    "    load_label.set_index('id', inplace = True)\n",
    "    return load_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def map_mathod(column):\n",
    "    values = []\n",
    "    indexs = []\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for count in range(len(train_file)):\n",
    "        value = train_file.get_value(count+1,column)\n",
    "        if value in values and value != np.nan:\n",
    "            continue\n",
    "        values.append(value)\n",
    "        indexs.append(len(values))\n",
    "    for j in range(len(indexs)):\n",
    "        mapping[values[j]] = indexs[j]\n",
    "    mapping[np.nan] = 0.0\n",
    "    return mapping\n",
    "#train_file['lvl3'] = train_file['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "#mapping_lvl3 = map_mathod('lvl3')\n",
    "#print(mapping_lvl3)\n",
    "'''\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=10000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(max_features=10000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=1,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "    \n",
    "    return avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "temp = load_data('train_data.csv')\n",
    "temp['descrption'] = temp['descrption'].str.lower()\n",
    "description_X = temp.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "def test_embedding(column):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=5000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english',max_features=5000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=5,size=50, workers=4)\n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(50, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "#        avg_data.append(vec)\n",
    "\n",
    "    tf_w_data = []\n",
    "    tf_data = tf_data.toarray()\n",
    "    i = 0\n",
    "    for row in splitted:\n",
    "        vec = [0 for i in range(50)]\n",
    "    \n",
    "        temp_tfidf = []\n",
    "        for val in tf_data[i]:\n",
    "            if val != 0:\n",
    "                temp_tfidf.append(val)\n",
    "    \n",
    "        count = 0\n",
    "        tf_idf_sum = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                count += 1\n",
    "                tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "                vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        if (tf_idf_sum == 0):\n",
    "            tf_w_data.append(vec)\n",
    "        else:\n",
    "            tf_w_data.append(vec/tf_idf_sum)\n",
    "#            vec = float(1/tf_idf_sum) * vec\n",
    "#        tf_w_data.append(vec)\n",
    "        i = i + 1\n",
    "    \n",
    "    return tf_w_data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "temp['descrption'] = test_embedding(description_X)\n",
    "temp.descrption.head()\n",
    "#tfidf_n = TfidfVectorizer(ngram_range=(1,2),stop_words = 'english')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "    filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].str.lower()\n",
    "    filename['name'] = filename['name'].str.lower()\n",
    "\n",
    "    '''\n",
    "    \n",
    "    mapping_lvl1 = map_mathod('lvl1')\n",
    "    mapping_lvl2 = map_mathod('lvl2')\n",
    "    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "    filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "    filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    '''\n",
    "    #clean up data for lvl 1&2&3\n",
    "    temp =  filename.drop(['price', 'descrption','name','type'], axis=1)\n",
    "    outfile = MultiColumnLabelEncoder(columns = ['lvl1','lvl2','lvl3']).fit_transform(temp.astype(str))\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(outfile)\n",
    "    outfile = enc.transform(outfile).toarray()\n",
    "    \n",
    "    #normalize price\n",
    "    maxp = filename.price.max()\n",
    "    valuethred = 1000.\n",
    "    filename['price'] = filename['price'].clip(lower=0.,upper=valuethred)\n",
    "    #filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=None)\n",
    "    #hist = train_file['price'].hist(bins=10)\n",
    "    #maxp\n",
    "    ld = filename.price.as_matrix(columns=None).tolist()\n",
    "    outfile = np.column_stack((outfile,ld))\n",
    "\n",
    "    #clean up type \n",
    "    mapping_type = {'international':1.,'local':2., np.nan:0.}\n",
    "    filename['type'] = filename['type'].map(mapping_type)\n",
    "    le = filename.type.as_matrix(columns=None).tolist()\n",
    "    outfile = np.column_stack((outfile,le))\n",
    "\n",
    "    #clean up text\n",
    "    description_X = filename.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    count_descrption = description_X.str.count('final').fillna(0).tolist()\n",
    "    outfile = np.column_stack((outfile,count_descrption))\n",
    "    \n",
    "    description_X = filename.descrption.str.lower().replace('final ','',regex=True)\n",
    "    filename['descrption'] = text_embedding(description_X)\n",
    " \n",
    "    lg = filename.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "    s_new = s[:8]\n",
    "    U_new = U[:, :8]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    lg_min = np.min(new_lg)\n",
    "    lg_out = new_lg  - (lg_min-2)*np.ones_like(new_lg.size)\n",
    "    outfile = np.column_stack((outfile,lg_out))\n",
    "    \n",
    "    \n",
    "    name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['name'] = text_embedding(name_X)\n",
    "    \n",
    "    lf = filename.name.as_matrix(columns=None).tolist()\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "    sf_new = sf[:8]\n",
    "    Uf_new = Uf[:, :8]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    lf_min = np.min(new_lf)\n",
    "    lf_out = new_lf  - (lf_min-2)*np.ones_like(new_lf.size)\n",
    "    outfile = np.column_stack((outfile,lf_out))\n",
    "   \n",
    "\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_file = pd.read_csv('train_data.csv',delimiter=',', header=0, nrows=10)\n",
    "train_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "description_X = train_file.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "count_descrption = description_X.str.count('final')\n",
    "description_X = train_file.descrption.str.lower().replace('final ','',regex=True)\n",
    "print(np.shape(count_descrption))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36283, 270)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = load_data('train_data.csv')\n",
    "test_file = load_data('test_data.csv')\n",
    "combined_file = pd.concat([train_file,test_file])\n",
    "cleaned_train = clean_data(combined_file)\n",
    "train_score = load_label('train_label.csv')\n",
    "np.shape(cleaned_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# test cell\n",
    "cleaned_train['descrption'] = temp['descrption']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ..., 10.13324131,\n",
       "        11.99554897, 11.35688418],\n",
       "       [ 0.        ,  1.        ,  0.        , ..., 11.9036486 ,\n",
       "         9.82575646, 10.58128699],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 11.73378475,\n",
       "        11.62101979, 11.51830028],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 12.33374908,\n",
       "        11.35996205, 11.5136934 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 11.17770531,\n",
       "        11.97945511, 11.61931454],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 10.76861269,\n",
       "        11.21852384, 11.04587675]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train\n",
    "#print(mapping_lvl1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36283, 270)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "def rearrange(cleaned_data):\n",
    "    la = cleaned_data.lvl1.as_matrix(columns=None).tolist()\n",
    "    lb = cleaned_data.lvl2.as_matrix(columns=None).tolist()\n",
    "    lc = cleaned_data.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = la\n",
    "    X = np.column_stack((X,lb))\n",
    "    X = np.column_stack((X,lc))\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    \n",
    "    ld = cleaned_data.price.as_matrix(columns=None).tolist()\n",
    "    le = cleaned_data.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = np.column_stack((X,ld))\n",
    "    X = np.column_stack((X,le))\n",
    "\n",
    "   \n",
    "    lf = cleaned_data.name.as_matrix(columns=None).tolist()\n",
    "    lg = cleaned_data.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "# only use U \\cdot s\n",
    "    \n",
    "#    s[8:] = 0.\n",
    "#    new_lg = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "    s_new = s[:5]\n",
    "    U_new = U[:, :5]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    lg_min = np.min(new_lg)\n",
    "    lg_out = new_lg  - (lg_min-2)*np.ones_like(new_lg.size)\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "#    sf[5:] = 0.\n",
    "#    new_lf = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "    sf_new = sf[:5]\n",
    "    Uf_new = Uf[:, :5]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    lf_min = np.min(new_lf)\n",
    "    lf_out = new_lf  - (lf_min-2)*np.ones_like(new_lf.size)\n",
    "\n",
    "    \n",
    "    X = np.column_stack((X,lf_out))\n",
    "    X = np.column_stack((X,lg_out))\n",
    "    X = X.tolist()\n",
    "   \n",
    "    return X,lf_min,lg_min\n",
    "'''\n",
    "    \n",
    "    #print(len(X))\n",
    "X = cleaned_train#rearrange(cleaned_train)\n",
    "print(np.shape(np.array(X)))\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "X[137,253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18141\n",
      "(18141, 270)\n",
      "(18142, 270)\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_train[:18141]\n",
    "XX = cleaned_train[18141:]\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "print(np.size(Y))\n",
    "print(np.shape(X))\n",
    "print(np.shape(XX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "maxlen = 270\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, dtype='float32')\n",
    "#X_train = np.any(np.isnan(X_train))\n",
    "#X_train = np.all(np.isfinite(X_train))\n",
    "print(X_train[1400].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Classifier\n",
    "def linear_regression_classifier(train_x, train_y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    param_grid = {'alpha': [math.pow(10,-i) for i in range(11)]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = MultinomialNB(alpha = best_parameters['alpha'])  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    param_grid = {'n_neighbors': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = best_parameters['n_neighbors'], algorithm='kd_tree')\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(100,300,10))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    #for para, val in best_parameters.items():\n",
    "        #print para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "def feature_select(x,y):\n",
    "    clf = ExtraTreesClassifier()\n",
    "    clf = clf.fit(x, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "******************* KNN ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 16.900737s!\n",
      "precision: 69.16%, recall: 100.00%\n",
      "accuracy: 69.17%\n",
      "[0.6920215  0.69588588 0.69051065]\n",
      "******************* LR ********************\n",
      "training took 0.241711s!\n",
      "precision: 77.36%, recall: 90.43%\n",
      "accuracy: 75.09%\n",
      "[0.72219926 0.7366136  0.73433947]\n",
      "******************* RF ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   19.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 21.534177s!\n",
      "precision: 79.11%, recall: 89.36%\n",
      "accuracy: 76.33%\n",
      "[0.73770153 0.7506719  0.74260906]\n",
      "******************* DT ********************\n",
      "training took 0.650436s!\n",
      "precision: 71.19%, recall: 92.07%\n",
      "accuracy: 68.75%\n",
      "[0.68561389 0.66694232 0.68658259]\n",
      "******************* GBC ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 410.052062s!\n",
      "precision: 79.57%, recall: 90.35%\n",
      "accuracy: 77.29%\n",
      "[0.75010335 0.76431673 0.76121563]\n"
     ]
    }
   ],
   "source": [
    "# just for my own record\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['KNN','LR','RF','DT','GBC']    \n",
    "    classifiers = {\n",
    "                   'KNN':knn_classifier,\n",
    "                    'LR':logistic_regression_classifier,\n",
    "                    'RF':random_forest_classifier,\n",
    "                    'DT':decision_tree_classifier,\n",
    "                    'GBC':gradient_boosting_classifier\n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_validation = select_model.transform(X_validation)\n",
    "\n",
    "    result = []\n",
    "        \n",
    "    for classifier in test_classifiers:    \n",
    "        print('******************* %s ********************' % classifier)    \n",
    "        start_time = time.time()    \n",
    "        model = classifiers[classifier](X_train, y_train)   \n",
    "        print('training took %fs!' % (time.time() - start_time))    \n",
    "        predict = model.predict(X_validation)\n",
    "\n",
    "        precision = metrics.precision_score(y_validation, predict)    \n",
    "        recall = metrics.recall_score(y_validation, predict)    \n",
    "        print('precision: %.2f%%, recall: %.2f%%' % (100 * precision, 100 * recall))    \n",
    "        accuracy = metrics.accuracy_score(y_validation, predict)    \n",
    "        print('accuracy: %.2f%%' % (100 * accuracy))\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  1.      ,  0.      , ..., 11.774629, 11.737314,\n",
       "        11.739768],\n",
       "       [ 0.      ,  0.      ,  0.      , ..., 11.737541, 12.21775 ,\n",
       "        11.256157],\n",
       "       [ 1.      ,  0.      ,  0.      , ..., 11.562332, 11.451252,\n",
       "        11.531164],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  1.      , ..., 11.248144, 11.919338,\n",
       "        12.642514],\n",
       "       [ 0.      ,  1.      ,  0.      , ..., 11.222754, 11.798913,\n",
       "        11.439027],\n",
       "       [ 0.      ,  1.      ,  0.      , ..., 11.568027, 11.701103,\n",
       "        11.861883]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "#X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "14512 train sequences\n",
      "3629 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (14512, 270)\n",
      "x_test shape: (3629, 270)\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#test cnn model\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Embedding\n",
    "max_features = 15000\n",
    "maxlen = 270\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 5\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "#X_train, X_validation, y_train, y_validation\n",
    "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "#X_train = np.asarray(np.abs(X))\n",
    "X_train = np.asarray(np.abs(X_train))\n",
    "X_validation = np.asarray(np.abs(X_validation))\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_validation), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, padding='post')\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_validation.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(max_features, embedding_size,input_length=maxlen))\n",
    "#model.add(Dense(32, activation='relu', input_dim=100))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "cnnmodel.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=pool_size))\n",
    "cnnmodel.add(LSTM(lstm_output_size))\n",
    "cnnmodel.add(Dense(1))\n",
    "cnnmodel.add(Activation('sigmoid'))\n",
    "\n",
    "cnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "training took 0.241209s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   25.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 27.739618s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  8.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 546.078951s!\n",
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "#    model_save_file = \"/home/jason/datamining/model/models\"     \n",
    "#    model_save = {}\n",
    "#    result_save_file = '/home/jason/datamining/result/results' \n",
    "     \n",
    "    test_classifiers = ['LR','RF','GBC']    \n",
    "    classifiers = {\n",
    "                   'LR':logistic_regression_classifier,\n",
    "                   'RF':random_forest_classifier,\n",
    "                   'GBC':gradient_boosting_classifier\n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "    #X_train, X_validation, y_train, y_validation\n",
    "#    X_test = rearrange(Xt)\n",
    "#    X_test = Xt\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_test = select_model.transform(X_test)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = classifiers['LR'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_lr = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['RF'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_rf = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['GBC'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_gbc = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "        \n",
    "#    for classifier in test_classifiers:    \n",
    "#        print('******************* %s ********************' % classifier)    \n",
    "#        start_time = time.time()    \n",
    "#        model = classifiers[classifier](X_train, y_train)   \n",
    "#        print('training took %fs!' % (time.time() - start_time))    \n",
    "#        Y_predict = model.predict_proba(X_test)[:,1]\n",
    "#        print('predict finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18141/18141 [==============================] - 60s 3ms/step - loss: 0.5846 - acc: 0.6992\n",
      "Epoch 2/5\n",
      "18141/18141 [==============================] - 59s 3ms/step - loss: 0.5381 - acc: 0.7351\n",
      "Epoch 3/5\n",
      "18141/18141 [==============================] - 59s 3ms/step - loss: 0.5243 - acc: 0.7417\n",
      "Epoch 4/5\n",
      "18141/18141 [==============================] - 62s 3ms/step - loss: 0.5139 - acc: 0.7487\n",
      "Epoch 5/5\n",
      "18141/18141 [==============================] - 65s 4ms/step - loss: 0.5042 - acc: 0.7539\n"
     ]
    }
   ],
   "source": [
    "#cnn test\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "#X_test = np.asarray(Xt)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "cnnmodel.fit(X_train, y_train, batch_size=128, epochs=5)\n",
    "\n",
    "Y_predict_cnn = cnnmodel.predict(X_test, verbose=0)\n",
    "Y_predict_cnn = np.squeeze(Y_predict_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "18141/18141 [==============================] - 1s 59us/step - loss: 0.6737 - acc: 0.6873\n",
      "Epoch 2/25\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.5794 - acc: 0.7171\n",
      "Epoch 3/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5597 - acc: 0.7253\n",
      "Epoch 4/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5516 - acc: 0.7259\n",
      "Epoch 5/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5565 - acc: 0.7264\n",
      "Epoch 6/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5491 - acc: 0.7297\n",
      "Epoch 7/25\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.5310 - acc: 0.7361\n",
      "Epoch 8/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5338 - acc: 0.7349\n",
      "Epoch 9/25\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.5247 - acc: 0.7377\n",
      "Epoch 10/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5242 - acc: 0.7415\n",
      "Epoch 11/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5275 - acc: 0.7386\n",
      "Epoch 12/25\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.5212 - acc: 0.7438\n",
      "Epoch 13/25\n",
      "18141/18141 [==============================] - 0s 24us/step - loss: 0.5219 - acc: 0.7452\n",
      "Epoch 14/25\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.5212 - acc: 0.7422\n",
      "Epoch 15/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5218 - acc: 0.7409\n",
      "Epoch 16/25\n",
      "18141/18141 [==============================] - 0s 22us/step - loss: 0.5178 - acc: 0.7452\n",
      "Epoch 17/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5130 - acc: 0.7465\n",
      "Epoch 18/25\n",
      "18141/18141 [==============================] - 1s 28us/step - loss: 0.5088 - acc: 0.7495\n",
      "Epoch 19/25\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.5174 - acc: 0.7449\n",
      "Epoch 20/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5054 - acc: 0.7480\n",
      "Epoch 21/25\n",
      "18141/18141 [==============================] - 0s 23us/step - loss: 0.5094 - acc: 0.7462\n",
      "Epoch 22/25\n",
      "18141/18141 [==============================] - 0s 24us/step - loss: 0.5045 - acc: 0.7511\n",
      "Epoch 23/25\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.4998 - acc: 0.7534\n",
      "Epoch 24/25\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.5001 - acc: 0.7516\n",
      "Epoch 25/25\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.4994 - acc: 0.7551\n",
      "18141/18141 [==============================] - 1s 28us/step\n",
      "\n",
      "acc: 75.64%\n"
     ]
    }
   ],
   "source": [
    "#nn train\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_array = np.asarray(X_train)\n",
    "Y_array = np.asarray(y_train)\n",
    "Xtest_array = np.asarray(X_test) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_array\n",
    "Y = Y_array\n",
    "# create model\n",
    "nnmodel = Sequential()\n",
    "nnmodel.add(Dense(100, input_dim=270, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "nnmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "nnmodel.fit(X, Y, epochs=25, batch_size=150)\n",
    "# evaluate the model\n",
    "scores = nnmodel.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (nnmodel.metrics_names[1], scores[1]*100))\n",
    "Y_predict_nn = nnmodel.predict(Xtest_array, verbose=0)\n",
    "Y_predict_nn = np.squeeze(Y_predict_nn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8579925 , 0.94041866, 0.8313853 , ..., 0.70595163, 0.50523454,\n",
       "       0.61891323], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8295582 , 0.78621113, 0.6645141 , ..., 0.64757854, 0.41389206,\n",
       "       0.569912  ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73480094, 0.67819865, 0.68934352, ..., 0.62148617, 0.41585488,\n",
       "       0.49107215])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77777778, 0.83333333, 0.61111111, ..., 0.66666667, 0.27777778,\n",
       "       0.72222222])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68636364, 0.79545455, 0.71818182, ..., 0.68636364, 0.52272727,\n",
       "       0.58636364])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict.append(( Y_predict_rf[i] + Y_predict_gbc[i] + Y_predict_cnn[i] + Y_predict_nn[i]) / 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7879230195223683,\n",
       " 0.8388544181079576,\n",
       " 0.7062980920377404,\n",
       " 0.943190951750736,\n",
       " 0.768477741034344,\n",
       " 0.7789613797508105,\n",
       " 0.49860520630773875,\n",
       " 0.7843875995790115,\n",
       " 0.7212234910690423,\n",
       " 0.7845466396122267,\n",
       " 0.8631969074709247,\n",
       " 0.3807709932026237,\n",
       " 0.6003489641830174,\n",
       " 0.40475087120677483,\n",
       " 0.8834678248323575,\n",
       " 0.2311255061265194,\n",
       " 0.873183729010399,\n",
       " 0.6564867117489227,\n",
       " 0.8245094442307347,\n",
       " 0.5769025559527705,\n",
       " 0.9085451350970701,\n",
       " 0.8365589516632485,\n",
       " 0.8573541283005416,\n",
       " 0.8478584218506862,\n",
       " 0.9570992681113156,\n",
       " 0.6973043681997242,\n",
       " 0.7669518869633627,\n",
       " 0.9678706323558635,\n",
       " 0.8102602714239949,\n",
       " 0.9404498155670937,\n",
       " 0.8150021161394889,\n",
       " 0.7075678125926943,\n",
       " 0.8056072395558309,\n",
       " 0.6466429714602653,\n",
       " 0.720919211645319,\n",
       " 0.45648324354128406,\n",
       " 0.6798729751748268,\n",
       " 0.9523854328827425,\n",
       " 0.5096632492963714,\n",
       " 0.8012050654852029,\n",
       " 0.5585386865367793,\n",
       " 0.8966844571961297,\n",
       " 0.9577842205762863,\n",
       " 0.6449160896166406,\n",
       " 0.5598847478327125,\n",
       " 0.8654995611821762,\n",
       " 0.7667021310690678,\n",
       " 0.31053749369551437,\n",
       " 0.9412690380606988,\n",
       " 0.6922423969314556,\n",
       " 0.6682960966018715,\n",
       " 0.8878353593927442,\n",
       " 0.7403141237569577,\n",
       " 0.5666097553390446,\n",
       " 0.34242223821806184,\n",
       " 0.5745137103579261,\n",
       " 0.5056084916597665,\n",
       " 0.8443176256887841,\n",
       " 0.31077154433034887,\n",
       " 0.652228018490955,\n",
       " 0.7001406736747183,\n",
       " 0.5219168139377026,\n",
       " 0.40930632921782406,\n",
       " 0.656459810562206,\n",
       " 0.3910168477983186,\n",
       " 0.9005869375936912,\n",
       " 0.6207800335956342,\n",
       " 0.8155419999300831,\n",
       " 0.7215054362410247,\n",
       " 0.674287120471097,\n",
       " 0.4946316438792932,\n",
       " 0.44189750848695486,\n",
       " 0.38850753680624145,\n",
       " 0.670125709790172,\n",
       " 0.8459431837604503,\n",
       " 0.456926736356032,\n",
       " 0.649020043858374,\n",
       " 0.6364261954119711,\n",
       " 0.35317333382789534,\n",
       " 0.49661802042915365,\n",
       " 0.5756961399557614,\n",
       " 0.3289054932768899,\n",
       " 0.2922133699661553,\n",
       " 0.5969380459099105,\n",
       " 0.6982537371642662,\n",
       " 0.6672041084880781,\n",
       " 0.9109541324051944,\n",
       " 0.38655630711353184,\n",
       " 0.9205146239562468,\n",
       " 0.7611620288605642,\n",
       " 0.847835368550185,\n",
       " 0.7250494646002548,\n",
       " 0.6386729278347709,\n",
       " 0.8814101485591946,\n",
       " 0.9426997240143593,\n",
       " 0.8593327081865735,\n",
       " 0.8854973321009164,\n",
       " 0.761613302670344,\n",
       " 0.8395708713567618,\n",
       " 0.7394665759320211,\n",
       " 0.8789345564866307,\n",
       " 0.9001224914584497,\n",
       " 0.7608284189243509,\n",
       " 0.890749600379154,\n",
       " 0.8078777135020554,\n",
       " 0.8980867635120044,\n",
       " 0.6896643446852462,\n",
       " 0.9010148370506788,\n",
       " 0.5399896084359198,\n",
       " 0.9436106944626028,\n",
       " 0.7173233939240677,\n",
       " 0.5830297254853778,\n",
       " 0.910392844135111,\n",
       " 0.840965419947499,\n",
       " 0.7943909067698199,\n",
       " 0.6443230259900141,\n",
       " 0.2854903297592895,\n",
       " 0.7789211454114529,\n",
       " 0.5589566548063297,\n",
       " 0.47925539472789475,\n",
       " 0.7434961258461981,\n",
       " 0.8142665823601714,\n",
       " 0.6737935066524179,\n",
       " 0.32949335013676173,\n",
       " 0.7789133901848937,\n",
       " 0.8469757748974693,\n",
       " 0.8763380272520913,\n",
       " 0.5939704627099663,\n",
       " 0.534669560449894,\n",
       " 0.8890788649970836,\n",
       " 0.9061540941999416,\n",
       " 0.8587573018639979,\n",
       " 0.6894106894129455,\n",
       " 0.42708793359272407,\n",
       " 0.9092364098387535,\n",
       " 0.7932932148979168,\n",
       " 0.9141534354048546,\n",
       " 0.8778116136488288,\n",
       " 0.31902512615979317,\n",
       " 0.5606782776689289,\n",
       " 0.799687826181903,\n",
       " 0.5312301490945046,\n",
       " 0.8372125069300334,\n",
       " 0.6845006552910564,\n",
       " 0.7081792589389917,\n",
       " 0.42824005556076467,\n",
       " 0.8511270585084202,\n",
       " 0.8249551722196617,\n",
       " 0.8076245030068387,\n",
       " 0.9575832835652611,\n",
       " 0.8483461627755502,\n",
       " 0.6348126131928329,\n",
       " 0.4342086132728692,\n",
       " 0.910660274432163,\n",
       " 0.7441339467812066,\n",
       " 0.5487191449813169,\n",
       " 0.9428483106873252,\n",
       " 0.6458503601828007,\n",
       " 0.6478715577510872,\n",
       " 0.6748445736037361,\n",
       " 0.6227878210821537,\n",
       " 0.47721030353897753,\n",
       " 0.38173084201836827,\n",
       " 0.5624694144936523,\n",
       " 0.4064605542204597,\n",
       " 0.42283884106260355,\n",
       " 0.7142826438552201,\n",
       " 0.25735870097773245,\n",
       " 0.300363426979142,\n",
       " 0.8148475043701403,\n",
       " 0.7483688134737689,\n",
       " 0.18327714607420592,\n",
       " 0.187424812940034,\n",
       " 0.5018928777238335,\n",
       " 0.8774436646940732,\n",
       " 0.7461095380060601,\n",
       " 0.5821547227074402,\n",
       " 0.7574544534237698,\n",
       " 0.9182699016248337,\n",
       " 0.4044226799348388,\n",
       " 0.9030570583813118,\n",
       " 0.9534332047809254,\n",
       " 0.6053482233273862,\n",
       " 0.8338723847059288,\n",
       " 0.6746717732663107,\n",
       " 0.4710536087101156,\n",
       " 0.7165282924668958,\n",
       " 0.5253831539792244,\n",
       " 0.5434089610672961,\n",
       " 0.5374659979584241,\n",
       " 0.4923096076707647,\n",
       " 0.9242050391556036,\n",
       " 0.7847563390779977,\n",
       " 0.9297017475270262,\n",
       " 0.24365215433968437,\n",
       " 0.8239973632675228,\n",
       " 0.8743608000001521,\n",
       " 0.7637416302254706,\n",
       " 0.8781491487917273,\n",
       " 0.5730877148834141,\n",
       " 0.4451484339857342,\n",
       " 0.8534584199238305,\n",
       " 0.8504844195312924,\n",
       " 0.9633799940347672,\n",
       " 0.8727364252613048,\n",
       " 0.5896017647752858,\n",
       " 0.7880152231515056,\n",
       " 0.7866893572638733,\n",
       " 0.6372686437585138,\n",
       " 0.557448315635474,\n",
       " 0.5232809077308636,\n",
       " 0.6768979428392468,\n",
       " 0.41333027170463044,\n",
       " 0.6733820088885047,\n",
       " 0.3901863222471391,\n",
       " 0.37588769712231374,\n",
       " 0.6202792083073144,\n",
       " 0.6115061372819574,\n",
       " 0.9277016359146195,\n",
       " 0.7874259028470878,\n",
       " 0.5903055927970193,\n",
       " 0.674875012612102,\n",
       " 0.5501333484745989,\n",
       " 0.8095936993757884,\n",
       " 0.7760574921514048,\n",
       " 0.8920446934724094,\n",
       " 0.7165780002420599,\n",
       " 0.7187719467613432,\n",
       " 0.8814212735253151,\n",
       " 0.5807313810845818,\n",
       " 0.3672774839130315,\n",
       " 0.3771320255717846,\n",
       " 0.7178939505056902,\n",
       " 0.23993113250518688,\n",
       " 0.8084332549511785,\n",
       " 0.6688980896665593,\n",
       " 0.5472185995843676,\n",
       " 0.4059818581649751,\n",
       " 0.4196123341117242,\n",
       " 0.48654953126955514,\n",
       " 0.48960989256097814,\n",
       " 0.8181980276348615,\n",
       " 0.6550325908444145,\n",
       " 0.4042796959178616,\n",
       " 0.26801735141662636,\n",
       " 0.32224028688864875,\n",
       " 0.792819875689468,\n",
       " 0.5036875356929471,\n",
       " 0.41166821430728895,\n",
       " 0.9129034184446239,\n",
       " 0.6911988183103427,\n",
       " 0.682110450574846,\n",
       " 0.8094428915267039,\n",
       " 0.7789399079903208,\n",
       " 0.8764303402166174,\n",
       " 0.8697151045907627,\n",
       " 0.8709752200227795,\n",
       " 0.8973718220236325,\n",
       " 0.8913357615470886,\n",
       " 0.7886683804519249,\n",
       " 0.9222270407459953,\n",
       " 0.3868860996281258,\n",
       " 0.8841624048322139,\n",
       " 0.93529912693934,\n",
       " 0.7465166455567485,\n",
       " 0.42523248632146854,\n",
       " 0.9372259506974557,\n",
       " 0.9483359392243202,\n",
       " 0.8820688567077271,\n",
       " 0.7292473619935489,\n",
       " 0.7551258529376502,\n",
       " 0.6753964922644875,\n",
       " 0.866395427933847,\n",
       " 0.9497094652869484,\n",
       " 0.7964538020919067,\n",
       " 0.9300987685268576,\n",
       " 0.9003800843099151,\n",
       " 0.4278203835391035,\n",
       " 0.9267455072414995,\n",
       " 0.47436560932735,\n",
       " 0.8086494818781361,\n",
       " 0.7873489572845325,\n",
       " 0.8615791296116029,\n",
       " 0.7466026792923609,\n",
       " 0.689918143219418,\n",
       " 0.6607753328602723,\n",
       " 0.4456969882653217,\n",
       " 0.749690659268938,\n",
       " 0.7708888434099429,\n",
       " 0.734322137934993,\n",
       " 0.5580578808230583,\n",
       " 0.7534403522207279,\n",
       " 0.5241464212837845,\n",
       " 0.8705191810022701,\n",
       " 0.42413503350031495,\n",
       " 0.35287840304651646,\n",
       " 0.6580460749641813,\n",
       " 0.7663515042777014,\n",
       " 0.6625641684339504,\n",
       " 0.5929596339211319,\n",
       " 0.32459908910471985,\n",
       " 0.889376096924146,\n",
       " 0.9109267601762155,\n",
       " 0.9192031358829652,\n",
       " 0.8022899895304381,\n",
       " 0.9361279361476802,\n",
       " 0.7880824397007624,\n",
       " 0.5849414509053182,\n",
       " 0.8972358349898849,\n",
       " 0.8594265683130784,\n",
       " 0.8043501635091473,\n",
       " 0.7545945063684926,\n",
       " 0.9132316573099657,\n",
       " 0.9459113776683807,\n",
       " 0.4982894633454506,\n",
       " 0.9029795646366446,\n",
       " 0.8340057898350436,\n",
       " 0.44305764192884617,\n",
       " 0.8350959051137019,\n",
       " 0.8564420763591323,\n",
       " 0.8746259006285908,\n",
       " 0.7906884379760184,\n",
       " 0.8366959465874566,\n",
       " 0.5337753916780154,\n",
       " 0.8058989164203104,\n",
       " 0.6716750606442943,\n",
       " 0.809103346743969,\n",
       " 0.8884953575904924,\n",
       " 0.5002396085045555,\n",
       " 0.7149070091319807,\n",
       " 0.4350913967448051,\n",
       " 0.8789101107554003,\n",
       " 0.46671295701855364,\n",
       " 0.597493737183436,\n",
       " 0.7157944269252545,\n",
       " 0.30779319616279216,\n",
       " 0.31967834889738245,\n",
       " 0.3729706623036452,\n",
       " 0.32380542139513324,\n",
       " 0.4057966457924458,\n",
       " 0.4992691194168245,\n",
       " 0.47575669637834184,\n",
       " 0.5767367167906328,\n",
       " 0.2931530572699778,\n",
       " 0.8955279203677418,\n",
       " 0.384741752986053,\n",
       " 0.855808419621352,\n",
       " 0.42032031480110055,\n",
       " 0.9209269355643879,\n",
       " 0.6788103330616999,\n",
       " 0.47227998574574787,\n",
       " 0.4001841742884029,\n",
       " 0.5892985716462136,\n",
       " 0.9187423077195582,\n",
       " 0.6371719511169376,\n",
       " 0.8178846693099147,\n",
       " 0.7647785050399376,\n",
       " 0.8042988872889316,\n",
       " 0.757292119661967,\n",
       " 0.888607545573302,\n",
       " 0.8474724759658178,\n",
       " 0.9578534979711879,\n",
       " 0.4961389179783638,\n",
       " 0.7548413442240821,\n",
       " 0.4101055415892842,\n",
       " 0.6277019953486895,\n",
       " 0.5365940389157545,\n",
       " 0.3704177319551959,\n",
       " 0.5888153023792035,\n",
       " 0.5242236738554156,\n",
       " 0.9496873981422849,\n",
       " 0.8443300134605831,\n",
       " 0.6415797694463923,\n",
       " 0.9593277966434306,\n",
       " 0.6091647285403627,\n",
       " 0.8655457016795572,\n",
       " 0.7507855719689167,\n",
       " 0.7546051210225231,\n",
       " 0.7758263726728131,\n",
       " 0.3751059223907163,\n",
       " 0.3423158787416689,\n",
       " 0.8653505594441385,\n",
       " 0.7227092297992321,\n",
       " 0.43182052370875773,\n",
       " 0.8085773186852234,\n",
       " 0.6896097604373489,\n",
       " 0.6933330390188429,\n",
       " 0.6969068336817953,\n",
       " 0.3701540231855229,\n",
       " 0.9184647465294058,\n",
       " 0.13544006724700783,\n",
       " 0.9093469715479648,\n",
       " 0.5321388243424772,\n",
       " 0.4576946802964114,\n",
       " 0.727243339142414,\n",
       " 0.4438348039232119,\n",
       " 0.9637766320597042,\n",
       " 0.9689246118068695,\n",
       " 0.8169049958990078,\n",
       " 0.919564668668641,\n",
       " 0.20554686580794027,\n",
       " 0.6648080778242361,\n",
       " 0.5578568451633357,\n",
       " 0.6442159159918024,\n",
       " 0.4795824647551835,\n",
       " 0.743976118980032,\n",
       " 0.464428401655621,\n",
       " 0.32222290223445554,\n",
       " 0.7983155416719842,\n",
       " 0.7495141196732569,\n",
       " 0.49164667039206533,\n",
       " 0.8083746325789076,\n",
       " 0.7292885642762136,\n",
       " 0.6584220804048306,\n",
       " 0.3972718074917793,\n",
       " 0.1028292984447696,\n",
       " 0.05326840234073726,\n",
       " 0.7955834566944777,\n",
       " 0.7289211059760565,\n",
       " 0.7083548713212061,\n",
       " 0.8309462668919804,\n",
       " 0.8010857508339063,\n",
       " 0.9074121671493607,\n",
       " 0.7328024386757552,\n",
       " 0.7808678450608494,\n",
       " 0.6055613092400811,\n",
       " 0.8571539946276732,\n",
       " 0.4696546486832879,\n",
       " 0.28327577216456634,\n",
       " 0.9422403510170754,\n",
       " 0.7699714381586422,\n",
       " 0.8715417295092285,\n",
       " 0.9366658614440397,\n",
       " 0.9321278364670397,\n",
       " 0.6327915799437147,\n",
       " 0.9014507049863989,\n",
       " 0.6959251076585115,\n",
       " 0.595489101202199,\n",
       " 0.4356335046766984,\n",
       " 0.8150362147225274,\n",
       " 0.9268806893717159,\n",
       " 0.3257427946636171,\n",
       " 0.8197516848944655,\n",
       " 0.7197908465609406,\n",
       " 0.10365140590478074,\n",
       " 0.8157847303633738,\n",
       " 0.5447167629545385,\n",
       " 0.558878982217625,\n",
       " 0.9167099753413538,\n",
       " 0.7278378998089319,\n",
       " 0.7693455044067268,\n",
       " 0.3853273902427067,\n",
       " 0.734401288688785,\n",
       " 0.7713853182816746,\n",
       " 0.8355310353064778,\n",
       " 0.9070854442890244,\n",
       " 0.8348229946813198,\n",
       " 0.6997885297043155,\n",
       " 0.8324414808340747,\n",
       " 0.6628010556252316,\n",
       " 0.3103325757339145,\n",
       " 0.7936818902841722,\n",
       " 0.8793618994529802,\n",
       " 0.4081586702152936,\n",
       " 0.7434379329284032,\n",
       " 0.20183814443422085,\n",
       " 0.6864081867415496,\n",
       " 0.9084792191031004,\n",
       " 0.6237328369659607,\n",
       " 0.7192723036414446,\n",
       " 0.5657037662285747,\n",
       " 0.5498628740960901,\n",
       " 0.7007513774765862,\n",
       " 0.6705296686502418,\n",
       " 0.8905225707425011,\n",
       " 0.8143293269354888,\n",
       " 0.46385399267227967,\n",
       " 0.8660266120024401,\n",
       " 0.9071223375171122,\n",
       " 0.9121611044864462,\n",
       " 0.7801007724169529,\n",
       " 0.8090312107945934,\n",
       " 0.9398656336948125,\n",
       " 0.9264702871291324,\n",
       " 0.7470003828857884,\n",
       " 0.849475904305776,\n",
       " 0.8143595014858728,\n",
       " 0.9029330836101012,\n",
       " 0.9118548187041523,\n",
       " 0.8930209364252861,\n",
       " 0.8769379066698479,\n",
       " 0.8485732344966946,\n",
       " 0.4375465753102543,\n",
       " 0.8197648066763926,\n",
       " 0.9108869037844918,\n",
       " 0.6449862662288878,\n",
       " 0.9186910711755656,\n",
       " 0.658431378849829,\n",
       " 0.7887469777254144,\n",
       " 0.7775346564223068,\n",
       " 0.35678259522625894,\n",
       " 0.42267902166855453,\n",
       " 0.83986246649063,\n",
       " 0.7070821835838184,\n",
       " 0.894167574126311,\n",
       " 0.5859400462622595,\n",
       " 0.8905502291941884,\n",
       " 0.33554185865202335,\n",
       " 0.37886788098499025,\n",
       " 0.3440730490016215,\n",
       " 0.729179378621506,\n",
       " 0.6898022575510873,\n",
       " 0.2878511454345602,\n",
       " 0.7443106286453478,\n",
       " 0.2915862341571336,\n",
       " 0.8045872831585431,\n",
       " 0.35082990915486306,\n",
       " 0.7108257103146929,\n",
       " 0.6259022390902644,\n",
       " 0.4495347396291868,\n",
       " 0.741992192828294,\n",
       " 0.6463522977901227,\n",
       " 0.5807271829759232,\n",
       " 0.6020084450642268,\n",
       " 0.7261942840886838,\n",
       " 0.5173846330004509,\n",
       " 0.36835122415513705,\n",
       " 0.7860754937535585,\n",
       " 0.7891653306857505,\n",
       " 0.6723196595004111,\n",
       " 0.45558334614592366,\n",
       " 0.9599750445647673,\n",
       " 0.5629204741030028,\n",
       " 0.9503787873670309,\n",
       " 0.859867684768908,\n",
       " 0.8321789311640191,\n",
       " 0.9202518269269153,\n",
       " 0.8649194291745773,\n",
       " 0.7552853695069901,\n",
       " 0.7954299054061523,\n",
       " 0.8355315815017681,\n",
       " 0.9473099441540361,\n",
       " 0.7726764951992516,\n",
       " 0.8898822652872163,\n",
       " 0.6314235114388995,\n",
       " 0.3838155296715823,\n",
       " 0.5239775431276572,\n",
       " 0.8985324497776802,\n",
       " 0.8374498656903855,\n",
       " 0.7955200344926179,\n",
       " 0.7532314709039649,\n",
       " 0.8690880518971067,\n",
       " 0.8622094330161509,\n",
       " 0.8181074752952113,\n",
       " 0.8112900867305621,\n",
       " 0.8614388316568702,\n",
       " 0.38136518137021497,\n",
       " 0.48709615276317403,\n",
       " 0.5635666191878945,\n",
       " 0.42303621169894634,\n",
       " 0.6369227657715479,\n",
       " 0.6419386001247348,\n",
       " 0.24425826901287745,\n",
       " 0.7132078319185913,\n",
       " 0.8307586353836638,\n",
       " 0.7492448065317039,\n",
       " 0.6531701219503325,\n",
       " 0.6608176450386192,\n",
       " 0.49004978992120185,\n",
       " 0.5269612772597208,\n",
       " 0.5462466916050573,\n",
       " 0.9082844938593682,\n",
       " 0.6020829531731028,\n",
       " 0.7255329367488321,\n",
       " 0.7802642231035715,\n",
       " 0.7156682534952357,\n",
       " 0.6562530000251954,\n",
       " 0.9009111145530084,\n",
       " 0.9601631183515895,\n",
       " 0.5610611627047712,\n",
       " 0.9158598087050698,\n",
       " 0.41309251372862343,\n",
       " 0.4526897855930858,\n",
       " 0.4782074906007208,\n",
       " 0.43593470603227613,\n",
       " 0.6557854638256208,\n",
       " 0.30235017770619105,\n",
       " 0.32731815435067574,\n",
       " 0.38970013107314255,\n",
       " 0.6547851233169286,\n",
       " 0.5663105560974642,\n",
       " 0.26096761526182444,\n",
       " 0.6606358373706991,\n",
       " 0.8017281283934912,\n",
       " 0.47105137823206006,\n",
       " 0.5252332033232006,\n",
       " 0.3625335262128801,\n",
       " 0.47551747136043776,\n",
       " 0.45711930774980125,\n",
       " 0.05902095386250453,\n",
       " 0.787687869866689,\n",
       " 0.3052899274485882,\n",
       " 0.7819288205618811,\n",
       " 0.38294206514503015,\n",
       " 0.8429615298003862,\n",
       " 0.7520125616078425,\n",
       " 0.7890625485868166,\n",
       " 0.5200500780584836,\n",
       " 0.23488469391007616,\n",
       " 0.8324995105314736,\n",
       " 0.8896085299927778,\n",
       " 0.858211874118959,\n",
       " 0.8270981555033212,\n",
       " 0.4496875521811572,\n",
       " 0.9211600911737692,\n",
       " 0.8220193033567582,\n",
       " 0.78929760194186,\n",
       " 0.5177481925999275,\n",
       " 0.7844253030389247,\n",
       " 0.8484801931212647,\n",
       " 0.8001171421824079,\n",
       " 0.41480516333772677,\n",
       " 0.8618271171745627,\n",
       " 0.854580536454615,\n",
       " 0.8593842302910005,\n",
       " 0.8247756405310197,\n",
       " 0.9546056005087766,\n",
       " 0.8031388124131194,\n",
       " 0.9424343022433195,\n",
       " 0.8511320826080111,\n",
       " 0.827624569818227,\n",
       " 0.7866125786244267,\n",
       " 0.9148824332639425,\n",
       " 0.7902558701809006,\n",
       " 0.6598129751104297,\n",
       " 0.7823420165163097,\n",
       " 0.3218209906659945,\n",
       " 0.3871970508134726,\n",
       " 0.730774763045889,\n",
       " 0.5063675195129231,\n",
       " 0.7091709606575244,\n",
       " 0.8933122584615091,\n",
       " 0.8754273354706138,\n",
       " 0.31748977554867963,\n",
       " 0.7404434863064024,\n",
       " 0.5334188088624164,\n",
       " 0.564526966771092,\n",
       " 0.7394960466358397,\n",
       " 0.326452602551441,\n",
       " 0.7236637247632248,\n",
       " 0.5851711707283752,\n",
       " 0.6229759257550191,\n",
       " 0.8900834606452421,\n",
       " 0.9314379628258522,\n",
       " 0.8714868636143328,\n",
       " 0.6038995921611786,\n",
       " 0.8013794960999729,\n",
       " 0.7845880152902218,\n",
       " 0.40001558647914365,\n",
       " 0.5621602526367313,\n",
       " 0.8281177141148635,\n",
       " 0.43678953333033455,\n",
       " 0.7717404727983956,\n",
       " 0.3562752952328836,\n",
       " 0.4038624115815066,\n",
       " 0.34944907285348337,\n",
       " 0.5136231831077374,\n",
       " 0.9200076937675477,\n",
       " 0.9050677021645537,\n",
       " 0.8558018631104267,\n",
       " 0.9258658757113447,\n",
       " 0.9111968085019275,\n",
       " 0.8136884852792278,\n",
       " 0.5506491722031073,\n",
       " 0.7393131019792172,\n",
       " 0.3964356944868059,\n",
       " 0.9114408005066592,\n",
       " 0.9214841476895592,\n",
       " 0.7854384439160125,\n",
       " 0.6253934148285124,\n",
       " 0.9224556067977289,\n",
       " 0.8777651586014815,\n",
       " 0.936825275963003,\n",
       " 0.7993254716348166,\n",
       " 0.7260014947616693,\n",
       " 0.6477493209369255,\n",
       " 0.79232004673192,\n",
       " 0.7647254532936848,\n",
       " 0.9169420971111818,\n",
       " 0.8550469211255661,\n",
       " 0.864100040058897,\n",
       " 0.792993063336671,\n",
       " 0.8191558237328673,\n",
       " 0.2435109046297242,\n",
       " 0.7884945525665477,\n",
       " 0.847916167131578,\n",
       " 0.17336645784853685,\n",
       " 0.8055338214443187,\n",
       " 0.9100347671845946,\n",
       " 0.8663167132271661,\n",
       " 0.8811299311392236,\n",
       " 0.7913085504914775,\n",
       " 0.39716114316022755,\n",
       " 0.3771075626515379,\n",
       " 0.9341997802257538,\n",
       " 0.6458960713461193,\n",
       " 0.7908398753765857,\n",
       " 0.6274451381931401,\n",
       " 0.9022722799368579,\n",
       " 0.8700531182867108,\n",
       " 0.3597250626845793,\n",
       " 0.7462744343461413,\n",
       " 0.6671692872589284,\n",
       " 0.5943291255620995,\n",
       " 0.13912622679514114,\n",
       " 0.3686602446617502,\n",
       " 0.5297194616963166,\n",
       " 0.8720610153494459,\n",
       " 0.4793304104395587,\n",
       " 0.7536379430029128,\n",
       " 0.8493370020630384,\n",
       " 0.9153751092727738,\n",
       " 0.48565360783326506,\n",
       " 0.935922782258554,\n",
       " 0.9070489707017185,\n",
       " 0.7644820706109807,\n",
       " 0.4658168690975266,\n",
       " 0.7927372709669248,\n",
       " 0.9703926411542025,\n",
       " 0.7349012274934787,\n",
       " 0.4293954891387862,\n",
       " 0.9176871607098916,\n",
       " 0.5404468921549392,\n",
       " 0.9656298734925011,\n",
       " 0.5396649338831805,\n",
       " 0.6040047725643775,\n",
       " 0.44933132660208325,\n",
       " 0.527478818098704,\n",
       " 0.6002599843975269,\n",
       " 0.6500359291380102,\n",
       " 0.8344554177137337,\n",
       " 0.8135585780095572,\n",
       " 0.9083320323565993,\n",
       " 0.803929597350082,\n",
       " 0.8585041982055914,\n",
       " 0.8743599624344797,\n",
       " 0.6980289100697546,\n",
       " 0.7136686215497027,\n",
       " 0.6902662993079485,\n",
       " 0.5050955508995538,\n",
       " 0.8792023096120719,\n",
       " 0.7476489152872201,\n",
       " 0.9401367968983121,\n",
       " 0.7270474076873124,\n",
       " 0.9757437099109997,\n",
       " 0.5213362146206577,\n",
       " 0.6655629533709901,\n",
       " 0.5157555628906597,\n",
       " 0.8286846267454552,\n",
       " 0.6608413184230978,\n",
       " 0.8418127854966154,\n",
       " 0.33477355467550685,\n",
       " 0.4960052791568968,\n",
       " 0.3419775910901301,\n",
       " 0.2896699372415591,\n",
       " 0.3562328591190203,\n",
       " 0.48902776903576317,\n",
       " 0.7169425722324487,\n",
       " 0.8259694466085146,\n",
       " 0.5589403622379207,\n",
       " 0.9461627973751588,\n",
       " 0.5835951017309922,\n",
       " 0.589495322424354,\n",
       " 0.33038517508392384,\n",
       " 0.4673619196571485,\n",
       " 0.850788276544725,\n",
       " 0.6934460300688792,\n",
       " 0.4858258688841203,\n",
       " 0.7777762259497787,\n",
       " 0.976058808781884,\n",
       " 0.9675060776146975,\n",
       " 0.9027483421142655,\n",
       " 0.599894448572939,\n",
       " 0.7160207851968631,\n",
       " 0.44587824385274544,\n",
       " 0.8329481082432197,\n",
       " 0.9242391085082835,\n",
       " 0.7562096073771968,\n",
       " 0.6336719548762446,\n",
       " 0.7749986921597009,\n",
       " 0.9167070532386953,\n",
       " 0.8129135453941846,\n",
       " 0.8702816483348307,\n",
       " 0.10495512248440222,\n",
       " 0.033144038322974335,\n",
       " 0.5834954345768149,\n",
       " 0.7578388076538991,\n",
       " 0.893047513835358,\n",
       " 0.7188508684888031,\n",
       " 0.9511283324523405,\n",
       " 0.4256792176552493,\n",
       " 0.271090364584116,\n",
       " 0.44630487637989447,\n",
       " 0.7574264776827109,\n",
       " 0.6591650060180462,\n",
       " 0.7372200908684972,\n",
       " 0.9177856813777576,\n",
       " 0.2656013273003728,\n",
       " 0.7537123854413177,\n",
       " 0.6406325691276127,\n",
       " 0.6621558812531558,\n",
       " 0.8076169346920168,\n",
       " 0.30865597390767296,\n",
       " 0.842190174863796,\n",
       " 0.778767822336669,\n",
       " 0.273904467298828,\n",
       " 0.7715323046602384,\n",
       " 0.8602748858507233,\n",
       " 0.726620542130085,\n",
       " 0.6108839739753742,\n",
       " 0.7364188602777443,\n",
       " 0.6916003118259738,\n",
       " 0.863901778513735,\n",
       " 0.8597175928378346,\n",
       " 0.8432592916067201,\n",
       " 0.501890183714303,\n",
       " 0.21940075374688162,\n",
       " 0.3287073458234469,\n",
       " 0.9025375494451234,\n",
       " 0.9228275938467546,\n",
       " 0.6783093330384504,\n",
       " 0.7730840746200446,\n",
       " 0.736280726242547,\n",
       " 0.7294487932414719,\n",
       " 0.5917655890337145,\n",
       " 0.916839751842046,\n",
       " 0.4268235251608521,\n",
       " 0.21054683601615404,\n",
       " 0.42905028406718765,\n",
       " 0.6716274393327308,\n",
       " 0.7520833444113684,\n",
       " 0.5900950534777207,\n",
       " 0.520497067558645,\n",
       " 0.6018791823525622,\n",
       " 0.44106063686235986,\n",
       " 0.7724467981343317,\n",
       " 0.5778893949106486,\n",
       " 0.44469946661982873,\n",
       " 0.5417783844651598,\n",
       " 0.9042863611320052,\n",
       " 0.26785599322180553,\n",
       " 0.748206107243143,\n",
       " 0.4480677302437599,\n",
       " 0.31609608740216555,\n",
       " 0.8160831307521974,\n",
       " 0.9181353016032112,\n",
       " 0.12824774717893261,\n",
       " 0.8549439351366024,\n",
       " 0.629485568855748,\n",
       " 0.5954501293674864,\n",
       " 0.8088600411559597,\n",
       " 0.8499969636551057,\n",
       " 0.9267046536761101,\n",
       " 0.5099593178641917,\n",
       " 0.8970545065222364,\n",
       " 0.8929224230725356,\n",
       " 0.8085074072233354,\n",
       " 0.5513218885419344,\n",
       " 0.48961812626532836,\n",
       " 0.6030695152854679,\n",
       " 0.649836534984184,\n",
       " 0.8866916691414033,\n",
       " 0.8468568741974204,\n",
       " 0.7661621690398515,\n",
       " 0.7346226676545962,\n",
       " 0.3825571549397827,\n",
       " 0.4901130429873563,\n",
       " 0.43762050022681553,\n",
       " 0.5322285868302741,\n",
       " 0.651788144521039,\n",
       " 0.8478331111597293,\n",
       " 0.6329213119516469,\n",
       " 0.6350479079015328,\n",
       " 0.7124549667040507,\n",
       " 0.8463533117915645,\n",
       " 0.539778175965102,\n",
       " 0.4338415113362399,\n",
       " 0.618502124105439,\n",
       " 0.44856004728512333,\n",
       " 0.09647370129823685,\n",
       " 0.4791076360025791,\n",
       " 0.7272516473074152,\n",
       " 0.5179096732928296,\n",
       " 0.8310377510509106,\n",
       " 0.8222080749393713,\n",
       " 0.4570708885487884,\n",
       " 0.9518342633139003,\n",
       " 0.8716532677412033,\n",
       " 0.4343061544527911,\n",
       " 0.6532043250221194,\n",
       " 0.5621861278709739,\n",
       " 0.7148254750503433,\n",
       " 0.7891695371480903,\n",
       " 0.9193875507874922,\n",
       " 0.35620876559705444,\n",
       " 0.8274226472835348,\n",
       " 0.5733025091766106,\n",
       " 0.5985998908979724,\n",
       " 0.8921740058696631,\n",
       " 0.8444695000997697,\n",
       " 0.5334795136192833,\n",
       " 0.4432387126666127,\n",
       " 0.547464120674013,\n",
       " 0.8537881079045209,\n",
       " 0.7934244191706783,\n",
       " 0.41874417483505577,\n",
       " 0.5908398338791095,\n",
       " 0.9263035312144443,\n",
       " 0.898107785498253,\n",
       " 0.6112812922458456,\n",
       " 0.9010241892903742,\n",
       " 0.8553693280978636,\n",
       " 0.8964370949400795,\n",
       " 0.185333118836085,\n",
       " 0.534944370840535,\n",
       " 0.6686003845448446,\n",
       " 0.5835403515533968,\n",
       " 0.6973734078985272,\n",
       " 0.6942390414199444,\n",
       " 0.7148904488243237,\n",
       " 0.8220461128336011,\n",
       " 0.7831209435306414,\n",
       " 0.7586249789505294,\n",
       " 0.9199771710417488,\n",
       " 0.6223448736198021,\n",
       " 0.5099099472165107,\n",
       " 0.8874459596896412,\n",
       " 0.962796746600758,\n",
       " 0.3573729398124146,\n",
       " 0.387807725896739,\n",
       " 0.6822845463499878,\n",
       " 0.8120342117668402,\n",
       " 0.6132718926126307,\n",
       " 0.5349965954669799,\n",
       " 0.6653914597299364,\n",
       " 0.2882020725520572,\n",
       " 0.7743458351703605,\n",
       " 0.7350094733515171,\n",
       " 0.6838421782760908,\n",
       " 0.7569975134700235,\n",
       " 0.4411341812273469,\n",
       " 0.7142410216909466,\n",
       " 0.6375649080131993,\n",
       " 0.6912523763348357,\n",
       " 0.36259249076247213,\n",
       " 0.6160562602257488,\n",
       " 0.43535042019805525,\n",
       " 0.6808919955985715,\n",
       " 0.7052298425725012,\n",
       " 0.6327057378159628,\n",
       " 0.7820889674051843,\n",
       " 0.45044085926780797,\n",
       " 0.6856062317285875,\n",
       " 0.5227355614455059,\n",
       " 0.7840680001058964,\n",
       " 0.7814689537792494,\n",
       " 0.8521055931093717,\n",
       " 0.8143975616404504,\n",
       " 0.8166510367634321,\n",
       " 0.5592338114374815,\n",
       " 0.8636048674583435,\n",
       " 0.46555604450028354,\n",
       " 0.8021451380818782,\n",
       " 0.5838297293041692,\n",
       " 0.9007595030948369,\n",
       " 0.751490477088726,\n",
       " 0.9144833569875871,\n",
       " 0.5356153687142362,\n",
       " 0.8782710076582552,\n",
       " 0.3201451778110832,\n",
       " 0.8058657756357481,\n",
       " 0.6573308645775824,\n",
       " 0.7995697505245305,\n",
       " 0.9229980880262876,\n",
       " 0.9121274190719681,\n",
       " 0.8425156342561799,\n",
       " 0.8987830066921735,\n",
       " 0.4438310125861505,\n",
       " 0.8963282037865032,\n",
       " 0.7028695393993397,\n",
       " 0.867933125929399,\n",
       " 0.7304127310562616,\n",
       " 0.578213436404864,\n",
       " 0.3260228379733032,\n",
       " 0.8932427215154725,\n",
       " 0.9148329760390099,\n",
       " 0.5005388915839821,\n",
       " 0.8016761651544859,\n",
       " 0.8241777198483246,\n",
       " 0.8153525488846229,\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict_temp.append((Y_predict_lr[i] + Y_predict_cnn[i]) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7821795656902211,\n",
       " 0.7322048928074766,\n",
       " 0.6769288237714355,\n",
       " 0.9232060886053208,\n",
       " 0.7636085474986847,\n",
       " 0.6989003050179315,\n",
       " 0.6514817259038301,\n",
       " 0.7032082072973302,\n",
       " 0.7123131421092022,\n",
       " 0.686663417226966,\n",
       " 0.9097344714158052,\n",
       " 0.4704497726606686,\n",
       " 0.6847283869564542,\n",
       " 0.5155052038948547,\n",
       " 0.915586192621144,\n",
       " 0.26201000179906697,\n",
       " 0.8994400718614441,\n",
       " 0.6368460209090437,\n",
       " 0.7858382623603368,\n",
       " 0.3839700593026235,\n",
       " 0.8084173498524736,\n",
       " 0.8017286241164303,\n",
       " 0.9136781189201648,\n",
       " 0.8471509975068205,\n",
       " 0.9566878157273446,\n",
       " 0.6656232986516144,\n",
       " 0.7254675937410695,\n",
       " 0.942262732292676,\n",
       " 0.8090296200115747,\n",
       " 0.94187385551492,\n",
       " 0.6098780237977018,\n",
       " 0.7268148201302096,\n",
       " 0.8395090462491981,\n",
       " 0.6232918159426388,\n",
       " 0.6252407055509681,\n",
       " 0.43514013130343,\n",
       " 0.7910901168834001,\n",
       " 0.880788700237831,\n",
       " 0.5815274392765865,\n",
       " 0.833717457521943,\n",
       " 0.6208477364940778,\n",
       " 0.8997755945424697,\n",
       " 0.8608512841555309,\n",
       " 0.5341327947115535,\n",
       " 0.5832420539201155,\n",
       " 0.8050140912758983,\n",
       " 0.7257269586465179,\n",
       " 0.39700496960241305,\n",
       " 0.9373817642419138,\n",
       " 0.6251972554270472,\n",
       " 0.6616831940778611,\n",
       " 0.8293898487326351,\n",
       " 0.7604909508112456,\n",
       " 0.41078796798244366,\n",
       " 0.3352090545599917,\n",
       " 0.6068195888591504,\n",
       " 0.40476183311258357,\n",
       " 0.7786806333868654,\n",
       " 0.2061106905182365,\n",
       " 0.7549282448030434,\n",
       " 0.6442759844233913,\n",
       " 0.3357037274814353,\n",
       " 0.5430152690739088,\n",
       " 0.736214731994117,\n",
       " 0.39536720138654113,\n",
       " 0.8969058181080405,\n",
       " 0.6612544120315921,\n",
       " 0.7403594320650244,\n",
       " 0.6961708151027337,\n",
       " 0.6523613088791868,\n",
       " 0.5450160943070765,\n",
       " 0.39881711600482644,\n",
       " 0.48661821457073595,\n",
       " 0.6283203088134405,\n",
       " 0.8786661040000441,\n",
       " 0.5047430680782868,\n",
       " 0.6128196915490453,\n",
       " 0.7105581361540423,\n",
       " 0.30775308912085086,\n",
       " 0.43236553799024624,\n",
       " 0.7100479210738778,\n",
       " 0.3684876993786167,\n",
       " 0.3303108143087755,\n",
       " 0.6526889643210693,\n",
       " 0.7651752437194378,\n",
       " 0.7640709521995297,\n",
       " 0.7810028721255022,\n",
       " 0.3944825592704594,\n",
       " 0.8833502843595946,\n",
       " 0.7054081337987321,\n",
       " 0.8070747003492663,\n",
       " 0.7328546490270558,\n",
       " 0.6990593386725208,\n",
       " 0.9410067033938156,\n",
       " 0.9530162872310899,\n",
       " 0.9029338126762267,\n",
       " 0.9353297162644898,\n",
       " 0.8507703394765673,\n",
       " 0.885866684424303,\n",
       " 0.7586171204425851,\n",
       " 0.9080969723445621,\n",
       " 0.8972688622243423,\n",
       " 0.8195621439728984,\n",
       " 0.8707603222193678,\n",
       " 0.8849174339029513,\n",
       " 0.8740595533143494,\n",
       " 0.7804722342920305,\n",
       " 0.892185576751045,\n",
       " 0.5605737684018699,\n",
       " 0.9238324819198016,\n",
       " 0.6331645917144522,\n",
       " 0.6915989819527546,\n",
       " 0.8552969392680172,\n",
       " 0.8410120469916582,\n",
       " 0.8193919067457041,\n",
       " 0.7338363126865453,\n",
       " 0.3968021557870913,\n",
       " 0.7862702533675465,\n",
       " 0.6460335872386622,\n",
       " 0.6090122281657223,\n",
       " 0.7692897661189033,\n",
       " 0.753225536331186,\n",
       " 0.7301558075759864,\n",
       " 0.4539512201194631,\n",
       " 0.6977929607972568,\n",
       " 0.9039255532323536,\n",
       " 0.900549084222781,\n",
       " 0.6378659926564475,\n",
       " 0.6606233832560964,\n",
       " 0.9061564878996772,\n",
       " 0.9394098776333173,\n",
       " 0.9130937171334212,\n",
       " 0.7511083365461888,\n",
       " 0.6476787052589035,\n",
       " 0.8829459051125343,\n",
       " 0.8163169402794972,\n",
       " 0.9394374590255121,\n",
       " 0.8731806146501844,\n",
       " 0.4029967326760912,\n",
       " 0.6228751708941576,\n",
       " 0.7579564681081481,\n",
       " 0.6216622950374027,\n",
       " 0.8373707330138562,\n",
       " 0.8047339914372325,\n",
       " 0.7250693858881334,\n",
       " 0.2440546419974551,\n",
       " 0.848939581206626,\n",
       " 0.8828499871745452,\n",
       " 0.7398465700817243,\n",
       " 0.9223551417889732,\n",
       " 0.7478145305687374,\n",
       " 0.5716903011393654,\n",
       " 0.6072527794979972,\n",
       " 0.8601305792084896,\n",
       " 0.6149164764410995,\n",
       " 0.4573596053355833,\n",
       " 0.8522337773638783,\n",
       " 0.5276608549177082,\n",
       " 0.5611256759591787,\n",
       " 0.7492935007022554,\n",
       " 0.717854804226158,\n",
       " 0.6319317862874563,\n",
       " 0.3944842905102864,\n",
       " 0.6204178291145335,\n",
       " 0.30427215374825894,\n",
       " 0.4740721955524346,\n",
       " 0.6182036038566228,\n",
       " 0.3123535910111749,\n",
       " 0.2784413197137051,\n",
       " 0.7480436885617001,\n",
       " 0.6865019869610895,\n",
       " 0.3239070713443384,\n",
       " 0.29002682606486796,\n",
       " 0.5624984969023954,\n",
       " 0.8811665078170146,\n",
       " 0.6494173914736046,\n",
       " 0.588873178632342,\n",
       " 0.8451962383598071,\n",
       " 0.9036894822051078,\n",
       " 0.3718680930637007,\n",
       " 0.9251715052543923,\n",
       " 0.9386523862373413,\n",
       " 0.7160379175824682,\n",
       " 0.8768198227654378,\n",
       " 0.6600744380993221,\n",
       " 0.39967988968224266,\n",
       " 0.5958184860975198,\n",
       " 0.472055807197432,\n",
       " 0.5067094904612348,\n",
       " 0.6592141070435386,\n",
       " 0.5949300119322491,\n",
       " 0.8980207224880452,\n",
       " 0.8288680431270391,\n",
       " 0.8837082567240793,\n",
       " 0.2357792951058836,\n",
       " 0.7527654411787473,\n",
       " 0.9237697718039144,\n",
       " 0.7697384300997197,\n",
       " 0.7893717423707935,\n",
       " 0.6071914759621388,\n",
       " 0.4236181923605337,\n",
       " 0.8986708928023162,\n",
       " 0.7157667600636022,\n",
       " 0.8693522884058227,\n",
       " 0.8539656608655892,\n",
       " 0.6220353456799559,\n",
       " 0.7513399691560582,\n",
       " 0.7621394386810819,\n",
       " 0.6911307074464328,\n",
       " 0.5781098532875286,\n",
       " 0.5764166738760195,\n",
       " 0.6348493468119669,\n",
       " 0.3671001470053037,\n",
       " 0.7013046371366476,\n",
       " 0.40810840998656595,\n",
       " 0.2848966110664797,\n",
       " 0.6286654137172387,\n",
       " 0.6805570901636007,\n",
       " 0.9271817085520203,\n",
       " 0.7539750413499253,\n",
       " 0.5695714238862055,\n",
       " 0.7089959411603004,\n",
       " 0.527752793874226,\n",
       " 0.8936125750018168,\n",
       " 0.7592180787181501,\n",
       " 0.8046743558056977,\n",
       " 0.7312154344777483,\n",
       " 0.724382971025483,\n",
       " 0.7017091973578474,\n",
       " 0.47839710906223043,\n",
       " 0.42096565587858165,\n",
       " 0.39507946378420217,\n",
       " 0.847396944894607,\n",
       " 0.3905981675504263,\n",
       " 0.7876406218888411,\n",
       " 0.6802071017155719,\n",
       " 0.7013570635561399,\n",
       " 0.4348438853415134,\n",
       " 0.36835677528543986,\n",
       " 0.6998523774364127,\n",
       " 0.5155229577061513,\n",
       " 0.8074702515368615,\n",
       " 0.7453368200200459,\n",
       " 0.6083049736192895,\n",
       " 0.39213700244036387,\n",
       " 0.3757626322627733,\n",
       " 0.7147726419315705,\n",
       " 0.5708791623993754,\n",
       " 0.38474743771141096,\n",
       " 0.8770025147661973,\n",
       " 0.6286182625656067,\n",
       " 0.6652205678414046,\n",
       " 0.9184494125266481,\n",
       " 0.8947099239526661,\n",
       " 0.8714803311271937,\n",
       " 0.7156647328333183,\n",
       " 0.7642354978454148,\n",
       " 0.9102014874822539,\n",
       " 0.7796859897263437,\n",
       " 0.9046570189295391,\n",
       " 0.8989353270909356,\n",
       " 0.39292084452768716,\n",
       " 0.85772079462495,\n",
       " 0.9318025891158939,\n",
       " 0.7720706592435981,\n",
       " 0.31110733292794995,\n",
       " 0.9383095720101021,\n",
       " 0.9268178712477491,\n",
       " 0.8647465225729268,\n",
       " 0.6721337053723557,\n",
       " 0.7284013358214876,\n",
       " 0.5623238646276166,\n",
       " 0.8898212353143968,\n",
       " 0.8842898480472905,\n",
       " 0.8624491668641971,\n",
       " 0.9081613968811055,\n",
       " 0.9020991664012695,\n",
       " 0.476004116747773,\n",
       " 0.904335151888914,\n",
       " 0.5179812054456745,\n",
       " 0.8667165360941953,\n",
       " 0.7914635631853193,\n",
       " 0.9076859665981067,\n",
       " 0.79247278399062,\n",
       " 0.5544006287480998,\n",
       " 0.7219354331883375,\n",
       " 0.46260231352467346,\n",
       " 0.7423315561400992,\n",
       " 0.8180902586915583,\n",
       " 0.7252978646682532,\n",
       " 0.6622965923039235,\n",
       " 0.8052158563922089,\n",
       " 0.6564374903548411,\n",
       " 0.8145725582482675,\n",
       " 0.4607353375274138,\n",
       " 0.51742217262162,\n",
       " 0.66514376899761,\n",
       " 0.7579981888552073,\n",
       " 0.6291634178582077,\n",
       " 0.5750103230518178,\n",
       " 0.2873643909585273,\n",
       " 0.9364445568365168,\n",
       " 0.8829587439337311,\n",
       " 0.9095405604561242,\n",
       " 0.8876278079596411,\n",
       " 0.9412210014566101,\n",
       " 0.8013677432787742,\n",
       " 0.5103531040624163,\n",
       " 0.9454312688022943,\n",
       " 0.7598031667644973,\n",
       " 0.8534409248372358,\n",
       " 0.8258836820492663,\n",
       " 0.8723845848843548,\n",
       " 0.8006971451539664,\n",
       " 0.46093201905794934,\n",
       " 0.8757129347540578,\n",
       " 0.706085400969005,\n",
       " 0.45625761257373887,\n",
       " 0.8110482626152412,\n",
       " 0.8877026143576001,\n",
       " 0.934691444228819,\n",
       " 0.7752518165302325,\n",
       " 0.8450417756602184,\n",
       " 0.6142283143635457,\n",
       " 0.7222853694050378,\n",
       " 0.642413606091177,\n",
       " 0.7826456445097763,\n",
       " 0.8043080724551323,\n",
       " 0.5770779419405375,\n",
       " 0.674255395217192,\n",
       " 0.32435131687495733,\n",
       " 0.7714459454519658,\n",
       " 0.5891598934192512,\n",
       " 0.5580525414688657,\n",
       " 0.5591147576454132,\n",
       " 0.31251706185419176,\n",
       " 0.29908648559450124,\n",
       " 0.3230223666234536,\n",
       " 0.3985865367815563,\n",
       " 0.465029351568457,\n",
       " 0.593228614739366,\n",
       " 0.5389413374594567,\n",
       " 0.6466157095421363,\n",
       " 0.21523833485101104,\n",
       " 0.8915101048308084,\n",
       " 0.41295658496295784,\n",
       " 0.8677606959211592,\n",
       " 0.43182879612656333,\n",
       " 0.8506048951808487,\n",
       " 0.6003336041993399,\n",
       " 0.604217402910904,\n",
       " 0.40948587270475023,\n",
       " 0.5313114002597106,\n",
       " 0.9359960559877283,\n",
       " 0.616676374355934,\n",
       " 0.8460154599018328,\n",
       " 0.7321982699133343,\n",
       " 0.8447298151336005,\n",
       " 0.906295328112686,\n",
       " 0.8239257033364367,\n",
       " 0.8687326442878046,\n",
       " 0.901534306372867,\n",
       " 0.5223281835836024,\n",
       " 0.8466458243785426,\n",
       " 0.4341248628093226,\n",
       " 0.6752810944821704,\n",
       " 0.7390637600568382,\n",
       " 0.4568581203008351,\n",
       " 0.6046877435903089,\n",
       " 0.6766693285576847,\n",
       " 0.9233534467149659,\n",
       " 0.7025769684679439,\n",
       " 0.657717147226889,\n",
       " 0.868720836453838,\n",
       " 0.6306959993700532,\n",
       " 0.741561686820041,\n",
       " 0.6579937492035547,\n",
       " 0.7506555505815411,\n",
       " 0.7160050909394626,\n",
       " 0.40972513668807337,\n",
       " 0.377207576560195,\n",
       " 0.8805793814850926,\n",
       " 0.7989809890416207,\n",
       " 0.43275780700205724,\n",
       " 0.8032101255469133,\n",
       " 0.7456362994527157,\n",
       " 0.6766197836389034,\n",
       " 0.670565864822124,\n",
       " 0.33214042468995864,\n",
       " 0.8636704695945852,\n",
       " 0.1699123003013948,\n",
       " 0.9197182063345544,\n",
       " 0.5274883706900565,\n",
       " 0.4817052630566778,\n",
       " 0.7577859438770785,\n",
       " 0.6258152055675892,\n",
       " 0.9222149859690014,\n",
       " 0.9241394354895855,\n",
       " 0.6788842260226914,\n",
       " 0.9336595777175631,\n",
       " 0.23156943525247697,\n",
       " 0.8007183023770901,\n",
       " 0.5848636934132553,\n",
       " 0.6723164962096082,\n",
       " 0.6899306307371134,\n",
       " 0.6779056622264193,\n",
       " 0.4214829960883993,\n",
       " 0.25486027742030537,\n",
       " 0.6935286172973185,\n",
       " 0.6564735894924839,\n",
       " 0.35243191128225426,\n",
       " 0.7582131540710879,\n",
       " 0.7442301908412121,\n",
       " 0.677830325769754,\n",
       " 0.29016272703974477,\n",
       " 0.28829644646633257,\n",
       " 0.24562366607563751,\n",
       " 0.7462541594239331,\n",
       " 0.6339995662569893,\n",
       " 0.5920669469261518,\n",
       " 0.9228099324592712,\n",
       " 0.6888536944163628,\n",
       " 0.8214834247805148,\n",
       " 0.7028129543876356,\n",
       " 0.7310111759376408,\n",
       " 0.6900903735538968,\n",
       " 0.8496419319287801,\n",
       " 0.46124812448661223,\n",
       " 0.3510018539424998,\n",
       " 0.9476091085828198,\n",
       " 0.6683389946331746,\n",
       " 0.8526015993062035,\n",
       " 0.9183835811842724,\n",
       " 0.9129273060823782,\n",
       " 0.563790404868507,\n",
       " 0.9003769516556512,\n",
       " 0.6051597977625359,\n",
       " 0.3977484880388265,\n",
       " 0.4840731915064977,\n",
       " 0.8939903070284161,\n",
       " 0.8964463782372687,\n",
       " 0.41457363216461285,\n",
       " 0.7112896239331337,\n",
       " 0.7858893774951301,\n",
       " 0.23234287367180112,\n",
       " 0.8578288633879825,\n",
       " 0.6493518638208973,\n",
       " 0.6636188473981504,\n",
       " 0.9255150814000017,\n",
       " 0.762618536733219,\n",
       " 0.7990332940111451,\n",
       " 0.4536132540800253,\n",
       " 0.7182909490801686,\n",
       " 0.7515305724385382,\n",
       " 0.8557330092125448,\n",
       " 0.8866324122081095,\n",
       " 0.8075077437604354,\n",
       " 0.6142495458421176,\n",
       " 0.7777863449017481,\n",
       " 0.7011219495310131,\n",
       " 0.3154531115517022,\n",
       " 0.820699111283588,\n",
       " 0.868536281742041,\n",
       " 0.4896857305021625,\n",
       " 0.6986039256534171,\n",
       " 0.17263652396892876,\n",
       " 0.6524841496503611,\n",
       " 0.9182023459906244,\n",
       " 0.6863606591987269,\n",
       " 0.7547371493125657,\n",
       " 0.6455911751248082,\n",
       " 0.6080003651519061,\n",
       " 0.6606495672010831,\n",
       " 0.5991653233207881,\n",
       " 0.8607827497701159,\n",
       " 0.9083035543436427,\n",
       " 0.40679538082746225,\n",
       " 0.9129362613795937,\n",
       " 0.9278126717100426,\n",
       " 0.9509796633095439,\n",
       " 0.8722725917652873,\n",
       " 0.8716026173192128,\n",
       " 0.9367777421731226,\n",
       " 0.9177978173360968,\n",
       " 0.8763318731310923,\n",
       " 0.8308060687995219,\n",
       " 0.7534476057299558,\n",
       " 0.8787527056544877,\n",
       " 0.9181407300384119,\n",
       " 0.8205869857406773,\n",
       " 0.9382225724481346,\n",
       " 0.8797227081656938,\n",
       " 0.5915478909689446,\n",
       " 0.8911132175143543,\n",
       " 0.8075172116439012,\n",
       " 0.5776022077125754,\n",
       " 0.818312441458611,\n",
       " 0.5653840926742782,\n",
       " 0.7712190828613947,\n",
       " 0.7296246557862198,\n",
       " 0.31426318991757907,\n",
       " 0.395114203942157,\n",
       " 0.8826024020957822,\n",
       " 0.762810009635791,\n",
       " 0.9116540412595945,\n",
       " 0.5296011487226859,\n",
       " 0.8625677284202644,\n",
       " 0.37608814035388416,\n",
       " 0.339931573911041,\n",
       " 0.21731563405189375,\n",
       " 0.6859884594092404,\n",
       " 0.5045070117645127,\n",
       " 0.2965002670607027,\n",
       " 0.7103689481767885,\n",
       " 0.26090326456770035,\n",
       " 0.6404021334455803,\n",
       " 0.34701904202193645,\n",
       " 0.6781157748014219,\n",
       " 0.5632228947606404,\n",
       " 0.5779696872847409,\n",
       " 0.7830096661440432,\n",
       " 0.6194614802966734,\n",
       " 0.6178491177010588,\n",
       " 0.5215456324362088,\n",
       " 0.7509928591981538,\n",
       " 0.4623263139694189,\n",
       " 0.40205563852204135,\n",
       " 0.812642741343635,\n",
       " 0.7548680061585333,\n",
       " 0.6565321881679813,\n",
       " 0.40518232033078144,\n",
       " 0.928217182725499,\n",
       " 0.45663167969377216,\n",
       " 0.9580673580067784,\n",
       " 0.8177397507263289,\n",
       " 0.859802338246357,\n",
       " 0.9037417440902196,\n",
       " 0.8960063253279053,\n",
       " 0.7606013687477527,\n",
       " 0.6531194959595699,\n",
       " 0.7404952921993884,\n",
       " 0.9487247501883747,\n",
       " 0.717244159760646,\n",
       " 0.9040022300397992,\n",
       " 0.5545802761460712,\n",
       " 0.3834289442206754,\n",
       " 0.5550011590185053,\n",
       " 0.910432085106222,\n",
       " 0.6912695213588499,\n",
       " 0.7817107598909713,\n",
       " 0.7428559925707032,\n",
       " 0.7839560312109313,\n",
       " 0.7760474652651919,\n",
       " 0.7384673716398553,\n",
       " 0.8637781091837058,\n",
       " 0.8083685862003265,\n",
       " 0.4113899810289847,\n",
       " 0.4905291625530129,\n",
       " 0.6197461985101362,\n",
       " 0.4657486818452604,\n",
       " 0.57124561801985,\n",
       " 0.6989387826220511,\n",
       " 0.26814309174511775,\n",
       " 0.6712629697972309,\n",
       " 0.7938834551744995,\n",
       " 0.7426952707454817,\n",
       " 0.6214309536259955,\n",
       " 0.5423554617001003,\n",
       " 0.6116999032810919,\n",
       " 0.5526731209355544,\n",
       " 0.5717147061791992,\n",
       " 0.8015857586100943,\n",
       " 0.7063985977133024,\n",
       " 0.6316887351539674,\n",
       " 0.7590062508578692,\n",
       " 0.7683806777892348,\n",
       " 0.6002250187322806,\n",
       " 0.8743995021883566,\n",
       " 0.9483155296811268,\n",
       " 0.6253262316417998,\n",
       " 0.7865672735890388,\n",
       " 0.6996594894560724,\n",
       " 0.4785435194811779,\n",
       " 0.7269240282607486,\n",
       " 0.4148976029030357,\n",
       " 0.6325303751897422,\n",
       " 0.3365701439075479,\n",
       " 0.2980748548010347,\n",
       " 0.42499375186761423,\n",
       " 0.6884000242219805,\n",
       " 0.6110609021956936,\n",
       " 0.32424236506853266,\n",
       " 0.66293787740597,\n",
       " 0.7861708063866402,\n",
       " 0.5175278452028417,\n",
       " 0.5168157453494164,\n",
       " 0.3237171071591329,\n",
       " 0.522950009823534,\n",
       " 0.3960410155258554,\n",
       " 0.24570746341178024,\n",
       " 0.7611299738424958,\n",
       " 0.2530393879381402,\n",
       " 0.6598404201863435,\n",
       " 0.44675886013211397,\n",
       " 0.8688638935329085,\n",
       " 0.6044204710202259,\n",
       " 0.7564743433071857,\n",
       " 0.5634595880513079,\n",
       " 0.36411895773480596,\n",
       " 0.7846588665879555,\n",
       " 0.9012743853506799,\n",
       " 0.8524120496375198,\n",
       " 0.8624568779600137,\n",
       " 0.36779808385768953,\n",
       " 0.9227737848015423,\n",
       " 0.7889372464535692,\n",
       " 0.8133671100859539,\n",
       " 0.5038870733097193,\n",
       " 0.7586376751280358,\n",
       " 0.8764653916965908,\n",
       " 0.7455788904955774,\n",
       " 0.32614853862238335,\n",
       " 0.8834031390537136,\n",
       " 0.8939545169741184,\n",
       " 0.8519291453952305,\n",
       " 0.8614630331099415,\n",
       " 0.9274129623136389,\n",
       " 0.8043973400568696,\n",
       " 0.8992748178745067,\n",
       " 0.8632473115680567,\n",
       " 0.8688058555772984,\n",
       " 0.8203943516752025,\n",
       " 0.9243985701261539,\n",
       " 0.8046802196465299,\n",
       " 0.7740212729313469,\n",
       " 0.8431865101531184,\n",
       " 0.35775970801036056,\n",
       " 0.4734700550348059,\n",
       " 0.7043360830466053,\n",
       " 0.5457581974162213,\n",
       " 0.6968403145117027,\n",
       " 0.8128370027772354,\n",
       " 0.8771507316145764,\n",
       " 0.31081813800847236,\n",
       " 0.753831965322844,\n",
       " 0.616411780443838,\n",
       " 0.667096235606834,\n",
       " 0.8347115844522471,\n",
       " 0.22549117185689072,\n",
       " 0.7677828177525708,\n",
       " 0.5867097222965858,\n",
       " 0.7175665279896918,\n",
       " 0.798192695226816,\n",
       " 0.9234268958066524,\n",
       " 0.8882369574452109,\n",
       " 0.7224830420464597,\n",
       " 0.7919132201157566,\n",
       " 0.8369004161769471,\n",
       " 0.4917216288879022,\n",
       " 0.5324753505330784,\n",
       " 0.7606190302588257,\n",
       " 0.5038602578427243,\n",
       " 0.8853340408638839,\n",
       " 0.4000230928413615,\n",
       " 0.36485476600205013,\n",
       " 0.3867773489327949,\n",
       " 0.4106004066687011,\n",
       " 0.867511144814012,\n",
       " 0.8923271486254352,\n",
       " 0.8895352504737601,\n",
       " 0.9225213879605687,\n",
       " 0.9052154873951481,\n",
       " 0.8033113654430349,\n",
       " 0.5785518238377056,\n",
       " 0.8814109238182751,\n",
       " 0.3817728851847201,\n",
       " 0.9257968241864525,\n",
       " 0.888516874132941,\n",
       " 0.7962225711151718,\n",
       " 0.49912588174888994,\n",
       " 0.9412223423927271,\n",
       " 0.9060748863463375,\n",
       " 0.9337076737611182,\n",
       " 0.8943855337595823,\n",
       " 0.6668254440442625,\n",
       " 0.5663739941289327,\n",
       " 0.72841054137459,\n",
       " 0.6258337451963637,\n",
       " 0.8837495199707532,\n",
       " 0.8228322751846426,\n",
       " 0.8197389701756583,\n",
       " 0.8291169644369225,\n",
       " 0.7490711732003144,\n",
       " 0.35286085520295163,\n",
       " 0.8392740677731784,\n",
       " 0.8515169183374305,\n",
       " 0.310563668014871,\n",
       " 0.7094787515310031,\n",
       " 0.8817428281745003,\n",
       " 0.812634578799644,\n",
       " 0.8912317474471625,\n",
       " 0.7060553678575532,\n",
       " 0.4182188659309616,\n",
       " 0.39503691459465673,\n",
       " 0.7792904182481422,\n",
       " 0.6650564725152109,\n",
       " 0.7883473530254571,\n",
       " 0.6769369807886264,\n",
       " 0.8662674404137607,\n",
       " 0.8169297695745502,\n",
       " 0.3594082865088182,\n",
       " 0.6762391434572543,\n",
       " 0.7539002007869653,\n",
       " 0.5342358121306476,\n",
       " 0.2342304963334233,\n",
       " 0.651189073589929,\n",
       " 0.4956215251980311,\n",
       " 0.8857643136984281,\n",
       " 0.6349840659290358,\n",
       " 0.7281402451557426,\n",
       " 0.8044716279976902,\n",
       " 0.8697594171020284,\n",
       " 0.355402891719228,\n",
       " 0.7859343205434918,\n",
       " 0.8816065863842495,\n",
       " 0.6092849907232616,\n",
       " 0.44791513705122676,\n",
       " 0.7989189590460424,\n",
       " 0.8582131033864211,\n",
       " 0.7485931092404292,\n",
       " 0.45470337333889704,\n",
       " 0.790704484411201,\n",
       " 0.5143688731513474,\n",
       " 0.9375268067003812,\n",
       " 0.6266020334320526,\n",
       " 0.6563713978356979,\n",
       " 0.5740389586156565,\n",
       " 0.5345443459490571,\n",
       " 0.57877114286727,\n",
       " 0.7542765548455743,\n",
       " 0.8576824379085941,\n",
       " 0.8672589422572621,\n",
       " 0.9093926801925715,\n",
       " 0.7763946172430541,\n",
       " 0.7035114358674479,\n",
       " 0.8410139321953746,\n",
       " 0.7463769560667302,\n",
       " 0.7784424367713634,\n",
       " 0.6167573639131345,\n",
       " 0.5701883123367806,\n",
       " 0.9030584076548588,\n",
       " 0.7152310111234663,\n",
       " 0.886384382051101,\n",
       " 0.7054574791438453,\n",
       " 0.9211820591293445,\n",
       " 0.6132211616146322,\n",
       " 0.6764860168367103,\n",
       " 0.5216350589474252,\n",
       " 0.7603845765822219,\n",
       " 0.75125956441724,\n",
       " 0.7795083935432047,\n",
       " 0.31160848052686374,\n",
       " 0.6391884468417337,\n",
       " 0.31169107815033614,\n",
       " 0.35500784320895984,\n",
       " 0.4509114146175582,\n",
       " 0.6429765143792273,\n",
       " 0.630331879348668,\n",
       " 0.8901949414229838,\n",
       " 0.6517514169061902,\n",
       " 0.9257945924154509,\n",
       " 0.596032273496416,\n",
       " 0.4916329242578853,\n",
       " 0.39328446056911237,\n",
       " 0.5409213805563053,\n",
       " 0.8966007037863171,\n",
       " 0.7466354626484812,\n",
       " 0.41027479446266646,\n",
       " 0.7056368755102428,\n",
       " 0.9330875165413353,\n",
       " 0.9294717515036759,\n",
       " 0.7698966841497873,\n",
       " 0.738177788782819,\n",
       " 0.7375661646804759,\n",
       " 0.4428434874548739,\n",
       " 0.8296547833642356,\n",
       " 0.8606466095706027,\n",
       " 0.7243684785410721,\n",
       " 0.6839935154514982,\n",
       " 0.7749957864467999,\n",
       " 0.929756235280638,\n",
       " 0.8578442960399559,\n",
       " 0.8802845553275882,\n",
       " 0.3220195214365667,\n",
       " 0.12260528773413969,\n",
       " 0.6380289449946218,\n",
       " 0.7696474238411406,\n",
       " 0.9505044128120912,\n",
       " 0.6870210571776038,\n",
       " 0.9132502142455079,\n",
       " 0.46217217287287926,\n",
       " 0.33339455854301886,\n",
       " 0.5500511236508631,\n",
       " 0.7694768092123494,\n",
       " 0.6528980928390129,\n",
       " 0.6613050261782949,\n",
       " 0.9090549422769305,\n",
       " 0.2970949108296128,\n",
       " 0.753019696654635,\n",
       " 0.7322113369876913,\n",
       " 0.7491704698240916,\n",
       " 0.7008413666086019,\n",
       " 0.33295829647004793,\n",
       " 0.854453792738257,\n",
       " 0.859549173103572,\n",
       " 0.28554080845038055,\n",
       " 0.8883013328853162,\n",
       " 0.9016003666092625,\n",
       " 0.7226660125776125,\n",
       " 0.5033620724808637,\n",
       " 0.7569748220363659,\n",
       " 0.7630294782838861,\n",
       " 0.7584525716599284,\n",
       " 0.8477983889809616,\n",
       " 0.8338523349150475,\n",
       " 0.5493922827772292,\n",
       " 0.14459614503789586,\n",
       " 0.41892574893771783,\n",
       " 0.9329155698525977,\n",
       " 0.920941009926004,\n",
       " 0.6726264904712851,\n",
       " 0.7605442268810829,\n",
       " 0.7441613796146198,\n",
       " 0.6790064116371832,\n",
       " 0.6407385878808741,\n",
       " 0.9192325030590403,\n",
       " 0.33896230637176683,\n",
       " 0.22388503442379065,\n",
       " 0.5194257632095086,\n",
       " 0.6273298082965704,\n",
       " 0.7735274773387052,\n",
       " 0.6730756547388195,\n",
       " 0.6028329555427231,\n",
       " 0.6256562570171352,\n",
       " 0.5718262662516196,\n",
       " 0.7697062011993204,\n",
       " 0.5711298572554453,\n",
       " 0.43072584166501854,\n",
       " 0.4744849941521141,\n",
       " 0.9184744012475123,\n",
       " 0.24043640204591646,\n",
       " 0.8494928912770354,\n",
       " 0.5135975150249434,\n",
       " 0.35250834906551376,\n",
       " 0.8031272138197691,\n",
       " 0.9281263651597131,\n",
       " 0.24714865677830922,\n",
       " 0.7996521805791235,\n",
       " 0.6646304906083884,\n",
       " 0.5043302349061263,\n",
       " 0.7782325854665573,\n",
       " 0.8218660577182499,\n",
       " 0.9354873436968653,\n",
       " 0.36070215658384347,\n",
       " 0.912917968401511,\n",
       " 0.8633915554222977,\n",
       " 0.6960352831393408,\n",
       " 0.549049119387335,\n",
       " 0.44926733016838205,\n",
       " 0.657896737563701,\n",
       " 0.5304277042217862,\n",
       " 0.8835951205951301,\n",
       " 0.6767104820556931,\n",
       " 0.7875569975856238,\n",
       " 0.7084991381613239,\n",
       " 0.387022395725377,\n",
       " 0.4697280902991107,\n",
       " 0.52514494610458,\n",
       " 0.6860693144776335,\n",
       " 0.6314180065310266,\n",
       " 0.7769412064231973,\n",
       " 0.683090920140984,\n",
       " 0.6338910829891349,\n",
       " 0.7583830432203014,\n",
       " 0.7428309407275873,\n",
       " 0.47251802625665096,\n",
       " 0.4237349256158946,\n",
       " 0.5304666359745938,\n",
       " 0.4792385831692441,\n",
       " 0.23981381368408416,\n",
       " 0.6803792645615021,\n",
       " 0.7235270218850136,\n",
       " 0.45918122136587997,\n",
       " 0.9095922856404874,\n",
       " 0.7881783201775849,\n",
       " 0.4397020350816844,\n",
       " 0.9015571634031733,\n",
       " 0.8319808036127405,\n",
       " 0.5608418318650067,\n",
       " 0.6949330504706979,\n",
       " 0.681970239290228,\n",
       " 0.623953965576751,\n",
       " 0.748185852808848,\n",
       " 0.8630026277575935,\n",
       " 0.3155491337524068,\n",
       " 0.7689271411258929,\n",
       " 0.5709451246525286,\n",
       " 0.592024511419719,\n",
       " 0.8864710404560532,\n",
       " 0.8449650174858736,\n",
       " 0.4409833871259977,\n",
       " 0.445325735003981,\n",
       " 0.5451462543815847,\n",
       " 0.718056597020411,\n",
       " 0.7660232615002412,\n",
       " 0.3628311764344674,\n",
       " 0.6262148100673475,\n",
       " 0.9185236629444709,\n",
       " 0.8505083308602477,\n",
       " 0.7007233073087459,\n",
       " 0.9413845980765653,\n",
       " 0.795444081473595,\n",
       " 0.7805989138376892,\n",
       " 0.2731109525079817,\n",
       " 0.5592748915157251,\n",
       " 0.7269104281246008,\n",
       " 0.6428220149940083,\n",
       " 0.6789495186550674,\n",
       " 0.6756462152140505,\n",
       " 0.7842417192506911,\n",
       " 0.7600206069655763,\n",
       " 0.7291861499590149,\n",
       " 0.6830013373971754,\n",
       " 0.8552506317882855,\n",
       " 0.5648446983769154,\n",
       " 0.5275727119304842,\n",
       " 0.9192235542558248,\n",
       " 0.8733835347936684,\n",
       " 0.3759166139476257,\n",
       " 0.26230582067216957,\n",
       " 0.6784211516674326,\n",
       " 0.7692125070412477,\n",
       " 0.58547188919391,\n",
       " 0.3973016308873436,\n",
       " 0.6491414446638393,\n",
       " 0.31474362688338786,\n",
       " 0.7751049610605258,\n",
       " 0.6913707523392743,\n",
       " 0.7181438419301747,\n",
       " 0.6551367901100342,\n",
       " 0.448814254294321,\n",
       " 0.6166770062949248,\n",
       " 0.7201458663297875,\n",
       " 0.7088366685667533,\n",
       " 0.26918001194423286,\n",
       " 0.677813946891435,\n",
       " 0.5771666740103331,\n",
       " 0.6011633298487866,\n",
       " 0.736159175950591,\n",
       " 0.6671486444996877,\n",
       " 0.8040902123034182,\n",
       " 0.506188628451708,\n",
       " 0.6016187515504896,\n",
       " 0.5458980483318386,\n",
       " 0.7515037938454826,\n",
       " 0.7670262321310537,\n",
       " 0.8155497728591551,\n",
       " 0.7726171497617483,\n",
       " 0.8672598122321105,\n",
       " 0.4474986927268572,\n",
       " 0.866962201398173,\n",
       " 0.34639172677673247,\n",
       " 0.8817484695257194,\n",
       " 0.5860214312441987,\n",
       " 0.8804696713210545,\n",
       " 0.7327175171663842,\n",
       " 0.9304433747186015,\n",
       " 0.6105319198504224,\n",
       " 0.8333243187675947,\n",
       " 0.3708020578625728,\n",
       " 0.8110653001287664,\n",
       " 0.6527648290347672,\n",
       " 0.9084213937300205,\n",
       " 0.9434293037097268,\n",
       " 0.8996655784299272,\n",
       " 0.8205967889569027,\n",
       " 0.9432477242953885,\n",
       " 0.4935559026539874,\n",
       " 0.8879124278229055,\n",
       " 0.6168507331806783,\n",
       " 0.7974252417770229,\n",
       " 0.7502218199412594,\n",
       " 0.6359117110003205,\n",
       " 0.2352293341130567,\n",
       " 0.8598461833646867,\n",
       " 0.9032568445992389,\n",
       " 0.5566626512872657,\n",
       " 0.8531834340332684,\n",
       " 0.7884462043212346,\n",
       " 0.7236393428857841,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict\n",
    "submit_score.to_csv('predict_result_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict_temp\n",
    "submit_score.to_csv('predict_result_ncnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(train_file,values = 'price', index=['lvl1','lvl2','lvl3'],columns=['type'],aggfunc=[min, max, np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
