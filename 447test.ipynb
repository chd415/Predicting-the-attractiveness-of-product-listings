{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chd415/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sys\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "sns.set(color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                   \n",
    "import nltk                                         \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords                   \n",
    "from nltk.stem import PorterStemmer  \n",
    "LA = np.linalg\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from gensim.models.word2vec import Word2Vec                                  \n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data text\n",
    "def load_data(filename):\n",
    "    load_file = pd.read_csv(filename,delimiter=',', header=0,\n",
    "                        dtype={'name':str, 'lvl1':str, 'lvl2':str, 'lvl3':str, 'descrption':str, 'type':str})\n",
    "    load_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "    load_file.duplicated(subset=None, keep='first')\n",
    "    load_file.set_index('id', inplace = True)\n",
    "    load_file.head()\n",
    "    return load_file\n",
    "#print(len(train_file))\n",
    "def load_label(filename):\n",
    "    load_label = pd.read_csv(filename,delimiter=',', header=0)\n",
    "    load_label.columns = ['id', 'score']\n",
    "    load_label.duplicated(subset=None, keep='first')\n",
    "    load_label.set_index('id', inplace = True)\n",
    "    return load_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def map_mathod(column):\n",
    "    values = []\n",
    "    indexs = []\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for count in range(len(train_file)):\n",
    "        value = train_file.get_value(count+1,column)\n",
    "        if value in values and value != np.nan:\n",
    "            continue\n",
    "        values.append(value)\n",
    "        indexs.append(len(values))\n",
    "    for j in range(len(indexs)):\n",
    "        mapping[values[j]] = indexs[j]\n",
    "    mapping[np.nan] = 0.0\n",
    "    return mapping\n",
    "#train_file['lvl3'] = train_file['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "#mapping_lvl3 = map_mathod('lvl3')\n",
    "#print(mapping_lvl3)\n",
    "'''\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def text_embedding(column,vecsize):\n",
    "    temp_X = column.astype(str)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    for sentence in temp_X: \n",
    "        words = [snow.stem(word) for word in sentence.split(' ') if word not in stopwords.words('english')]   # Stemming and removing stopwords\n",
    "        temp.append(sentence)\n",
    "\n",
    "    count_vect = CountVectorizer(max_features=10000)\n",
    "    bow_data = count_vect.fit_transform(temp)\n",
    "\n",
    "    final_tf = temp\n",
    "    tf_idf = TfidfVectorizer(max_features=10000)\n",
    "    tf_data = tf_idf.fit_transform(final_tf)\n",
    "    w2v_data = temp\n",
    "    splitted = []\n",
    "    for row in w2v_data: \n",
    "        splitted.append([word for word in row.split()])     #splitting words\n",
    "    \n",
    "    train_w2v = Word2Vec(splitted,min_count=1,size=vecsize, workers=4)\n",
    "    \n",
    "    avg_data = []\n",
    "    for row in splitted:\n",
    "        vec = np.zeros(vecsize, dtype=float)\n",
    "        count = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                vec += train_w2v[word]\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "        if (count == 0):\n",
    "            avg_data.append(vec)\n",
    "        else:\n",
    "            avg_data.append(vec/count)\n",
    "\n",
    "        \n",
    "    tf_w_data = []\n",
    "    tf_data = tf_data.toarray()\n",
    "    i = 0\n",
    "    for row in splitted:\n",
    "        vec = [0 for i in range(vecsize)]\n",
    "    \n",
    "        temp_tfidf = []\n",
    "        for val in tf_data[i]:\n",
    "            if val != 0:\n",
    "                temp_tfidf.append(val)\n",
    "    \n",
    "        count = 0\n",
    "        tf_idf_sum = 0\n",
    "        for word in row:\n",
    "            try:\n",
    "                count += 1\n",
    "                tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "                vec += (temp_tfidf[count-1] * train_w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        if (tf_idf_sum == 0):\n",
    "            tf_w_data.append(vec)\n",
    "        else:\n",
    "            tf_w_data.append(vec/tf_idf_sum)\n",
    "        i = i + 1\n",
    "    \n",
    "    return tf_w_data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "temp = load_data('train_data.csv')\n",
    "price_X = temp['price'].as_matrix(columns=None).reshape(1, -1)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(price_X)\n",
    "# outfile = scaler.transform(price_X)\n",
    "\n",
    "# est = KBinsDiscretizer(n_bins=15, encode='onehot-dense', strategy='uniform')\n",
    "# est.fit(price_X)\n",
    "# outfile = est.transform(price_X)\n",
    "\n",
    "transformer = Normalizer(norm='l2').fit(price_X)\n",
    "outfile = transformer.transform(price_X)\n",
    "\n",
    "\n",
    "# temp['price'] = temp['price']\n",
    "# hist = temp['price'].hist(bins=15)\n",
    "\n",
    "# price_X = temp['price'].clip(lower=0.).as_matrix(columns=None).tolist()\n",
    "# price_X.sort()\n",
    "\n",
    "#np.histogram(outfile,bins=10)\n",
    "\n",
    "outfilelist = outfile.tolist()\n",
    "outfilelist.sort()\n",
    "\n",
    "file = open('temp.dat','w')\n",
    "file.writelines([\"%s\\n\" % item  for item in outfilelist])\n",
    "file.close()\n",
    "#print(outfile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test cell\n",
    "\n",
    "temp = pd.read_csv('train_data.csv',delimiter=',', header=0)\n",
    "description_X = temp.descrption.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "#description_X.head()\n",
    "tfidf_n = TfidfVectorizer(max_features=1000,stop_words = 'english')\n",
    "tf_out = tfidf_n.fit_transform(description_X.astype('U')).toarray()\n",
    "#lg_array = csr_matrix(tf_out, dtype=np.int8).toarray()\n",
    "#lg_array = np.vstack( lg_array )\n",
    "    \n",
    "#U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "#assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "print(tf_out.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].str.lower().replace('[^a-zA-Z]+',' ',regex=True)\n",
    "    filename['lvl2'] = filename['lvl2'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['lvl3'] = filename['lvl3'].str.lower().replace('[^\\'\\w]+',' ',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].str.lower()\n",
    "    filename['name'] = filename['name'].str.lower()\n",
    "\n",
    "    '''\n",
    "    \n",
    "    mapping_lvl1 = map_mathod('lvl1')\n",
    "    mapping_lvl2 = map_mathod('lvl2')\n",
    "    mapping_lvl3 = map_mathod('lvl3')\n",
    "    \n",
    "    filename['lvl1'] = filename['lvl1'].map(mapping_lvl1)\n",
    "    filename['lvl2'] = filename['lvl2'].map(mapping_lvl2)\n",
    "    filename['lvl3'] = filename['lvl3'].map(mapping_lvl3)\n",
    "    \n",
    "    '''\n",
    "    #clean up data for lvl 1&2&3 and type\n",
    "    temp =  filename.drop(['price', 'descrption','name'], axis=1)\n",
    "    outfile = MultiColumnLabelEncoder(columns = ['lvl1','lvl2','lvl3','type']).fit_transform(temp.astype(str))\n",
    "#    outfile = outfile.as_matrix(columns=None).tolist()\n",
    "    \n",
    "    \n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(outfile)\n",
    "    outfile = enc.transform(outfile).toarray()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #normalize price\n",
    "#     maxp = filename.price.max()\n",
    "#     valuethred = 500.\n",
    "#     filename['price'] = filename['price'].clip(lower=0.,upper=valuethred).div(valuethred,fill_value=0.)\n",
    "#     hist = filename['price'].hist(bins=10)\n",
    "#     #maxp\n",
    "#     ld = filename.price.as_matrix(columns=None).tolist()\n",
    "\n",
    "    price_X = filename['price'].as_matrix(columns=None).reshape(-1, 1)\n",
    "    transformer = Normalizer(copy=True,norm='l2')\n",
    "    ld = transformer.fit_transform(price_X)\n",
    "    outfile = np.column_stack((outfile,ld))\n",
    "\n",
    "    \n",
    "\n",
    "    #clean up text\n",
    "    description_X = filename.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    count_descrption = description_X.str.count('final').fillna(0).tolist()\n",
    "    outfile = np.column_stack((outfile,count_descrption))\n",
    "    \n",
    "    description_X = description_X.str.lower().replace('final ','',regex=True)\n",
    "    filename['descrption'] = filename['descrption'].replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    descrption_Xstring = pd.Series(filename['descrption'].tolist()).astype(str)\n",
    "    count_wordcount = descrption_Xstring.apply(lambda x: len(x.split(' ')))\n",
    "    count_lettercount = descrption_Xstring.apply(lambda x: len(x))\n",
    "    outfile = np.column_stack((outfile,count_wordcount))\n",
    "    outfile = np.column_stack((outfile,count_lettercount))\n",
    "    \n",
    "#    tfidf_n = HashingVectorizer(n_features=2**20,stop_words = 'english')\n",
    "    #tfidf_n = TfidfVectorizer(max_features=500,stop_words = 'english')\n",
    "    tfidf_n = CountVectorizer(max_features=500,stop_words = 'english')\n",
    "    lg = tfidf_n.fit_transform(description_X.astype('U')).toarray()\n",
    "    lg_array = np.vstack( lg )\n",
    "#    print(lg)\n",
    "    \n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "    s_new = s[:30]\n",
    "    U_new = U[:, :30]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    outfile = np.column_stack((outfile,new_lg))\n",
    "    \n",
    "    \n",
    "    name_X = filename.name.str.lower().replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    filename['name'] = filename['name'].replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "    name_Xstring = pd.Series(filename['name'].tolist()).astype(str)\n",
    "    name_wordcount = name_Xstring.apply(lambda x: len(x.split(' ')))\n",
    "    name_lettercount = name_Xstring.apply(lambda x: len(x))\n",
    "    outfile = np.column_stack((outfile,name_wordcount))\n",
    "    outfile = np.column_stack((outfile,name_lettercount))\n",
    "\n",
    "\n",
    "#    tfidf_f = HashingVectorizer(n_features=2**20,stop_words = 'english')\n",
    "    #tfidf_f = TfidfVectorizer(max_features=500,stop_words = 'english')\n",
    "    tfidf_f = CountVectorizer(max_features=500,stop_words = 'english')\n",
    "    lf = tfidf_f.fit_transform(name_X.astype('U')).toarray()\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "    sf_new = sf[:15]\n",
    "    Uf_new = Uf[:, :15]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    outfile = np.column_stack((outfile,new_lf))\n",
    "    \n",
    "    '''\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(outfile)\n",
    "    outfile = enc.transform(outfile).toarray()\n",
    "    '''\n",
    "\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_file = pd.read_csv('train_data.csv',delimiter=',', header=0, nrows=10)\n",
    "train_file.columns = ['id', 'name','lvl1','lvl2','lvl3','descrption','price','type']\n",
    "description_X = train_file.descrption.str.lower().replace('<li>','final ',regex=True).replace('<.*?>','',regex=True).replace('[^\\w\\s]+',' ',regex=True)\n",
    "count_descrption = description_X.str.count('final')\n",
    "description_X = train_file.descrption.str.lower().replace('final ','',regex=True)\n",
    "print(np.shape(count_descrption))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36283, 305)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = load_data('train_data.csv')\n",
    "test_file = load_data('test_data.csv')\n",
    "combined_file = pd.concat([train_file,test_file])\n",
    "cleaned_train = clean_data(combined_file)\n",
    "train_score = load_label('train_label.csv')\n",
    "np.shape(cleaned_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "s_gra = np.abs(np.gradient(s))\n",
    "slist = s.tolist()\n",
    "file = open('svd_secrption.dat','w')\n",
    "file.writelines([\"%s\\n\" % item  for item in s_gra])\n",
    "file.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_ylabel('Eign Values', color=color,fontweight='bold')\n",
    "ax1.plot(slist, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Gradient of Eign Value', color=color,fontweight='bold')  # we already handled the x-label with ax1\n",
    "ax2.plot(s_gra, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "plt.grid()\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('Eign_Values_descrption.png', dpi=fig.dpi)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ..., -0.00184826,\n",
       "         0.00700171, -0.00469088],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  0.25687928,\n",
       "        -0.2348867 , -0.04022385],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  1.17822738,\n",
       "        -0.37884069, -0.1530413 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.01015433,\n",
       "        -0.00546545,  0.01432396],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.037503  ,\n",
       "        -0.12822085, -0.01662279],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.03782297,\n",
       "        -0.08829066, -0.1051121 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# test cell\n",
    "cleaned_train['descrption'] = temp['descrption']\n",
    "'''\n",
    "cleaned_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pca = PCA(n_components=100, svd_solver='full')\n",
    "cleaned_train_temp = pca.fit_transform(cleaned_train)\n",
    "\n",
    "cleaned_train_min = np.min(cleaned_train_temp)\n",
    "cleaned_train_out = cleaned_train_temp - (cleaned_train_min)*np.ones_like(cleaned_train_temp.size)\n",
    "print(cleaned_train_out)\n",
    "cleaned_train = cleaned_train_out\n",
    "print(cleaned_train.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36283, 305)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "def rearrange(cleaned_data):\n",
    "    la = cleaned_data.lvl1.as_matrix(columns=None).tolist()\n",
    "    lb = cleaned_data.lvl2.as_matrix(columns=None).tolist()\n",
    "    lc = cleaned_data.lvl3.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = la\n",
    "    X = np.column_stack((X,lb))\n",
    "    X = np.column_stack((X,lc))\n",
    "\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    \n",
    "    ld = cleaned_data.price.as_matrix(columns=None).tolist()\n",
    "    le = cleaned_data.type.as_matrix(columns=None).tolist()\n",
    "\n",
    "    X = np.column_stack((X,ld))\n",
    "    X = np.column_stack((X,le))\n",
    "\n",
    "   \n",
    "    lf = cleaned_data.name.as_matrix(columns=None).tolist()\n",
    "    lg = cleaned_data.descrption.as_matrix(columns=None).tolist()\n",
    "    lg_array = np.vstack( lg )\n",
    "    lf_array = np.vstack( lf )\n",
    "\n",
    "    U, s, Vh = LA.svd(lg_array, full_matrices=False)\n",
    "    assert np.allclose(lg_array, np.dot(U, np.dot(np.diag(s), Vh)))\n",
    "\n",
    "# only use U \\cdot s\n",
    "    \n",
    "#    s[8:] = 0.\n",
    "#    new_lg = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "    s_new = s[:5]\n",
    "    U_new = U[:, :5]\n",
    "    new_lg = np.dot(U_new, np.diag(s_new))\n",
    "    lg_min = np.min(new_lg)\n",
    "    lg_out = new_lg  - (lg_min-2)*np.ones_like(new_lg.size)\n",
    "\n",
    "    Uf, sf, Vhf = LA.svd(lf_array, full_matrices=False)\n",
    "    assert np.allclose(lf_array, np.dot(Uf, np.dot(np.diag(sf), Vhf)))\n",
    "\n",
    "#    sf[5:] = 0.\n",
    "#    new_lf = np.dot(Uf, np.dot(np.diag(sf), Vhf))\n",
    "    sf_new = sf[:5]\n",
    "    Uf_new = Uf[:, :5]\n",
    "    new_lf = np.dot(Uf_new, np.diag(sf_new))\n",
    "    lf_min = np.min(new_lf)\n",
    "    lf_out = new_lf  - (lf_min-2)*np.ones_like(new_lf.size)\n",
    "\n",
    "    \n",
    "    X = np.column_stack((X,lf_out))\n",
    "    X = np.column_stack((X,lg_out))\n",
    "    X = X.tolist()\n",
    "   \n",
    "    return X,lf_min,lg_min\n",
    "'''\n",
    "    \n",
    "    #print(len(X))\n",
    "X = cleaned_train#rearrange(cleaned_train)\n",
    "w,b = np.shape(np.array(X))\n",
    "print(np.shape(np.array(X)))\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18141\n",
      "(18141, 305)\n",
      "(18142, 305)\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_train[:18141]\n",
    "XX = cleaned_train[18141:]\n",
    "Y = train_score.score.as_matrix(columns=None).tolist()\n",
    "print(np.size(Y))\n",
    "print(np.shape(X))\n",
    "print(np.shape(XX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    "#maxlen = b\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=maxlen, dtype='float32')\n",
    "#X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, dtype='float32')\n",
    "print(X_train[1400].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier  \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Classifier\n",
    "def linear_regression_classifier(train_x, train_y):\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    param_grid = {'alpha': [math.pow(10,-i) for i in range(11)]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = MultinomialNB(alpha = best_parameters['alpha'])  \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    param_grid = {'n_neighbors': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = best_parameters['n_neighbors'], algorithm='kd_tree')\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    return bagging\n",
    " \n",
    " \n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(1,21))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    " \n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    bagging = BaggingClassifier(model, max_samples=0.5, max_features=1 )\n",
    "    bagging.fit(train_x, train_y)\n",
    "    \n",
    "    return bagging\n",
    " \n",
    " \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {'n_estimators': list(range(100,300,10))}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators = best_parameters['n_estimators'])\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    " \n",
    "# SVM Classifier using cross validation\n",
    "def svm_cross_validation(train_x, train_y):\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    #for para, val in best_parameters.items():\n",
    "        #print para, val\n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "def feature_select(x,y):\n",
    "    clf = ExtraTreesClassifier()\n",
    "    clf = clf.fit(x, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "******************* KNN ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   16.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 16.141019s!\n",
      "accuracy: 68.92%\n",
      "loss: 10.74\n",
      "******************* LR ********************\n",
      "training took 0.080522s!\n",
      "accuracy: 78.23%\n",
      "loss: 7.52\n",
      "******************* RF ********************\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   15.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 16.412083s!\n",
      "accuracy: 79.72%\n",
      "loss: 7.00\n",
      "******************* DT ********************\n",
      "training took 0.329029s!\n",
      "accuracy: 68.45%\n",
      "loss: 10.90\n"
     ]
    }
   ],
   "source": [
    "# just for my own record\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "    \n",
    "    test_classifiers = ['KNN','LR','RF','DT']    \n",
    "    classifiers = {\n",
    "                   'KNN':knn_classifier,\n",
    "                    'LR':logistic_regression_classifier,\n",
    "                    'RF':random_forest_classifier,\n",
    "                    'DT':decision_tree_classifier,\n",
    "    }\n",
    "    \n",
    "    '''\n",
    "    test_classifiers = ['SVC']    \n",
    "    classifiers = {'SVC':svm_cross_validation}\n",
    "    ''' \n",
    "    \n",
    "    print('reading training and testing data...')    \n",
    "\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_validation = select_model.transform(X_validation)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    '''\n",
    "    start_time = time.time()    \n",
    "    model = classifiers[classifier](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    regressor = DecisionTreeRegressor()\n",
    "    regressor.fit(train_x, train_y)\n",
    "    predict = model.predict(X_validation)\n",
    "    '''\n",
    "\n",
    "        \n",
    "    for classifier in test_classifiers:    \n",
    "        print('******************* %s ********************' % classifier)    \n",
    "        start_time = time.time()    \n",
    "        model = classifiers[classifier](X_train, y_train)   \n",
    "        print('training took %fs!' % (time.time() - start_time))    \n",
    "        predict = model.predict(X_validation)\n",
    "\n",
    "        precision = metrics.precision_score(y_validation, predict)    \n",
    "        recall = metrics.recall_score(y_validation, predict)    \n",
    "        accuracy = metrics.accuracy_score(y_validation, predict)    \n",
    "        print('accuracy: %.2f%%' % (100 * accuracy))\n",
    "        logloss = metrics.log_loss(y_validation, predict)\n",
    "        print('loss: %.2f' % (logloss))\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "#        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        , ..., -0.13150647,\n",
       "        -0.2752178 , -0.18125084],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.09084801,\n",
       "         0.06534211, -0.06657633],\n",
       "       [ 0.        ,  1.        ,  0.        , ..., -0.05854416,\n",
       "        -0.09125824, -0.03681251],\n",
       "       ...,\n",
       "       [ 1.        ,  1.        ,  0.        , ...,  0.80953534,\n",
       "        -0.00779874, -0.7293216 ],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.0056889 ,\n",
       "        -0.05862812,  0.13158637],\n",
       "       [ 0.        ,  0.        ,  1.        , ..., -0.02963569,\n",
       "         0.0087225 ,  0.02193457]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "#X_train, y_train = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "14512 train sequences\n",
      "3629 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (14512, 305)\n",
      "x_test shape: (3629, 305)\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#test cnn model\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# Embedding\n",
    "max_features = 5000\n",
    "maxlen = b\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 32\n",
    "pool_size = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 5\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "X_train = np.asarray(np.abs(X_train))\n",
    "X_validation = np.asarray(np.abs(X_validation))\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_validation), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_validation = sequence.pad_sequences(X_validation, maxlen=maxlen, padding='post')\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_validation.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(max_features, embedding_size,input_length=maxlen))\n",
    "#model.add(Dense(32, activation='relu', input_dim=100))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "cnnmodel.add(Dense(32, activation='relu', input_dim=64))\n",
    "cnnmodel.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=pool_size))\n",
    "cnnmodel.add(LSTM(lstm_output_size))\n",
    "cnnmodel.add(Dense(1))\n",
    "cnnmodel.add(Activation('sigmoid'))\n",
    "\n",
    "cnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing data...\n",
      "training took 1.012821s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   43.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 49.043003s!\n",
      "predict finished\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 838.457982s!\n",
      "predict finished\n",
      "training took 840.440748s!\n",
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    thresh = 0.5    \n",
    "     \n",
    "    test_classifiers = ['LR','RF','GBC','DT']    \n",
    "    classifiers = {\n",
    "                   'LR':logistic_regression_classifier,\n",
    "                   'RF':random_forest_classifier,\n",
    "                   'GBC':gradient_boosting_classifier,\n",
    "                   'DT':decision_tree_classifier\n",
    "    }\n",
    "        \n",
    "    print('reading training and testing data...')    \n",
    "\n",
    "    select_model = feature_select(X_train, y_train)\n",
    "    X_train = select_model.transform(X_train)\n",
    "    X_test = select_model.transform(X_test)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = classifiers['LR'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_lr = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['RF'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))\n",
    "    regr = RandomForestRegressor(max_depth=2, max_features=1)\n",
    "    regr.fit(X_train, y_train)\n",
    "    Y_predict_rf = regr.predict(X_test)\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['GBC'](X_train, y_train)   \n",
    "    print('training took %fs!' % (time.time() - start_time))    \n",
    "    Y_predict_gbc = model.predict_proba(X_test)[:,1]\n",
    "    print('predict finished')\n",
    "    \n",
    "    model = classifiers['DT'](X_train, y_train)  \n",
    "    print('training took %fs!' % (time.time() - start_time))\n",
    "    regressor = DecisionTreeRegressor(max_depth=2) \n",
    "    regressor.fit(X_train, y_train)\n",
    "    Y_predict_dt = regressor.predict(X_test)\n",
    "    print('predict finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[124,266] = -2 is not in [0, 5000)\n\t [[Node: embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast, training/Adam/gradients/embedding_1/embedding_lookup_grad/concat/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0a2cecb7ca13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcnnmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mY_predict_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnnmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[124,266] = -2 is not in [0, 5000)\n\t [[Node: embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast, training/Adam/gradients/embedding_1/embedding_lookup_grad/concat/axis)]]"
     ]
    }
   ],
   "source": [
    "#cnn test\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "cnnmodel.fit(X_train, y_train, batch_size=128, epochs=8)\n",
    "\n",
    "Y_predict_cnn = cnnmodel.predict(X_test, verbose=0)\n",
    "Y_predict_cnn = np.squeeze(Y_predict_cnn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18141/18141 [==============================] - 1s 58us/step - loss: 0.6471 - acc: 0.6958\n",
      "Epoch 2/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.5038 - acc: 0.7681\n",
      "Epoch 3/20\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.4774 - acc: 0.7781\n",
      "Epoch 4/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.4535 - acc: 0.7962\n",
      "Epoch 5/20\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.4408 - acc: 0.8025\n",
      "Epoch 6/20\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.4482 - acc: 0.7946\n",
      "Epoch 7/20\n",
      "18141/18141 [==============================] - 0s 25us/step - loss: 0.4371 - acc: 0.8002\n",
      "Epoch 8/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.4372 - acc: 0.8034\n",
      "Epoch 9/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4265 - acc: 0.8072\n",
      "Epoch 10/20\n",
      "18141/18141 [==============================] - 1s 31us/step - loss: 0.4259 - acc: 0.8083\n",
      "Epoch 11/20\n",
      "18141/18141 [==============================] - 1s 30us/step - loss: 0.4286 - acc: 0.8065\n",
      "Epoch 12/20\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.4256 - acc: 0.8065\n",
      "Epoch 13/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4286 - acc: 0.8026\n",
      "Epoch 14/20\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.4174 - acc: 0.8118\n",
      "Epoch 15/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.4128 - acc: 0.8137\n",
      "Epoch 16/20\n",
      "18141/18141 [==============================] - 0s 26us/step - loss: 0.4197 - acc: 0.8087\n",
      "Epoch 17/20\n",
      "18141/18141 [==============================] - 1s 31us/step - loss: 0.4213 - acc: 0.8105\n",
      "Epoch 18/20\n",
      "18141/18141 [==============================] - 1s 29us/step - loss: 0.4230 - acc: 0.8065\n",
      "Epoch 19/20\n",
      "18141/18141 [==============================] - 0s 27us/step - loss: 0.4156 - acc: 0.8118\n",
      "Epoch 20/20\n",
      "18141/18141 [==============================] - 0s 21us/step - loss: 0.4115 - acc: 0.8150\n",
      "18141/18141 [==============================] - 1s 28us/step\n",
      "\n",
      "acc: 81.11%\n"
     ]
    }
   ],
   "source": [
    "#nn train\n",
    "X_train = cleaned_train[:18141]\n",
    "X_test = cleaned_train[18141:]\n",
    "y_train = train_score.score.as_matrix(columns=None).tolist()\n",
    "X_array = np.asarray(X_train)\n",
    "Y_array = np.asarray(y_train)\n",
    "Xtest_array = np.asarray(X_test) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_array\n",
    "Y = Y_array\n",
    "# create model\n",
    "nnmodel = Sequential()\n",
    "nnmodel.add(Dense(100, input_dim=b, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(100, activation='relu'))\n",
    "nnmodel.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "nnmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "nnmodel.fit(X, Y, epochs=20, batch_size=150)\n",
    "# evaluate the model\n",
    "scores = nnmodel.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (nnmodel.metrics_names[1], scores[1]*100))\n",
    "Y_predict_nn = nnmodel.predict(Xtest_array, verbose=0)\n",
    "Y_predict_nn = np.squeeze(Y_predict_nn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94319683, 0.883781  , 0.42291856, ..., 0.81127375, 0.3868661 ,\n",
       "       0.9203107 ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_predict_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a03559446bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_predict_cnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_predict_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "Y_predict_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83165771, 0.65365511, 0.13319898, ..., 0.62160562, 0.31380171,\n",
       "       0.89526246])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72274931, 0.67831972, 0.68261472, ..., 0.69777779, 0.67477614,\n",
       "       0.67595352])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93846154, 0.80769231, 0.48461538, ..., 0.69230769, 0.46153846,\n",
       "       0.74615385])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68045831, 0.5925335 , 0.5925335 , ..., 0.88458665, 0.26255507,\n",
       "       0.88458665])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_dt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict.append(( Y_predict_rf[i] + Y_predict_lr[i] + Y_predict_gbc[i] + Y_predict_nn[i] ) / 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8590163466297449,\n",
       " 0.7558620363130346,\n",
       " 0.43083691027233806,\n",
       " 0.8804154103492244,\n",
       " 0.8685857363349854,\n",
       " 0.803982722191195,\n",
       " 0.3702047910112911,\n",
       " 0.8821336861563667,\n",
       " 0.7060969727764501,\n",
       " 0.8419911805000246,\n",
       " 0.845600119417621,\n",
       " 0.5769959365445285,\n",
       " 0.6638917911539857,\n",
       " 0.49603111755952123,\n",
       " 0.9041633913442157,\n",
       " 0.4669674019164296,\n",
       " 0.8494922424850394,\n",
       " 0.2939765106866558,\n",
       " 0.8479966208965481,\n",
       " 0.7023654367491607,\n",
       " 0.8926189427318618,\n",
       " 0.8075674221410716,\n",
       " 0.8900378071554869,\n",
       " 0.803539955430323,\n",
       " 0.8879795677536544,\n",
       " 0.7362218941551693,\n",
       " 0.6231352368319243,\n",
       " 0.9084247127671244,\n",
       " 0.8264634977099837,\n",
       " 0.8813748464462534,\n",
       " 0.8996368760280984,\n",
       " 0.7652600087000799,\n",
       " 0.8697013559822653,\n",
       " 0.790142160404932,\n",
       " 0.711038400553345,\n",
       " 0.6474023492348753,\n",
       " 0.5300532425972011,\n",
       " 0.8258774456968576,\n",
       " 0.558745779128343,\n",
       " 0.8427897542676432,\n",
       " 0.7411771736784677,\n",
       " 0.8997632682046338,\n",
       " 0.8622091052427368,\n",
       " 0.7542736762625616,\n",
       " 0.5775285248246292,\n",
       " 0.8842282264033381,\n",
       " 0.6966422561149436,\n",
       " 0.3417506513557487,\n",
       " 0.9189027849822644,\n",
       " 0.4135477701917783,\n",
       " 0.7657575685027138,\n",
       " 0.898883592222364,\n",
       " 0.6496688212087236,\n",
       " 0.734460372218605,\n",
       " 0.21150066224043962,\n",
       " 0.8281442089099653,\n",
       " 0.7380633828663,\n",
       " 0.7848812951852485,\n",
       " 0.5145912140408346,\n",
       " 0.512394291205834,\n",
       " 0.550816355246224,\n",
       " 0.8136487421311123,\n",
       " 0.44945517903170606,\n",
       " 0.5552925393013903,\n",
       " 0.7295208229770244,\n",
       " 0.8773627750904135,\n",
       " 0.7933963183328451,\n",
       " 0.8160796861763309,\n",
       " 0.48894910814544096,\n",
       " 0.6468240031271933,\n",
       " 0.35778040596448885,\n",
       " 0.8313243801384032,\n",
       " 0.7931285281550297,\n",
       " 0.7044267167709453,\n",
       " 0.5477129281039231,\n",
       " 0.5421974576638187,\n",
       " 0.7212015474534458,\n",
       " 0.8446088966294649,\n",
       " 0.29149403590201617,\n",
       " 0.6600562616739704,\n",
       " 0.3505106657265116,\n",
       " 0.4066421874572292,\n",
       " 0.19738912143174775,\n",
       " 0.8529278215033018,\n",
       " 0.7775386642962192,\n",
       " 0.5705103638762559,\n",
       " 0.8081380138399279,\n",
       " 0.370917792777952,\n",
       " 0.8794927856935513,\n",
       " 0.8055837400342715,\n",
       " 0.7161865463820758,\n",
       " 0.8057870410675854,\n",
       " 0.7365237384076594,\n",
       " 0.7875260265065283,\n",
       " 0.8981934645702702,\n",
       " 0.652863482791171,\n",
       " 0.8090611357450957,\n",
       " 0.8734067859066554,\n",
       " 0.8817945939857628,\n",
       " 0.8576442421715101,\n",
       " 0.6833804822776877,\n",
       " 0.8775795434815895,\n",
       " 0.8943559317679888,\n",
       " 0.8282439395122543,\n",
       " 0.8019616752661937,\n",
       " 0.6323696153016174,\n",
       " 0.8940319725841948,\n",
       " 0.9099805154199256,\n",
       " 0.7654223435334724,\n",
       " 0.7865312451134415,\n",
       " 0.7826120707616485,\n",
       " 0.7277321028235085,\n",
       " 0.8715721239386993,\n",
       " 0.8031721342705751,\n",
       " 0.7631115062168083,\n",
       " 0.3089247108461081,\n",
       " 0.6032312583845628,\n",
       " 0.4885629725769439,\n",
       " 0.3509741233341676,\n",
       " 0.6714349741189909,\n",
       " 0.773340963436779,\n",
       " 0.862647750204431,\n",
       " 0.6498305639471129,\n",
       " 0.25891016797215205,\n",
       " 0.8842406971759003,\n",
       " 0.8219472593434739,\n",
       " 0.8981973078438572,\n",
       " 0.4143611835347917,\n",
       " 0.7024080773892655,\n",
       " 0.8975945006581019,\n",
       " 0.9188804933090453,\n",
       " 0.8504695419699698,\n",
       " 0.7823875825946726,\n",
       " 0.30827977761144826,\n",
       " 0.8806312407838606,\n",
       " 0.8554152925430525,\n",
       " 0.8476132211868839,\n",
       " 0.858855454053746,\n",
       " 0.5858820048781532,\n",
       " 0.5666557588086827,\n",
       " 0.7941171442741682,\n",
       " 0.8464460420925133,\n",
       " 0.6278889567296623,\n",
       " 0.8603217599920722,\n",
       " 0.587985685582214,\n",
       " 0.5817951022992252,\n",
       " 0.8730241536076759,\n",
       " 0.7911169490561741,\n",
       " 0.8720653965876718,\n",
       " 0.8500471305495078,\n",
       " 0.8371379296609294,\n",
       " 0.578803628901436,\n",
       " 0.6655067545568797,\n",
       " 0.9004074040293747,\n",
       " 0.6596579752389755,\n",
       " 0.299895947588337,\n",
       " 0.8596223639325503,\n",
       " 0.5878361494641741,\n",
       " 0.7386717274404618,\n",
       " 0.8671204020814223,\n",
       " 0.6303629197351684,\n",
       " 0.6175900868881262,\n",
       " 0.3899376293323695,\n",
       " 0.6134487550913161,\n",
       " 0.6411025850394056,\n",
       " 0.27569979017190416,\n",
       " 0.7488307019996931,\n",
       " 0.4128850668911403,\n",
       " 0.5040065589046462,\n",
       " 0.6033152257168151,\n",
       " 0.6837716668306333,\n",
       " 0.3898556240741867,\n",
       " 0.4510140307962079,\n",
       " 0.7826456748298657,\n",
       " 0.9102061182618336,\n",
       " 0.8039653984020487,\n",
       " 0.5859270707236103,\n",
       " 0.9079414873661373,\n",
       " 0.8734305398844509,\n",
       " 0.3545243433013574,\n",
       " 0.44374942689033986,\n",
       " 0.6421355845702641,\n",
       " 0.8496943746892012,\n",
       " 0.8841335085855337,\n",
       " 0.7352885270864791,\n",
       " 0.6357950216340855,\n",
       " 0.6932608516000656,\n",
       " 0.6034649439989945,\n",
       " 0.7584942685547179,\n",
       " 0.3732344634723119,\n",
       " 0.2714473421435172,\n",
       " 0.8704948475577932,\n",
       " 0.8177551454802058,\n",
       " 0.8851920973853578,\n",
       " 0.4029053995884772,\n",
       " 0.8889892881881298,\n",
       " 0.8461704375286194,\n",
       " 0.8435573011987059,\n",
       " 0.8783957303095844,\n",
       " 0.8074697036243274,\n",
       " 0.6095960283506472,\n",
       " 0.8969986373924052,\n",
       " 0.8347249697501277,\n",
       " 0.875021208174503,\n",
       " 0.7116357794019854,\n",
       " 0.4583222088622778,\n",
       " 0.8407678816119137,\n",
       " 0.46222446379964655,\n",
       " 0.7082922152147508,\n",
       " 0.25728161457393484,\n",
       " 0.6938879910952712,\n",
       " 0.6127067852439638,\n",
       " 0.7625169289905399,\n",
       " 0.6336305064512006,\n",
       " 0.7078948035435714,\n",
       " 0.6088198843907329,\n",
       " 0.558084623644065,\n",
       " 0.5977843203798439,\n",
       " 0.9086226911392422,\n",
       " 0.7730716651278227,\n",
       " 0.7072593694814541,\n",
       " 0.6408804792458235,\n",
       " 0.4779593384226543,\n",
       " 0.6832396340040489,\n",
       " 0.7691903620049683,\n",
       " 0.885332013064652,\n",
       " 0.7625703782922931,\n",
       " 0.743785517679981,\n",
       " 0.879952594961723,\n",
       " 0.790717071994432,\n",
       " 0.7636950799139868,\n",
       " 0.26000457907019114,\n",
       " 0.8713600509824424,\n",
       " 0.2417699223607854,\n",
       " 0.8760863204913866,\n",
       " 0.654471262134496,\n",
       " 0.3249889157225686,\n",
       " 0.5189653734210079,\n",
       " 0.7149363693811668,\n",
       " 0.5778839338492661,\n",
       " 0.8074553352373551,\n",
       " 0.8035374122945703,\n",
       " 0.7365788982441883,\n",
       " 0.400907425319874,\n",
       " 0.4128572850252362,\n",
       " 0.6854667075125684,\n",
       " 0.7425251252789465,\n",
       " 0.6885960823481445,\n",
       " 0.7705381041325051,\n",
       " 0.8929533136573953,\n",
       " 0.7984441603572209,\n",
       " 0.23099338279568982,\n",
       " 0.8994915350723482,\n",
       " 0.8345179651428591,\n",
       " 0.8525530868665316,\n",
       " 0.8703023574608602,\n",
       " 0.863728797199464,\n",
       " 0.7284758220607604,\n",
       " 0.7607814138376259,\n",
       " 0.2911730843087823,\n",
       " 0.9048593496487216,\n",
       " 0.27834045139132557,\n",
       " 0.6768765161363257,\n",
       " 0.901906728579734,\n",
       " 0.8108576427226833,\n",
       " 0.3751849129490263,\n",
       " 0.8811125703521901,\n",
       " 0.879117572486503,\n",
       " 0.8504299124656403,\n",
       " 0.891996527939729,\n",
       " 0.3810757160260852,\n",
       " 0.33993138083649854,\n",
       " 0.8599406007367172,\n",
       " 0.9008095200198838,\n",
       " 0.8705900486291838,\n",
       " 0.8969298989686588,\n",
       " 0.9049700304915509,\n",
       " 0.4755751346223868,\n",
       " 0.890762966274574,\n",
       " 0.33063387140605927,\n",
       " 0.8803820157388852,\n",
       " 0.6051826046544875,\n",
       " 0.896099251296981,\n",
       " 0.8911837493422107,\n",
       " 0.6744272741254972,\n",
       " 0.6726548714136232,\n",
       " 0.6198205830031313,\n",
       " 0.8359404581526076,\n",
       " 0.7721031240318285,\n",
       " 0.8365859705365793,\n",
       " 0.43085250038896195,\n",
       " 0.5913719118981571,\n",
       " 0.42785038320840524,\n",
       " 0.8466883399590427,\n",
       " 0.5093391707359234,\n",
       " 0.5567740084720756,\n",
       " 0.3104423203752059,\n",
       " 0.8600232490053301,\n",
       " 0.5319547184754353,\n",
       " 0.4261403730700689,\n",
       " 0.22344935014234218,\n",
       " 0.8911113755672648,\n",
       " 0.8847031252053552,\n",
       " 0.7847179295299522,\n",
       " 0.852238475375686,\n",
       " 0.886198296816661,\n",
       " 0.8906120726585988,\n",
       " 0.7097955660388322,\n",
       " 0.763209713338419,\n",
       " 0.8848129584135833,\n",
       " 0.7799327191712775,\n",
       " 0.8826344678117831,\n",
       " 0.8995725638038549,\n",
       " 0.9081451052427384,\n",
       " 0.4718953596501809,\n",
       " 0.8874968129595099,\n",
       " 0.8413954309160008,\n",
       " 0.5588388688023518,\n",
       " 0.9009690748254467,\n",
       " 0.8688910907366861,\n",
       " 0.8820240115889988,\n",
       " 0.7528618623083405,\n",
       " 0.8383284260913773,\n",
       " 0.5569485742961348,\n",
       " 0.8095379720732202,\n",
       " 0.3705186236280998,\n",
       " 0.8498597016723655,\n",
       " 0.8771802365657607,\n",
       " 0.5453547880215717,\n",
       " 0.842947329159346,\n",
       " 0.41526053930425727,\n",
       " 0.7674995942474752,\n",
       " 0.7341767938700705,\n",
       " 0.7745205595141501,\n",
       " 0.6717238156905527,\n",
       " 0.21331295561707073,\n",
       " 0.28674369473335143,\n",
       " 0.7100805544653572,\n",
       " 0.31233943435519973,\n",
       " 0.5278246307775762,\n",
       " 0.6803871905729546,\n",
       " 0.6225230819370644,\n",
       " 0.8476185003790248,\n",
       " 0.6656751816528953,\n",
       " 0.8953160827319027,\n",
       " 0.26156051052504703,\n",
       " 0.8747529142792851,\n",
       " 0.47958801090483955,\n",
       " 0.8852338924857653,\n",
       " 0.44936516722618636,\n",
       " 0.6821288463759473,\n",
       " 0.5382997378235101,\n",
       " 0.3234628724772417,\n",
       " 0.9179995991482426,\n",
       " 0.6478659277106613,\n",
       " 0.6636435956020653,\n",
       " 0.8424563472416328,\n",
       " 0.6945152169800861,\n",
       " 0.7680936140619617,\n",
       " 0.8647774003578236,\n",
       " 0.7705853266944355,\n",
       " 0.8765327735403836,\n",
       " 0.629361699072981,\n",
       " 0.9001238072525686,\n",
       " 0.6083842840723839,\n",
       " 0.28075294270408563,\n",
       " 0.27015606237084533,\n",
       " 0.7473082818533899,\n",
       " 0.4032950800403755,\n",
       " 0.7900817483794913,\n",
       " 0.9059178156525404,\n",
       " 0.8971822604488745,\n",
       " 0.5612319487313343,\n",
       " 0.9062422608582494,\n",
       " 0.6805218131051458,\n",
       " 0.723000002550783,\n",
       " 0.7675771944606444,\n",
       " 0.8497944879320548,\n",
       " 0.6797353714990702,\n",
       " 0.595278677105308,\n",
       " 0.6845455094224213,\n",
       " 0.9132131483520634,\n",
       " 0.8097337256638336,\n",
       " 0.6152643601800291,\n",
       " 0.9042524119160824,\n",
       " 0.8627267507050392,\n",
       " 0.59515249026449,\n",
       " 0.7675471517839976,\n",
       " 0.37850437443749396,\n",
       " 0.8939106435760439,\n",
       " 0.1914761442145319,\n",
       " 0.8921873193233318,\n",
       " 0.704878942619177,\n",
       " 0.552195834585553,\n",
       " 0.8638217163878754,\n",
       " 0.6437112405389483,\n",
       " 0.9081281293224165,\n",
       " 0.9200702826549424,\n",
       " 0.834860785994396,\n",
       " 0.8638764817162838,\n",
       " 0.16772957855173384,\n",
       " 0.5542134147773117,\n",
       " 0.5884376827564859,\n",
       " 0.7797677252514089,\n",
       " 0.6073726786419362,\n",
       " 0.8407888356215064,\n",
       " 0.7330489439512782,\n",
       " 0.5940237911642867,\n",
       " 0.880701467018824,\n",
       " 0.8734683687114582,\n",
       " 0.6614880266700459,\n",
       " 0.8815823762236983,\n",
       " 0.7640678976162273,\n",
       " 0.344796366431754,\n",
       " 0.5351370395308195,\n",
       " 0.18422690800438282,\n",
       " 0.17794713859096595,\n",
       " 0.8135696497397399,\n",
       " 0.7954485355277539,\n",
       " 0.749540295744089,\n",
       " 0.849227513353662,\n",
       " 0.7436451952349796,\n",
       " 0.8922017038353816,\n",
       " 0.7863934891431634,\n",
       " 0.8384630859410739,\n",
       " 0.4395489113221678,\n",
       " 0.6223157125374479,\n",
       " 0.42448221798627706,\n",
       " 0.4517661519880035,\n",
       " 0.9122387270010565,\n",
       " 0.7245907917923718,\n",
       " 0.8965976169589431,\n",
       " 0.8215840686341975,\n",
       " 0.8985339115046954,\n",
       " 0.2681275512231872,\n",
       " 0.8959578325893838,\n",
       " 0.7855068377222165,\n",
       " 0.711863822000051,\n",
       " 0.48351335729874534,\n",
       " 0.8863502156257959,\n",
       " 0.8996807163194107,\n",
       " 0.5866467244945941,\n",
       " 0.8495538736415702,\n",
       " 0.7577123134552897,\n",
       " 0.20758131547842368,\n",
       " 0.5805347577275323,\n",
       " 0.5252651610597079,\n",
       " 0.45497737992032794,\n",
       " 0.8226422931719848,\n",
       " 0.8220222495885379,\n",
       " 0.5061014053656165,\n",
       " 0.6024870659234345,\n",
       " 0.8369629297725563,\n",
       " 0.8552175087797032,\n",
       " 0.7516958460064631,\n",
       " 0.8869921387048185,\n",
       " 0.7956304331778811,\n",
       " 0.6436931896550786,\n",
       " 0.3981734727990763,\n",
       " 0.703114125233008,\n",
       " 0.297875264248423,\n",
       " 0.8171801460306765,\n",
       " 0.8791991473047651,\n",
       " 0.5163030926031292,\n",
       " 0.6709503257493619,\n",
       " 0.27104346279684777,\n",
       " 0.7671674461805599,\n",
       " 0.9127076897950253,\n",
       " 0.4548589530061782,\n",
       " 0.852957025139965,\n",
       " 0.5727749280483492,\n",
       " 0.29863259113007623,\n",
       " 0.4410920209995107,\n",
       " 0.6897423403851523,\n",
       " 0.8617402323907062,\n",
       " 0.6117195336349021,\n",
       " 0.5569359721439313,\n",
       " 0.5977099237083131,\n",
       " 0.5082892982245131,\n",
       " 0.8759222408341605,\n",
       " 0.8654985987387229,\n",
       " 0.6990544399026709,\n",
       " 0.8866068707133649,\n",
       " 0.888535203390185,\n",
       " 0.8860604661831719,\n",
       " 0.5908200860102422,\n",
       " 0.9040700684622902,\n",
       " 0.8688193174585894,\n",
       " 0.9020379639488262,\n",
       " 0.7739119230821682,\n",
       " 0.896191789883201,\n",
       " 0.8792678481388233,\n",
       " 0.45787138415160655,\n",
       " 0.7374025843705461,\n",
       " 0.7375258341631424,\n",
       " 0.7926047775860499,\n",
       " 0.8400187295929998,\n",
       " 0.7763941524021145,\n",
       " 0.609637231801927,\n",
       " 0.5229226344689986,\n",
       " 0.5010851470359716,\n",
       " 0.33041631536424676,\n",
       " 0.9018772473873018,\n",
       " 0.7268184361565889,\n",
       " 0.8952515659773732,\n",
       " 0.717738215927388,\n",
       " 0.9129042234863342,\n",
       " 0.1653561339293882,\n",
       " 0.3614861467541192,\n",
       " 0.6395590171378615,\n",
       " 0.759091463749554,\n",
       " 0.6320804384421097,\n",
       " 0.6074798333210873,\n",
       " 0.3892547160923624,\n",
       " 0.4975310586101854,\n",
       " 0.8098945154456053,\n",
       " 0.36850113761372866,\n",
       " 0.2376801528172859,\n",
       " 0.502964028576389,\n",
       " 0.6378197165402619,\n",
       " 0.39888782873244877,\n",
       " 0.7758154651129963,\n",
       " 0.5049819144669829,\n",
       " 0.8723467838629176,\n",
       " 0.8174248791198573,\n",
       " 0.520376798903358,\n",
       " 0.23333107923854582,\n",
       " 0.8446768544583011,\n",
       " 0.8037228470615291,\n",
       " 0.4018229992925042,\n",
       " 0.777919575987162,\n",
       " 0.8956257974632755,\n",
       " 0.6287965350580118,\n",
       " 0.8966056030763856,\n",
       " 0.8991678646665855,\n",
       " 0.7353616551097286,\n",
       " 0.8867663689839649,\n",
       " 0.8504738344679024,\n",
       " 0.8230328955483801,\n",
       " 0.8640721895141545,\n",
       " 0.783067851302016,\n",
       " 0.8908543355138215,\n",
       " 0.8686649882364023,\n",
       " 0.8519552414959486,\n",
       " 0.7731084146026705,\n",
       " 0.4622096828785329,\n",
       " 0.6228464065656171,\n",
       " 0.8628995484221569,\n",
       " 0.8907942372105558,\n",
       " 0.7227892432066789,\n",
       " 0.86788471379322,\n",
       " 0.8407972983876336,\n",
       " 0.7490178663237823,\n",
       " 0.7870184611108446,\n",
       " 0.8287867983590582,\n",
       " 0.9108499031403842,\n",
       " 0.2862827180820796,\n",
       " 0.4973412255378417,\n",
       " 0.6332773389524056,\n",
       " 0.4736374751299338,\n",
       " 0.8351500764661127,\n",
       " 0.756521706906466,\n",
       " 0.20407058070756845,\n",
       " 0.3509974185403111,\n",
       " 0.7204293485895459,\n",
       " 0.6949743086541318,\n",
       " 0.35108295763314223,\n",
       " 0.3513003936422515,\n",
       " 0.7992709625558736,\n",
       " 0.7085290138945612,\n",
       " 0.6266559954873563,\n",
       " 0.7568126523627771,\n",
       " 0.46077195055422904,\n",
       " 0.5412543434882152,\n",
       " 0.8693771172195321,\n",
       " 0.7229738310058778,\n",
       " 0.7825434122705713,\n",
       " 0.7433175403400045,\n",
       " 0.9090365963290868,\n",
       " 0.626588917587241,\n",
       " 0.8446312665818074,\n",
       " 0.2740922017523686,\n",
       " 0.6247710970081939,\n",
       " 0.3537373428099679,\n",
       " 0.33005561040770115,\n",
       " 0.8073149144849774,\n",
       " 0.3540454628062912,\n",
       " 0.7549119575770551,\n",
       " 0.794096528207032,\n",
       " 0.6267287363549607,\n",
       " 0.7744317053792216,\n",
       " 0.29466796388886396,\n",
       " 0.8548646413093672,\n",
       " 0.8587300152424453,\n",
       " 0.4237070549777583,\n",
       " 0.4814343060335639,\n",
       " 0.5835992396662306,\n",
       " 0.5315864808778143,\n",
       " 0.49284241024646036,\n",
       " 0.1799538689302087,\n",
       " 0.7985647637570645,\n",
       " 0.7444260080078223,\n",
       " 0.898809666599475,\n",
       " 0.5970551275542915,\n",
       " 0.8839955139640487,\n",
       " 0.8903730583158835,\n",
       " 0.832332950331701,\n",
       " 0.47889209505134245,\n",
       " 0.22672130871200108,\n",
       " 0.8556109202375007,\n",
       " 0.8931719182826061,\n",
       " 0.8875154558879248,\n",
       " 0.6607702932189903,\n",
       " 0.5397749063696833,\n",
       " 0.9088674638756793,\n",
       " 0.8814231831967374,\n",
       " 0.7444386555340956,\n",
       " 0.8332901329965997,\n",
       " 0.871501056637679,\n",
       " 0.837256613969705,\n",
       " 0.8529384203905532,\n",
       " 0.35811211947402066,\n",
       " 0.8579736471086338,\n",
       " 0.7777363619974824,\n",
       " 0.8862286078599055,\n",
       " 0.8774668651204083,\n",
       " 0.9100139198194643,\n",
       " 0.8057819084635738,\n",
       " 0.7699974669636371,\n",
       " 0.7489482959962172,\n",
       " 0.8778558314875745,\n",
       " 0.6225986661841998,\n",
       " 0.8687567176568343,\n",
       " 0.8108613509417065,\n",
       " 0.8192084178482404,\n",
       " 0.7927972998618169,\n",
       " 0.3655124672883417,\n",
       " 0.3931360763901246,\n",
       " 0.48101447418504406,\n",
       " 0.6809119994120441,\n",
       " 0.39698217659860846,\n",
       " 0.8626847933028758,\n",
       " 0.904688109202857,\n",
       " 0.328714404843685,\n",
       " 0.6896147846666748,\n",
       " 0.5670411985909318,\n",
       " 0.37620962423968013,\n",
       " 0.7322086311382443,\n",
       " 0.6311199556603779,\n",
       " 0.8062251090144186,\n",
       " 0.5398638553834092,\n",
       " 0.6737388624878561,\n",
       " 0.8849981098486832,\n",
       " 0.8898661151101828,\n",
       " 0.8518041859949705,\n",
       " 0.5728185038144165,\n",
       " 0.8663798903848616,\n",
       " 0.7913948467299906,\n",
       " 0.6824810079372887,\n",
       " 0.4911285566528621,\n",
       " 0.6275714499823579,\n",
       " 0.6725997965795123,\n",
       " 0.8258648668084043,\n",
       " 0.30851902109557533,\n",
       " 0.33521879713283953,\n",
       " 0.18342059149549794,\n",
       " 0.388491185285152,\n",
       " 0.8949946222457534,\n",
       " 0.8810923145161581,\n",
       " 0.8795019436804487,\n",
       " 0.8776607019979209,\n",
       " 0.9116803852934607,\n",
       " 0.8111078532727689,\n",
       " 0.4219068192744577,\n",
       " 0.7141800568451991,\n",
       " 0.5707760844287324,\n",
       " 0.8884324943793369,\n",
       " 0.9116117558158466,\n",
       " 0.7395824954399843,\n",
       " 0.7865090441092758,\n",
       " 0.8367559136566161,\n",
       " 0.8229178086207667,\n",
       " 0.865515794989921,\n",
       " 0.8462575856100748,\n",
       " 0.4003016208740378,\n",
       " 0.8097945345441742,\n",
       " 0.8757803365965711,\n",
       " 0.5597829931619362,\n",
       " 0.8878312699865436,\n",
       " 0.9032959067155456,\n",
       " 0.8916650336076775,\n",
       " 0.8769671395077399,\n",
       " 0.846847024824628,\n",
       " 0.3918296572409733,\n",
       " 0.8112936492501064,\n",
       " 0.9185390405766263,\n",
       " 0.5061377418621873,\n",
       " 0.7199001890368188,\n",
       " 0.8875663775663911,\n",
       " 0.8969670475917821,\n",
       " 0.9060918341945299,\n",
       " 0.8049159513266915,\n",
       " 0.2765457250972496,\n",
       " 0.555649271839064,\n",
       " 0.9009567724536539,\n",
       " 0.7790490054969069,\n",
       " 0.8075121470167703,\n",
       " 0.3470266056397123,\n",
       " 0.8147683441114792,\n",
       " 0.8974160446339177,\n",
       " 0.25474800447259416,\n",
       " 0.767137164607427,\n",
       " 0.5658825748878448,\n",
       " 0.5832311778104985,\n",
       " 0.16765326575076234,\n",
       " 0.28149375461482784,\n",
       " 0.8766893847224698,\n",
       " 0.8737656595660853,\n",
       " 0.6826644474747585,\n",
       " 0.8549694101826036,\n",
       " 0.8423434925045555,\n",
       " 0.8790380644354445,\n",
       " 0.22505055021910625,\n",
       " 0.8203360120933597,\n",
       " 0.8936763549857136,\n",
       " 0.8774253252652708,\n",
       " 0.6742012157532613,\n",
       " 0.9054949614320438,\n",
       " 0.8943875256432869,\n",
       " 0.7604840896355605,\n",
       " 0.6054342307664744,\n",
       " 0.8512212183320838,\n",
       " 0.7683131191556777,\n",
       " 0.845657233408357,\n",
       " 0.7099303108089483,\n",
       " 0.8679830050869218,\n",
       " 0.732119177914556,\n",
       " 0.5954708426162512,\n",
       " 0.8067147355041834,\n",
       " 0.6447717314797605,\n",
       " 0.7906028508137182,\n",
       " 0.642102355434787,\n",
       " 0.9112261911618011,\n",
       " 0.8545752548358504,\n",
       " 0.6901405028547541,\n",
       " 0.9002006382883415,\n",
       " 0.6578826855560578,\n",
       " 0.42150642583574505,\n",
       " 0.7657536636237808,\n",
       " 0.7179581054086809,\n",
       " 0.8882807216228246,\n",
       " 0.8613141630692421,\n",
       " 0.8954147412634559,\n",
       " 0.7480143752887827,\n",
       " 0.9148178362824143,\n",
       " 0.7372248016332963,\n",
       " 0.803730716482895,\n",
       " 0.814693422331033,\n",
       " 0.8262589519253047,\n",
       " 0.49728910579008057,\n",
       " 0.7383759183683253,\n",
       " 0.4056430658599764,\n",
       " 0.8004804158193752,\n",
       " 0.6719204882212123,\n",
       " 0.3556115594141831,\n",
       " 0.453725564343172,\n",
       " 0.7026779564827262,\n",
       " 0.7094679444737579,\n",
       " 0.42476150717529704,\n",
       " 0.7830529385967683,\n",
       " 0.844270217273691,\n",
       " 0.6958803060802545,\n",
       " 0.5156227115608881,\n",
       " 0.5772064690890611,\n",
       " 0.5582553637785304,\n",
       " 0.809358770972963,\n",
       " 0.879960806508024,\n",
       " 0.5730888388464161,\n",
       " 0.7465541324352029,\n",
       " 0.9098075259607646,\n",
       " 0.8776844134837997,\n",
       " 0.8996887674864631,\n",
       " 0.8667683283142883,\n",
       " 0.7518138592980543,\n",
       " 0.49060029720311976,\n",
       " 0.8840642914833032,\n",
       " 0.8790296557861902,\n",
       " 0.5070086622044724,\n",
       " 0.7451594993124193,\n",
       " 0.8332327105689448,\n",
       " 0.8241929546925754,\n",
       " 0.8687097243678858,\n",
       " 0.9046464498517812,\n",
       " 0.1773950810580735,\n",
       " 0.19105988437251817,\n",
       " 0.6050852240612692,\n",
       " 0.8343796923591918,\n",
       " 0.9049406222986306,\n",
       " 0.7897623307371504,\n",
       " 0.9137833110467728,\n",
       " 0.2772225856944174,\n",
       " 0.7119233972856127,\n",
       " 0.5144325236113942,\n",
       " 0.5520576196100289,\n",
       " 0.8280239806765481,\n",
       " 0.7367137716231874,\n",
       " 0.5952450111043678,\n",
       " 0.2817321211282897,\n",
       " 0.3722584479329085,\n",
       " 0.646968595610043,\n",
       " 0.6728808657109766,\n",
       " 0.8591805453416715,\n",
       " 0.22923623610197827,\n",
       " 0.5878392802218163,\n",
       " 0.6977441457096255,\n",
       " 0.2497426753614063,\n",
       " 0.8765174962553868,\n",
       " 0.8857446824306507,\n",
       " 0.8294561273553005,\n",
       " 0.8804915846360681,\n",
       " 0.7886352099546619,\n",
       " 0.572632663173591,\n",
       " 0.8052427963721103,\n",
       " 0.7185137026885197,\n",
       " 0.7585645706516317,\n",
       " 0.6732010037756776,\n",
       " 0.20436888393974895,\n",
       " 0.7296639032158065,\n",
       " 0.8748356746351762,\n",
       " 0.894013617481221,\n",
       " 0.623884596768733,\n",
       " 0.7169820462845123,\n",
       " 0.8362243102325664,\n",
       " 0.7810649737563145,\n",
       " 0.602535064733871,\n",
       " 0.8701378290262352,\n",
       " 0.7456549576189986,\n",
       " 0.37482693104129083,\n",
       " 0.5047657462579358,\n",
       " 0.6523970590797167,\n",
       " 0.7772842955486146,\n",
       " 0.7271632100295331,\n",
       " 0.6139591524095065,\n",
       " 0.38899753644301194,\n",
       " 0.594730874004855,\n",
       " 0.8315061290915589,\n",
       " 0.3940182531803339,\n",
       " 0.7435188925141144,\n",
       " 0.6896188401610934,\n",
       " 0.817975986119503,\n",
       " 0.278635600108231,\n",
       " 0.4029013295088486,\n",
       " 0.6144845978966912,\n",
       " 0.6402800641217385,\n",
       " 0.8076854213410771,\n",
       " 0.8974728568591757,\n",
       " 0.2608426801945973,\n",
       " 0.8527193591401668,\n",
       " 0.7739038150693527,\n",
       " 0.7788911982501117,\n",
       " 0.7920222498892004,\n",
       " 0.8842070942391138,\n",
       " 0.8793053792654777,\n",
       " 0.7144022463963051,\n",
       " 0.8207229356600215,\n",
       " 0.9015524726268039,\n",
       " 0.8014326126105495,\n",
       " 0.8436559726420779,\n",
       " 0.8427333723141428,\n",
       " 0.7309382140445214,\n",
       " 0.8329713655741363,\n",
       " 0.8856737640395245,\n",
       " 0.8549154491654765,\n",
       " 0.739584906074659,\n",
       " 0.8353025491455068,\n",
       " 0.5180827261787173,\n",
       " 0.5188372613818385,\n",
       " 0.6997290727654635,\n",
       " 0.7123450202959378,\n",
       " 0.3006672790686654,\n",
       " 0.8312613592969231,\n",
       " 0.6554954203654453,\n",
       " 0.861220479243954,\n",
       " 0.4604833445773304,\n",
       " 0.7951593759604333,\n",
       " 0.8507808233171505,\n",
       " 0.6284728397974979,\n",
       " 0.41901065716231267,\n",
       " 0.6247685712139219,\n",
       " 0.17574662733332835,\n",
       " 0.4160840374709935,\n",
       " 0.678299990494471,\n",
       " 0.33069109709602607,\n",
       " 0.7770807127933806,\n",
       " 0.8908332577347797,\n",
       " 0.3742101677556939,\n",
       " 0.9082493442799274,\n",
       " 0.8220957918599214,\n",
       " 0.393180011341984,\n",
       " 0.7217702794027145,\n",
       " 0.3929363351990245,\n",
       " 0.7301664098500673,\n",
       " 0.8313785870709673,\n",
       " 0.8617183709010916,\n",
       " 0.7570163795125217,\n",
       " 0.8626967553142945,\n",
       " 0.5213809590081443,\n",
       " 0.6188278687254191,\n",
       " 0.8577820796747487,\n",
       " 0.8692159608919006,\n",
       " 0.4566335548051922,\n",
       " 0.4088343082534575,\n",
       " 0.48069766305198974,\n",
       " 0.846821335860581,\n",
       " 0.8742960885352054,\n",
       " 0.33541145599171507,\n",
       " 0.8381209092534027,\n",
       " 0.8727422318911509,\n",
       " 0.8748954184830896,\n",
       " 0.8092255157312036,\n",
       " 0.886995145285473,\n",
       " 0.8843905106759749,\n",
       " 0.837069451671496,\n",
       " 0.22154147010135927,\n",
       " 0.7302963381502039,\n",
       " 0.8047634793432294,\n",
       " 0.2715662801863071,\n",
       " 0.5200469284153197,\n",
       " 0.8334472924441446,\n",
       " 0.8631959614001415,\n",
       " 0.7972617814888057,\n",
       " 0.8318892790834056,\n",
       " 0.5675102428584539,\n",
       " 0.8731701992919837,\n",
       " 0.7865311174139058,\n",
       " 0.5662413899115826,\n",
       " 0.8553640067667041,\n",
       " 0.9152531119032553,\n",
       " 0.35760151710508403,\n",
       " 0.423260424893758,\n",
       " 0.8210637107333756,\n",
       " 0.5906320343767488,\n",
       " 0.8459336424612743,\n",
       " 0.6460812754879831,\n",
       " 0.8360126711278681,\n",
       " 0.368561442119002,\n",
       " 0.7922449114787224,\n",
       " 0.6972680052761003,\n",
       " 0.8650108279047286,\n",
       " 0.4018486530548132,\n",
       " 0.48153152747761563,\n",
       " 0.705465239095199,\n",
       " 0.6689340991479736,\n",
       " 0.8540821136278621,\n",
       " 0.45061346351917686,\n",
       " 0.520339731607768,\n",
       " 0.39560425466049565,\n",
       " 0.6093379604277691,\n",
       " 0.6992728315628878,\n",
       " 0.7554716572440724,\n",
       " 0.8691269563100473,\n",
       " 0.5159490963252065,\n",
       " 0.44145044519677923,\n",
       " 0.64535522820211,\n",
       " 0.7142888019836051,\n",
       " 0.21761693260768994,\n",
       " 0.8490495532496329,\n",
       " 0.6449012101744683,\n",
       " 0.7381068449988842,\n",
       " 0.19014825637655322,\n",
       " 0.8983494257981344,\n",
       " 0.387405923516125,\n",
       " 0.5778539366149409,\n",
       " 0.8197770213250674,\n",
       " 0.8694267868222532,\n",
       " 0.5201411661370647,\n",
       " 0.8856316988946105,\n",
       " 0.6797626047680044,\n",
       " 0.6531473353631609,\n",
       " 0.42446415160128914,\n",
       " 0.7922310794556678,\n",
       " 0.6191543198418208,\n",
       " 0.9046345743766089,\n",
       " 0.9167194856406214,\n",
       " 0.8958859896921088,\n",
       " 0.821893897129673,\n",
       " 0.8812758349093115,\n",
       " 0.5638149849875169,\n",
       " 0.8838552362320223,\n",
       " 0.7914771909506836,\n",
       " 0.874170781880245,\n",
       " 0.8346346754506476,\n",
       " 0.8179028605717183,\n",
       " 0.29815397816182626,\n",
       " 0.8424850222530179,\n",
       " 0.7614424803489251,\n",
       " 0.6976663000446566,\n",
       " 0.8951829179293678,\n",
       " 0.7447218370266115,\n",
       " 0.7083693689374916,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp = []\n",
    "for i in range(len(Y_predict_rf)):\n",
    "    Y_predict_temp.append((Y_predict_rf[i] + Y_predict_gbc[i]) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict\n",
    "submit_score.to_csv('predict_result_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = Y_predict_temp\n",
    "submit_score.to_csv('predict_result_ncnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AA = load_label('predict_result_488.csv')\n",
    "aa = AA.score.as_matrix(columns=None).tolist()\n",
    "BB = load_label('predict_result_495.csv')\n",
    "bb = BB.score.as_matrix(columns=None).tolist()\n",
    "CC = load_label('predict_result_509.csv')\n",
    "cc = CC.score.as_matrix(columns=None).tolist()\n",
    "DD = load_label('predict_result_cnn.csv')\n",
    "dd = DD.score.as_matrix(columns=None).tolist()\n",
    "\n",
    "comb = []\n",
    "for i in range(len(cc)):\n",
    "    comb.append((aa[i] + bb[i] + cc[i] + dd[i]) / 4.0)\n",
    "\n",
    "#comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_score = load_label('submission.csv')\n",
    "submit_score  = temp_score\n",
    "submit_score['score'] = dd\n",
    "submit_score.to_csv('combined.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(train_file,values = 'price', index=['lvl1','lvl2','lvl3'],columns=['type'],aggfunc=[min, max, np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
